Missing .coveralls.yml file. Using only env variables.
Submitting coverage to coveralls.io...
{"source_files": [{"name": "optim_esm_tools/__init__.py", "source": "__version__ = '3.1.0'\n__author__ = 'Joran R. Angevaare'\n\nfrom .plotting import plot_utils\nfrom . import utils\nfrom . import config\nfrom . import analyze\nfrom . import region_finding\nfrom . import _test_utils\nfrom . import plotting\n\n# Forward some of the essential tools to this main\nfrom .analyze.cmip_handler import read_ds\nfrom .analyze.io import load_glob\nfrom .utils import print_versions\nfrom .config import get_logger\n", "coverage": [1, 1, null, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, 1]}, {"name": "optim_esm_tools/_test_utils.py", "source": "import os\n\nEXMPLE_DATA_SET = 'CMIP6/ScenarioMIP/CCCma/CanESM5/ssp585/r3i1p2f1/Amon/tas/gn/v20190429/tas_Amon_CanESM5_ssp585_r3i1p2f1_gn_201501-210012.nc'\n\n\ndef cmip_store():\n    import intake\n\n    return intake.open_esm_datastore(\n        'https://storage.googleapis.com/cmip6/pangeo-cmip6.json',\n    )  # type: ignore\n\n\ndef get_file_from_pangeo(experiment_id='ssp585', refresh=True):\n    # sourcery skip: dict-assign-update-to-union\n    dest_folder = os.path.split(\n        get_example_data_loc().replace('ssp585', experiment_id),\n    )[0]\n    if experiment_id in ['piControl', 'historical']:\n        dest_folder = dest_folder.replace('ScenarioMIP', 'CMIP')\n    write_to = os.path.join(dest_folder, 'test.nc')\n    if os.path.exists(write_to) and not refresh:\n        print(f'already file at {write_to}')\n        return write_to\n\n    col = cmip_store()\n    query = dict(\n        source_id='CanESM5',\n        variable_id='tas',\n        table_id='Amon',\n        experiment_id=experiment_id,\n    )\n    if experiment_id in ['historical', 'ssp585']:\n        query.update(dict(member_id=['r3i1p2f1']))  # type: ignore\n    else:\n        query.update(dict(member_id=['r1i1p1f1']))  # type: ignore\n    search = col.search(**query)  # type: ignore\n\n    ddict = search.to_dataset_dict(\n        xarray_open_kwargs={'use_cftime': True},\n    )\n    data = list(ddict.values())[0]\n\n    data = data.mean(set(data.dims) - {'x', 'y', 'lat', 'lon', 'time'})\n    if query['variable_id'] != 'tas':  # pragma: no cover\n        raise ValueError(\n            'Only tas for now as only areacella is hardcoded (see line below)',\n        )\n\n    data.attrs.update(\n        dict(\n            external_variables='areacella',\n            variable_id='tas',\n        ),\n    )\n    os.makedirs(dest_folder, exist_ok=True)\n    assert data.attrs.get('variable_id')\n    data.to_netcdf(write_to)\n    return write_to\n\n\ndef year_means(path, refresh=True):\n    new_dir = os.path.split(path.replace('Amon', 'AYear'))[0]\n    new_dest = os.path.join(new_dir, 'test_merged.nc')\n    if os.path.exists(new_dest) and not refresh:\n        print(f'File at {new_dest} already exists')\n        return new_dest\n    import cftime\n    import optim_esm_tools as oet\n\n    data = oet.analyze.io.load_glob(path)\n\n    data = data.groupby('time.year').mean('time')\n    data = data.rename(year='time')\n    data['time'] = [cftime.DatetimeNoLeap(y, 1, 1) for y in data['time']]\n    data.attrs.update(dict(external_variables='areacella', variable_id='tas'))\n    os.makedirs(new_dir, exist_ok=True)\n    assert os.path.exists(new_dir)\n    assert data.attrs.get('variable_id')\n    data.to_netcdf(new_dest)\n    return new_dest\n\n\ndef get_synda_loc():\n    return os.path.join(\n        os.environ.get('ST_HOME', os.path.join(os.path.abspath('.'), 'cmip')),\n        'data',\n    )\n\n\ndef get_example_data_loc():\n    return os.path.join(get_synda_loc(), EXMPLE_DATA_SET)\n\n\ndef synda_test_available():\n    \"\"\"Check if we can run a synda-dependent test.\"\"\"\n    return os.environ.get('ST_HOME') is not None and os.path.exists(\n        get_example_data_loc(),\n    )\n\n\ndef get_path_for_ds(data_name, refresh=True):\n    path = get_file_from_pangeo(data_name, refresh=refresh)\n    year_path = year_means(path, refresh=refresh)\n    assert year_path\n    assert os.path.exists(year_path)\n    return year_path\n\n\ndef complete_ds(start_year=2000, **kw):\n    import cftime\n    import optim_esm_tools as oet\n\n    ds = oet._test_utils.minimal_xr_ds(**kw)\n    ds['time'] = [cftime.datetime(y + start_year, 1, 1) for y in range(len(ds['time']))]\n    ds['lat'].attrs.update(\n        {\n            'standard_name': 'latitude',\n            'long_name': 'Latitude',\n            'units': 'degrees_north',\n            'axis': 'Y',\n        },\n    )\n    ds['lon'].attrs.update(\n        {\n            'standard_name': 'longitude',\n            'long_name': 'Longitude',\n            'units': 'degrees_east',\n            'axis': 'X',\n        },\n    )\n    return ds\n\n\ndef minimal_xr_ds(len_x=513, len_y=181, len_time=10, add_nans=True):\n    import numpy as np\n    import xarray as xr\n\n    lon = np.linspace(0, 360, len_x + 1)[:-1]\n    lat = np.linspace(-90, 90, len_y + 1)[:-1]\n    time = np.arange(len_time)\n    # Totally arbitrary data\n    data = (\n        np.zeros(len(lat) * len(lon) * len(time)).reshape(len(time), len(lat), len(lon))\n        * lon\n    )\n\n    # Add some NaN values just as an example\n    if add_nans:\n        data[:, :, len(lon) // 2 + 30 : len(lon) // 2 + 50] = np.nan\n\n    return xr.Dataset(\n        data_vars=dict(\n            var=(\n                ('time', 'lat', 'lon'),\n                data,\n            ),\n        ),\n        coords=dict(\n            time=time,\n            lat=lat,\n            lon=lon,\n        ),\n        attrs=dict(source_id='bla', variable_id='var'),\n    )\n", "coverage": [1, null, 1, null, null, 1, 1, null, 1, null, null, null, null, 1, null, 1, null, null, 1, 1, 1, 1, 1, 1, null, 1, 1, null, null, null, null, null, 1, 1, null, 1, 1, null, 1, null, null, 1, null, 1, null, null, null, null, null, 1, null, null, null, null, null, 1, 1, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, null, null, null, 1, 1, null, null, 1, null, 0, null, null, null, null, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, null, 1, 1, 1, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, 1, null, null, 1, 1, 1, null, 1, 1, 1, null, 1, null, null, null, null, null, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null]}, {"name": "optim_esm_tools/analyze/__init__.py", "source": "from . import clustering\nfrom . import cmip_handler\nfrom . import combine_variables\nfrom . import concise_dataframe\nfrom . import find_matches\nfrom . import io\nfrom . import merge_candidate_regions\nfrom . import pre_process\nfrom . import region_finding\nfrom . import time_statistics\nfrom . import tools\nfrom . import xarray_tools\nfrom . import region_calculation\nfrom . import discontinuous_grid_patcher\n", "coverage": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, {"name": "optim_esm_tools/analyze/clustering.py", "source": "import contextlib\nimport typing as ty\nfrom math import atan2\nfrom math import cos\nfrom math import radians\nfrom math import sin\nfrom math import sqrt\n\nimport numba\nimport numpy as np\nimport xarray as xr\n\nfrom optim_esm_tools.config import config\nfrom optim_esm_tools.config import get_logger\nfrom optim_esm_tools.utils import timed, deprecated\nfrom optim_esm_tools.utils import tqdm\n\n\n@timed()\ndef build_clusters(\n    coordinates_deg: np.ndarray,\n    weights: ty.Optional[np.ndarray] = None,\n    max_distance_km: ty.Union[float, int] = 750,\n    only_core: bool = True,\n    min_samples: int = int(config['analyze']['clustering_min_neighbors']),\n    cluster_opts: ty.Optional[dict] = None,\n    keep_masks: bool = False,\n) -> ty.Union[ty.List[np.ndarray], ty.Tuple[ty.List[np.ndarray], ty.List[np.ndarray]]]:\n    \"\"\"Build clusters based on a list of coordinates, use halfsine metric for\n    spherical spatial data.\n\n    Args:\n        coordinates_deg (np.ndarray): set of xy coordinates in degrees\n        weights (ty.Optional[np.ndarray], optional): weights (in the range [0,1]) corresponding to each coordinate\n        max_distance_km (ty.Union[float, int], optional): max distance to other points to consider part of\n            cluster (see DBSCAN(eps=<..>)). Defaults to 750.\n        only_core (bool, optional): Use only core samples. Defaults to True.\n        min_samples (int): Minimum number of samples in cluster. Defaults to 8.\n        cluster_opts (ty.Optional[dict], optional): Additional options passed to sklearn.cluster.DBSCAN. Defaults to None.\n        keep_masks (bool): return a tuple with both the clusters (coords) and masks (2d boolean arrays)\n\n    Returns:\n        ty.List[np.ndarray]: list of clustered points (in radians)\n        or\n        ty.Tuple[ty.List[np.ndarray], ty.List[np.ndarray]]]: list of clustered points (in radians) and list\n            of boolean masks with the same length as the input coordinates deg.\n    \"\"\"\n    cluster_coords, cluster_masks = _build_clusters(\n        coordinates_deg,\n        weights,\n        max_distance_km,\n        only_core,\n        min_samples,\n        cluster_opts,\n    )\n    if keep_masks:\n        return cluster_coords, cluster_masks\n    return cluster_coords\n\n\ndef _build_clusters(\n    coordinates_deg: np.ndarray,\n    weights: ty.Optional[np.ndarray] = None,\n    max_distance_km: ty.Union[float, int] = 750,\n    only_core: bool = True,\n    min_samples: int = int(config['analyze']['clustering_min_neighbors']),\n    cluster_opts: ty.Optional[dict] = None,\n) -> ty.Tuple[ty.List[np.ndarray], ty.List[np.ndarray]]:\n    cluster_opts = cluster_opts or {}\n    for class_label, v in dict(algorithm='ball_tree', metric='haversine').items():\n        cluster_opts.setdefault(class_label, v)\n    cluster_opts['min_samples'] = min_samples\n\n    from sklearn.cluster import DBSCAN\n\n    coordinates_rad = np.radians(coordinates_deg).T\n\n    # TODO use a more up to date version:\n    #  https://scikit-learn.org/stable/auto_examples/cluster/plot_hdbscan.html#sphx-glr-auto-examples-cluster-plot-hdbscan-py\n    #  https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html#sklearn.cluster.HDBSCAN\n    # Thanks https://stackoverflow.com/a/38731787/18280620!\n    try:\n        db_fit = DBSCAN(eps=max_distance_km / 6371.0, **cluster_opts).fit(\n            X=coordinates_rad,\n            sample_weight=weights,\n        )\n    except ValueError as e:  # pragma: no cover\n        raise ValueError(\n            f'With {coordinates_rad.shape} and {getattr(weights, \"shape\", None)} {coordinates_rad}, {weights}',\n        ) from e\n\n    labels = db_fit.labels_\n\n    unique_labels = sorted(set(labels))\n    is_core_sample = np.zeros_like(labels, dtype=bool)\n    is_core_sample[db_fit.core_sample_indices_] = True\n\n    return_masks = []\n    return_coord = []\n    for class_label in unique_labels:\n        is_noise = class_label == -1\n        if is_noise:\n            continue\n\n        is_class_member = labels == class_label\n        coord_mask = is_class_member\n        if only_core:\n            coord_mask &= is_core_sample\n\n        masked_points = coordinates_rad[coord_mask]\n        return_coord.append(masked_points)\n        return_masks.append(coord_mask)\n\n    return return_coord, return_masks\n\n\n@timed()\ndef build_cluster_mask(\n    global_mask: np.ndarray,\n    lat_coord: np.ndarray,\n    lon_coord: np.ndarray,\n    show_tqdm: ty.Optional[bool] = None,\n    max_distance_km: ty.Union[str, float, int] = 'infer',\n    **kw,\n) -> ty.Tuple[ty.List[np.ndarray], ty.List[np.ndarray]]:\n    \"\"\"Build set of clusters and masks based on the global mask, basically a\n    utility wrapper around build_clusters'.\n\n    Args:\n        global_mask (np.ndarray): full 2d mask of the data\n        lon_coord (np.array): all longitude values\n        lat_coord (np.array): all latitude values\n        max_distance_km (ty.Union[str, float, int]): find an appropriate distance\n            threshold for build_clusters' max_distance_km argument. If nothing is\n            provided, make a guess based on the distance between grid cells.\n            Defaults to 'infer'.\n\n    Returns:\n        ty.List[ty.List[np.ndarray], ty.List[np.ndarray]]: Return two lists, containing the masks, and clusters respectively.\n    \"\"\"\n    if max_distance_km == 'infer':\n        max_distance_km = infer_max_step_size(lat_coord, lon_coord)\n    lat, lon = _check_input(\n        global_mask,\n        lat_coord,\n        lon_coord,\n    )\n    xy_data = np.array([lat[global_mask], lon[global_mask]])\n\n    if len(xy_data.T) <= 2:\n        get_logger().info(f'No data from this mask {xy_data}!')\n        return [], []\n\n    masks, clusters = _build_cluster_with_kw(\n        lat=lat,\n        lon=lon,\n        coordinates_deg=xy_data,\n        max_distance_km=max_distance_km,\n        global_mask=global_mask,\n        show_tqdm=show_tqdm,\n        **kw,\n    )\n\n    return masks, clusters\n\n\n@timed()\ndef build_weighted_cluster(\n    weights: np.ndarray,\n    lat_coord: np.ndarray,\n    lon_coord: np.ndarray,\n    show_tqdm: ty.Optional[bool] = None,\n    threshold: ty.Optional[float] = 0.99,\n    max_distance_km: ty.Union[str, float, int] = 'infer',\n    **kw,\n) -> ty.Tuple[ty.List[np.ndarray], ty.List[np.ndarray]]:\n    \"\"\"Build set of clusters and masks based on the weights (which should be a\n    grid)'.\n\n    Args:\n        weights (np.ndarray): normalized score data (values in [0,1])\n        lon_coord (np.array): all longitude values\n        lat_coord (np.array): all latitude values\n        max_distance_km (ty.Union[str, float, int]): find an appropriate distance\n            threshold for build_clusters' max_distance_km argument. If nothing is\n            provided, make a guess based on the distance between grid cells.\n            Defaults to 'infer'.\n        threshold: float, min value of the passed weights. Defaults to 0.99.\n\n    Returns:\n        ty.List[ty.List[np.ndarray], ty.List[np.ndarray]]: Return two lists, containing the masks, and clusters respectively.\n    \"\"\"\n    if max_distance_km == 'infer':\n        max_distance_km = infer_max_step_size(lat_coord, lon_coord)\n\n    lat, lon = _check_input(weights, lat_coord, lon_coord)\n    xy_data = np.array([lat.flatten(), lon.flatten()])\n\n    flat_weights = weights.flatten()\n    mask = flat_weights > threshold\n    global_mask = weights > threshold\n    masks, clusters = _build_cluster_with_kw(\n        lat=lat,\n        lon=lon,\n        coordinates_deg=xy_data[:, mask],\n        weights=flat_weights[mask],\n        show_tqdm=show_tqdm,\n        max_distance_km=max_distance_km,\n        global_mask=global_mask,\n        **kw,\n    )\n\n    return masks, clusters\n\n\ndef _check_input(\n    data: np.ndarray,\n    lat_coord: np.ndarray,\n    lon_coord: np.ndarray,\n) -> ty.Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Check for consistency and if we need to convert the lon/lat coordinates\n    to a meshgrid.\"\"\"\n    if len(lon_coord.shape) <= 1:\n        lon, lat = np.meshgrid(lon_coord, lat_coord)\n    else:\n        # In an older version, this would have been the default.\n        lat, lon = lat_coord, lon_coord\n\n    if data.shape != lon.shape or data.shape != lat.shape:  # pragma: no cover\n        message = f'Wrong input {data.shape} != {lon.shape, lat.shape}'\n        raise ValueError(message)\n    return lat, lon\n\n\n@deprecated\ndef _split_to_continous(*a, **kw):\n    return _split_to_continuous(*a, **kw)\n\n\ndef _split_to_continuous(\n    masks: ty.List,\n    **kw,\n) -> ty.List[np.ndarray]:\n    no_group = -1\n    mask_groups = masks_array_to_coninuous_sets(masks, no_group_value=no_group, **kw)\n    continous_masks = []\n    for grouped_members in mask_groups:\n        for group_id in np.unique(grouped_members):\n            if group_id == no_group:\n                continue\n            continous_masks.append(grouped_members == group_id)\n\n    small_first = np.argsort([np.sum(c) for c in continous_masks])\n    large_first = small_first[::-1]\n    continous_masks = [\n        group for i in large_first for k, group in enumerate(continous_masks) if i == k\n    ]\n\n    return continous_masks\n\n\ndef _find_lat_lon_values(\n    mask_2d: np.ndarray,\n    lats: np.ndarray,\n    lons: np.ndarray,\n) -> np.ndarray:\n    lon_coords = lons[mask_2d]\n    lat_coords = lats[mask_2d]\n    return np.vstack([lat_coords, lon_coords]).T\n\n\ndef _build_cluster_with_kw(\n    lat: np.ndarray,\n    lon: np.ndarray,\n    show_tqdm=None,\n    global_mask=None,\n    force_continuity: bool = False,\n    **cluster_kw,\n) -> ty.Tuple[ty.List[np.ndarray], ty.List[np.ndarray]]:\n    \"\"\"Overlapping logic between functions to get the masks and clusters.\n\n    force_continuity (bool): split the masks until each is a continuous\n    set\n    \"\"\"\n\n    clusters, sub_masks = build_clusters(**cluster_kw, keep_masks=True)\n\n    if global_mask is None:\n        raise ValueError('global_mask is required')\n    clusters = [np.rad2deg(cluster) for cluster in clusters]\n\n    if lat.shape != lon.shape:\n        raise ValueError(\n            f'Got inconsistent input {lat.shape} != {lon.shape}',\n        )  # pragma: no cover\n\n    masks: ty.List[np.ndarray] = []\n    for sub_mask in sub_masks:\n        full_2d_mask = np.zeros_like(global_mask)\n        full_2d_mask[global_mask] = sub_mask\n\n        masks.append(np.array(full_2d_mask))\n\n    if force_continuity:\n        masks = _split_to_continuous(masks=masks)\n\n        clusters = [_find_lat_lon_values(m, lats=lat, lons=lon) for m in masks]\n\n    return masks, clusters\n\n\ndef infer_max_step_size(\n    lat: np.ndarray,\n    lon: np.ndarray,\n    off_by_factor: ty.Optional[float] = None,\n) -> float:\n    \"\"\"Infer the max. distance between two points to be considered as belonging\n    to the same cluster.\n\n    There are two methods implemented, preferably, the lon, lat values\n    are 1d-arrays, which can be interpreted as a regular grid. If this\n    is the case, calculate the distance for each point to it's neighbors\n    (also diagonally). Then, the max distance for the clustering can be\n    taken as the max. distance to any of the neighboring points.\n\n    Empirically, we found that this distance is not enough, and an\n    additional fudge factor is taken into account from version v1.0.3\n    onwards, this is taken to be sqrt(2). This is probably not a\n    coincidence, but it's not really clear where it's coming from.\n    \"\"\"\n    if off_by_factor is None:\n        off_by_factor = float(config['analyze']['clustering_fudge_factor'])\n    assert len(lat.shape) == 1\n    # Simple 1D array\n    return off_by_factor * np.max(calculate_distance_map(lat, lon))\n\n\ndef calculate_distance_map(lat: np.ndarray, lon: np.ndarray) -> np.ndarray:\n    \"\"\"For each point in a spanned lat lon grid, calculate the distance to the\n    neighboring points.\"\"\"\n    if isinstance(lat, xr.DataArray):  # pragma: no cover\n        raise ValueError('Numpy array required')\n    return _calculate_distance_map(lat, lon)\n\n\n@numba.njit\ndef _calculate_distance_map(\n    lat: np.ndarray,\n    lon: np.ndarray,\n) -> np.ndarray:  # sourcery skip: use-itertools-product\n    n_lat = len(lat)\n    n_lon = len(lon)\n    distances = np.zeros((n_lat, n_lon))\n\n    shift_by_index = np.array(\n        [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (1, 1), (1, -1), (-1, 1)],\n    )\n    neighbors = np.zeros(len(shift_by_index), dtype=np.float64)\n    for lon_i in range(n_lon):\n        for lat_i in range(n_lat):\n            neighbors[:] = 0\n            current = (lat[lat_i], lon[lon_i])\n            for i, (x, y) in enumerate(shift_by_index):\n                alt_lon = np.mod(lon_i + x, n_lon)\n                alt_lat = lat_i + y\n                if alt_lat == n_lat or alt_lat < 0:\n                    continue\n                alt_coord = (lat[alt_lat], lon[alt_lon])\n                if alt_coord == current:\n                    raise ValueError('How can this happen?')  # pragma: no cover\n                neighbors[i] = _distance_bf_coord(*current, *alt_coord)\n            distances[lat_i][lon_i] = np.max(neighbors)\n    return distances\n\n\ndef _distance(coords: np.ndarray, force_math: bool = False) -> float:\n    \"\"\"Wrapper for if geopy is not installed.\"\"\"\n    if not force_math:\n        with contextlib.suppress(ImportError):\n            from geopy.distance import geodesic\n\n            return geodesic(*coords).km\n    if len(coords) != 4:\n        coords = np.array([c for cc in coords for c in cc])\n    return _distance_bf_coord(*coords)\n\n\n@numba.njit\ndef _distance_bf_coord(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n    lat1 = radians(lat1)\n    lon1 = radians(lon1)\n    lat2 = radians(lat2)\n    lon2 = radians(lon2)\n    return _distance_bf(lat1, lon1, lat2, lon2)\n\n\n@numba.njit\ndef _distance_bf(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n    # sourcery skip: inline-immediately-returned-variable\n    # https://stackoverflow.com/a/19412565/18280620\n\n    # Approximate radius of earth in km\n    R = 6373.0\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    distance = R * c\n\n    return distance\n\n\n@numba.njit\ndef _nb_clip(\n    a: ty.Union[float, int],\n    b: ty.Union[float, int],\n    c: ty.Union[float, int],\n) -> ty.Union[float, int]:\n    \"\"\"Cheap numba alternative to np.clip.\"\"\"\n    x = max(a, b)\n    return min(x, c)\n\n\n@numba.njit\ndef _all_adjacent_indexes(\n    index: np.ndarray,\n    len_lon: int,\n    len_lat: int,\n    add_diagonal: bool = True,\n    add_double_lat: bool = False,\n    add_double_lon: bool = True,\n    add_90NS_bound: bool = True,\n) -> np.ndarray:\n    \"\"\"For a given index, return an array of indexes that are adjacent.\n\n    There are several options to add points:\n        - add_diagonal: add diagonal elements seen from index\n        - add_double_lat: add items that are 2 steps from the index in the lat direction\n        - add_double_lon: add items that are 2 steps from the index in the lon direction\n        - add_90NS_bound: for items that are at the lat bound, add all lon at the same lat.\n    \"\"\"\n    lat, lon = index\n    lat_up = _nb_clip(lat + 1, 0, len_lat - 1)\n    lat_do = _nb_clip(lat - 1, 0, len_lat - 1)\n    lon_up = np.mod(lon + 1, len_lon)\n    lon_do = np.mod(lon - 1, len_lon)\n\n    alt = [(lat_up, lon), (lat_do, lon), (lat, lon_up), (lat, lon_do)]\n\n    if add_diagonal:\n        alt = alt + [\n            (lat_up, lon_up),\n            (lat_do, lon_up),\n            (lat_up, lon_do),\n            (lat_do, lon_do),\n        ]\n    if add_double_lat:\n        lat_double_up = _nb_clip(lat + 2, 0, len_lat - 1)\n        lat_double_do = _nb_clip(lat - 2, 0, len_lat - 1)\n        alt = alt + [(lat_double_up, lon), (lat_double_do, lon)]\n    if add_double_lon:\n        lon_double_up = np.mod(lon + 2, len_lon)\n        lon_double_do = np.mod(lon - 2, len_lon)\n        alt = alt + [(lat, lon_double_up), (lat, lon_double_do)]\n    if add_90NS_bound and (lat == len_lat - 1 or lat == 0):\n        alt = alt + [(lat, i) for i in range(len_lon)]\n    return np.array([a for a in alt if ~np.array_equal(a, index)])\n\n\n@numba.njit\ndef _indexes_to_2d_buffer(\n    indexes: np.ndarray,\n    buffer_2d: np.ndarray,\n    result_2d: np.ndarray,\n    only_if_val,\n) -> None:\n    \"\"\"Fill elements in buffer2d with on indexes if they are not in\n    exclude_2d.\"\"\"\n    for index in indexes:\n        if result_2d[index[0], index[1]] == only_if_val:\n            buffer_2d[index[0], index[1]] = True\n\n\ndef masks_array_to_coninuous_sets(\n    masks: ty.List,\n    no_group_value: int = -1,\n    add_diagonal: bool = True,\n    **kw,\n) -> ty.List:\n    \"\"\"Call _group_mask_in_continous_sets for a group of masks with the same\n    dimensions to reuse buffer arrays.\"\"\"\n    if not masks:\n        return []\n\n    len_x, len_y = masks[0].shape\n\n    result_groups = np.ones_like(masks[0], dtype=np.int64) * no_group_value\n    check_buffer = np.zeros_like(masks[0], dtype=np.bool_)\n    kw_cont_sets = dict(\n        len_x=len_x,\n        len_y=len_y,\n        add_diagonal=add_diagonal,\n    )\n    kw_cont_sets.update(kw)\n    # Warning, do notice that the result_buffer and check_buffer are modified in place! However, _group_mask_in_continous_sets does reset the buffer each time\n    # Therefore, we have to copy the result each time! Otherwise that result will be overwritten in the next iteration\n    return [\n        _group_mask_in_continous_sets(\n            mask=mask,\n            no_group_value=no_group_value,\n            result_buffer=result_groups,\n            check_buffer=check_buffer,\n            **kw_cont_sets,\n        ).copy()\n        for mask in masks\n    ]\n\n\ndef group_mask_in_continous_sets(mask: np.ndarray, *a, **kw) -> np.ndarray:\n    return masks_array_to_coninuous_sets([mask])[0]\n\n\n@numba.njit\ndef _group_mask_in_continous_sets(\n    mask: np.ndarray,\n    no_group_value: int,\n    len_x: int,\n    len_y: int,\n    result_buffer: np.ndarray,\n    check_buffer: np.ndarray,\n    add_diagonal: bool = True,\n    add_double_lat: bool = False,\n    add_double_lon: bool = True,\n    add_90NS_bound: bool = True,\n) -> np.ndarray:\n    # resetting the buffer is essential for calling `masks_array_to_coninuous_sets`\n    result_buffer[:] = no_group_value\n    check_buffer[:] = False\n    indexes_to_iterate = np.argwhere(mask)\n    group_id = 0\n\n    for index in indexes_to_iterate:\n        if result_buffer[index[0], index[1]] != no_group_value:\n            continue\n\n        group_id += 1\n        check_buffer[:] = False\n        adjacent_indexes = _all_adjacent_indexes(\n            index,\n            add_diagonal=add_diagonal,\n            add_double_lat=add_double_lat,\n            add_double_lon=add_double_lon,\n            add_90NS_bound=add_90NS_bound,\n            len_lat=len_x,\n            len_lon=len_y,\n        )\n        _indexes_to_2d_buffer(\n            adjacent_indexes,\n            check_buffer,\n            result_buffer,\n            only_if_val=no_group_value,\n        )\n\n        included_another_index = True\n        while included_another_index:\n            included_another_index = False\n            for another_index in np.argwhere(check_buffer):\n                i, j = another_index\n\n                if not mask[i][j]:\n                    continue\n                if result_buffer[i][j] == group_id:\n                    continue\n\n                included_another_index = True\n                result_buffer[i][j] = group_id\n                adjacent_indexes = _all_adjacent_indexes(\n                    another_index,\n                    add_diagonal=add_diagonal,\n                    add_double_lat=add_double_lat,\n                    add_double_lon=add_double_lon,\n                    add_90NS_bound=add_90NS_bound,\n                    len_lat=len_x,\n                    len_lon=len_y,\n                )\n                _indexes_to_2d_buffer(\n                    adjacent_indexes,\n                    check_buffer,\n                    result_buffer,\n                    only_if_val=no_group_value,\n                )\n    return result_buffer\n", "coverage": [1, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, 1, 1, 0, null, null, 1, null, null, null, null, null, null, null, 1, 1, 1, 1, null, 1, null, 1, null, null, null, null, null, 1, 1, null, null, null, null, null, null, null, null, 1, null, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, 1, 1, 1, null, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, null, null, 1, null, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, null, 1, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, 1, null, null, 1, null, null, null, null, null, null, 1, 1, null, null, 1, null, null, null, null, 1, null, null, 1, 1, 0, null, null, 1, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, null, null, null, 1, null, null, 1, null, null, null, null, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1, 0, 1, null, 1, null, null, null, null, 1, 1, 1, 1, null, 1, null, 1, 1, null, 1, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, 1, null, null, 1, null, null, null, null, 1, null, null, 1, 1, null, null, null, 1, 1, 1, null, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, null, null, 1, null, 1, 1, 1, null, 1, 1, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, null, null, 1, null, 1, 1, null, 1, 1, null, 1, null, 1, null, null, 1, 1, null, null, null, null, null, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, 1, null, 1, 1, null, null, null, null, null, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, 1, 1, null, 1, null, 1, 1, 1, null, null, null, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, 1, 0, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, null, 1, 1, 1, null, 1, 1, 1, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, 1, 1, 1, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, 1]}, {"name": "optim_esm_tools/analyze/cmip_handler.py", "source": "import os\nimport typing as ty\nfrom warnings import warn\n\nimport xarray as xr\n\nimport optim_esm_tools as oet\nfrom .globals import _DEFAULT_MAX_TIME\nfrom .globals import _FOLDER_FMT\nfrom optim_esm_tools.analyze import tipping_criteria\n\n\ndef add_conditions_to_ds(\n    ds: xr.Dataset,\n    calculate_conditions: ty.Optional[\n        ty.Tuple[tipping_criteria._Condition, ...]\n    ] = None,\n    condition_kwargs: ty.Optional[ty.Mapping] = None,\n    variable_of_interest: ty.Tuple[str] = ('tas',),\n    _ma_window: ty.Optional[ty.Union[int, str]] = None,\n) -> xr.Dataset:\n    \"\"\"Transform the dataset to get it ready for handling in optim_esm_tools.\n\n    Args:\n        ds (xr.Dataset): input dataset\n        calculate_conditions (ty.Tuple[tipping_criteria._Condition], optional): Calculate the\n            results of these tipping conditions. Defaults to None.\n        condition_kwargs (ty.Mapping, optional): kwargs for the tipping conditions. Defaults to\n            None.\n        variable_of_interest (ty.Tuple[str], optional): Variables to handle. Defaults to ('tas',).\n        _ma_window (int, optional): Moving average window (assumed to be years). Defaults to 10.\n\n    Raises:\n        ValueError: If there are multiple tipping conditions with the same short_description\n\n    Returns:\n        xr.Dataset: The fully initialized dataset\n    \"\"\"\n    _ma_window = _ma_window or oet.config.config['analyze']['moving_average_years']\n    if calculate_conditions is None:\n        calculate_conditions = (\n            tipping_criteria.StartEndDifference,\n            tipping_criteria.StdDetrended,\n            tipping_criteria.StdDetrendedYearly,\n            tipping_criteria.MaxJump,\n            tipping_criteria.MaxJumpYearly,\n            tipping_criteria.MaxDerivitive,\n            tipping_criteria.MaxJumpAndStd,\n            tipping_criteria.SNR,\n        )  # type: ignore\n    if len(set(desc := (c.short_description for c in calculate_conditions))) != len(  # type: ignore\n        calculate_conditions,  # type: ignore\n    ):\n        raise ValueError(\n            f'One or more non unique descriptions {desc}',\n        )  # pragma: no cover\n    if condition_kwargs is None:\n        condition_kwargs = {}\n\n    for variable in oet.utils.to_str_tuple(variable_of_interest):\n        assert calculate_conditions is not None\n        for cls in calculate_conditions:\n            condition = cls(**condition_kwargs, variable=variable, running_mean=_ma_window)  # type: ignore\n            oet.get_logger().debug(\n                f'{condition} from {cls} set ma= {condition.running_mean} ma={_ma_window}',\n            )\n            condition_array = condition.calculate(ds)\n            condition_array = condition_array.assign_attrs(\n                dict(\n                    short_description=cls.short_description,\n                    long_description=condition.long_description,\n                    name=condition_array.name,\n                ),\n            )\n            ds[condition.short_description] = condition_array\n    return ds\n\n\n@oet.utils.add_load_kw\n@oet.utils.timed(_stacklevel=3, _args_max=50)\ndef read_ds(\n    base: str,\n    variable_of_interest: ty.Optional[ty.Tuple[str]] = None,\n    max_time: ty.Optional[ty.Tuple[int, ...]] = _DEFAULT_MAX_TIME,\n    min_time: ty.Optional[ty.Tuple[int, ...]] = None,\n    apply_transform: bool = True,\n    pre_process: bool = True,\n    strict: bool = True,\n    load: ty.Optional[bool] = None,\n    add_history: bool = False,\n    _ma_window: ty.Optional[ty.Union[int, str]] = None,\n    _cache: bool = True,\n    _file_name: ty.Optional[str] = None,\n    _skip_folder_info: bool = False,\n    _historical_path: ty.Optional[str] = None,\n    pre_proc_kw=None,\n    **kwargs,\n) -> ty.Optional[xr.Dataset]:\n    \"\"\"Read a dataset from a folder called \"base\".\n\n    Args:\n        base (str): Folder to load the data from\n        variable_of_interest (ty.Tuple[str], optional): Variables to handle. Defaults to ('tas',).\n        max_time (ty.Optional[ty.Tuple[int, int, int]], optional): Defines time range in which to\n            load data. Defaults to (2100, 12, 31).\n        min_time (ty.Optional[ty.Tuple[int, int, int]], optional): Defines time range in which to\n            load data. Defaults to None.\n        apply_transform: (bool, optional): Apply analysis specific postprocessing algorithms.\n            Defaults to True.\n        pre_process (bool, optional): Should be true, this pre-processing of the data is required\n            later on. Defaults to True.\n        area_query_kwargs (ty.Mapping, optional): additionally keyword arguments for searching.\n        strict (bool, optional): raise errors on loading, if any. Defaults to True.\n        load (bool, optional): apply dataset.load to dataset directly. Defaults to False.\n        add_history (bool, optional): start by merging historical dataset to the dataset.\n        _ma_window (int, optional): Moving average window (assumed to be years). Defaults to 10.\n        _cache (bool, optional): cache the dataset with it's extra fields to allow faster\n            (re)loading. Defaults to True.\n        _file_name (str, optional): name to match. Defaults to configs settings.\n        _skip_folder_info (bool, optional): if set to True, do not infer the properties from the\n            (synda) path of the file\n        _historical_path (str, optional): If add_history is True, load from this (full) path\n\n    kwargs:\n        any kwargs are passed onto transform_ds.\n\n    Returns:\n        xr.Dataset: An xarray dataset with the appropriate variables\n    \"\"\"\n    log = oet.config.get_logger()\n    _file_name = _file_name or oet.config.config['CMIP_files']['base_name']\n    _ma_window = _ma_window or int(oet.config.config['analyze']['moving_average_years'])\n    data_path = os.path.join(base, _file_name)\n    variable_of_interest = (\n        variable_of_interest or oet.analyze.pre_process._read_variable_id(data_path)\n    )\n    _historical_path = _historical_file(add_history, base, _file_name, _historical_path)\n\n    if not isinstance(variable_of_interest, str):\n        raise ValueError('Only single vars supported')  # pragma: no cover\n    if kwargs:\n        log.error(f'Not really advised yet to call with {kwargs}')\n        _cache = False\n    if not apply_transform and strict:  # pragma: no cover\n        # Don't cache the partial ds\n        _cache = False\n\n    log.debug(f'read_ds {variable_of_interest}')\n    res_file = _name_cache_file(\n        base,\n        variable_of_interest,\n        min_time,\n        max_time,\n        _ma_window,\n        is_historical=_historical_path is not None,\n    )\n\n    if os.path.exists(res_file) and _cache:\n        return oet.analyze.io.load_glob(res_file)\n\n    if not os.path.exists(data_path):  # pragma: no cover\n        message = f'No dataset at {data_path}'\n        if strict:\n            raise FileNotFoundError(message)\n        log.warning(message)\n        return None\n\n    if pre_process:\n        pre_proc_kw = pre_proc_kw or dict()\n        data_set = oet.analyze.pre_process.get_preprocessed_ds(\n            sources=data_path,\n            historical_path=_historical_path,\n            max_time=max_time,\n            min_time=min_time,\n            _ma_window=_ma_window,\n            variable_id=variable_of_interest,\n            **pre_proc_kw,\n        )\n    else:  # pragma: no cover\n        message = 'Not preprocessing file is dangerous, dimensions may differ wildly!'\n        if strict:\n            raise ValueError(message)\n        log.warning(message)\n        data_set = oet.analyze.io.load_glob(data_path, load=load)\n\n    if apply_transform:\n        kwargs.update(\n            dict(\n                variable_of_interest=variable_of_interest,\n                _ma_window=_ma_window,\n            ),\n        )\n        data_set = add_conditions_to_ds(data_set, **kwargs)\n\n    # start with -1 (for i==0)\n    metadata = (\n        {} if _skip_folder_info else oet.analyze.find_matches.folder_to_dict(base)\n    )\n    metadata.update(dict(path=base, file=res_file, running_mean_period=_ma_window))  # type: ignore\n    if _historical_path:\n        metadata.update(dict(historical_file=_historical_path))\n\n    data_set.attrs.update(metadata)\n\n    if _cache:\n        log.info(f'Write {res_file}')\n        if oet.config.config['CMIP_files']['compress'] == 'True':\n            oet.analyze.pre_process.save_nc(data_set, res_file)\n        else:\n            data_set.to_netcdf(res_file)\n\n    return data_set\n\n\ndef _historical_file(\n    add_history: bool,\n    base: str,\n    _file_name: str,\n    _historical_path: ty.Optional[str],\n) -> ty.Optional[str]:\n    if add_history:\n        if _historical_path is not None:\n            return _historical_path\n        historical_heads = oet.analyze.find_matches.associate_historical(\n            path=base,\n            match_to='historical',\n            strict=False,\n        )\n        if not historical_heads:\n            raise FileNotFoundError(f'No historical matches for {base}')\n        _historical_path = os.path.join(historical_heads[0], _file_name)\n        if not os.path.exists(_historical_path):  # pragma: no cover\n            raise ValueError(\n                f'{_historical_path} not found, (check {historical_heads}?)',\n            )\n    elif _historical_path:\n        raise ValueError(\n            f'Wrong input _historical_path is {_historical_path} but add_history is False',\n        )\n    return _historical_path\n\n\ndef _name_cache_file(\n    base,\n    variable_of_interest: str,\n    min_time: ty.Optional[ty.Tuple[int, ...]],\n    max_time: ty.Optional[ty.Tuple[int, ...]],\n    _ma_window: int,\n    is_historical: bool,\n    version: ty.Optional[str] = None,\n) -> str:\n    \"\"\"Get a file name that identifies the settings.\"\"\"\n    version = version or oet.config.config['versions']['cmip_handler']\n    _ma_window = _ma_window or int(oet.config.config['analyze']['moving_average_years'])\n    path = os.path.join(\n        base,\n        f'{variable_of_interest}'\n        f'_s{tuple(min_time) if min_time else \"\"}'\n        f'_e{tuple(max_time) if max_time else \"\"}'\n        f'_ma{_ma_window}'\n        + ('_hist' if is_historical else '')\n        + f'_optimesm_v{version}.nc',\n    )\n    normalized_path = (\n        path.replace('(', '')\n        .replace(')', '')\n        .replace(']', '')\n        .replace('[', '')\n        .replace(' ', '_')\n        .replace(',', '')\n        .replace('\\'', '')\n    )\n    oet.config.get_logger().debug(f'got {normalized_path}')\n    return normalized_path\n", "coverage": [1, 1, 1, null, 1, null, 1, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, 1, 1, null, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, null, null, null, null, 1, 1, null, null, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, null, 1, null, 1, null, 1, 0, 0, null, null, null, null, 1, 1, null, null, null, null, null, null, null, null, 1, 1, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, null, null, null, null, 1, null, null, 1, null, null, 1, 1, 1, null, 1, null, 1, 1, 1, 1, null, 0, null, 1, null, null, 1, null, null, null, null, null, 1, 1, 1, 1, null, null, null, null, 1, 0, 1, null, null, null, null, 1, 1, null, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, 1, 1]}, {"name": "optim_esm_tools/analyze/combine_variables.py", "source": "import string\nimport typing as ty\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom immutabledict import immutabledict as imdict\nfrom matplotlib.legend_handler import HandlerTuple\n\nimport optim_esm_tools as oet\n\n\nclass VariableMerger:\n    \"\"\"The `VariableMerger` class is used to merge and process variables from\n    multiple datasets, applying masks and generating visualizations.\"\"\"\n\n    full_paths: ty.Optional[ty.Iterable] = None\n    source_files: ty.Mapping\n    common_mask: ty.Union[xr.DataArray, ty.Mapping[str, xr.DataArray]]\n\n    _independent_cmaps: imdict = imdict(\n        zip(\n            ['siconc', 'sos', 'tas', 'tos', 'msftbarot'],\n            ['Blues_r', 'Greens_r', 'Reds_r', 'Purples_r', 'Oranges_r'],\n        ),\n    )\n    _independent_legend_kw: imdict = imdict(\n        numpoints=1,\n        handler_map={tuple: HandlerTuple(ndivide=None, pad=0)},\n        **oet.utils.legend_kw(ncol=2),\n    )\n    _contour_f_kw: imdict = imdict(alpha=0.5)\n\n    def __init__(\n        self,\n        data_set: ty.Optional[xr.Dataset] = None,\n        paths: ty.Optional[ty.Iterable[str]] = None,\n        other_paths: ty.Optional[ty.Iterable[str]] = None,\n        merge_method: str = 'logical_or',\n        tipping_thresholds: ty.Optional[ty.Mapping] = None,\n        table_formats: ty.Optional[ty.Dict[str, str]] = None,\n        use_cftime: bool = True,\n        load: bool = True,\n    ) -> None:\n        if data_set is None:\n            assert paths, \"Dataset specified, don't give paths\"\n        else:\n            assert not paths, 'Dataset not specified, give paths!'  # pragma: no cover\n        self.data_set = data_set\n        self.load = load\n        self.mask_paths = paths\n        self.other_paths = other_paths or []\n\n        self.merge_method = merge_method\n        self.tipping_thresholds = tipping_thresholds\n        self.table_formats = table_formats\n        self.use_cftime = use_cftime\n        if data_set:\n            self.source_files = dict(\n                zip(\n                    oet.utils.to_str_tuple(data_set.attrs['variables']),\n                    oet.utils.to_str_tuple(data_set.attrs['source_files']),\n                ),\n            )\n\n            self.common_mask = {\n                k[len('global_mask_') :]: data_set[k]\n                for k in list(data_set.data_vars)\n                if k.startswith('global_mask_')\n            }\n            self.common_mask.update(dict(common_mask=data_set['common_mask']))\n            return  # pragma: no cover\n        source_files, common_mask = self.process_masks()\n        self.source_files = source_files\n        self.common_mask = common_mask\n\n    def squash_sources(self) -> xr.Dataset:\n        if self.data_set:\n            return self.data_set  # pragma: no cover\n        new_ds = self._squash_variables()\n        new_ds = self._merge_squash(new_ds)\n        self.data_set = new_ds\n        return new_ds\n\n    def get_common_mask(self, variable_id: ty.Optional[str] = None) -> xr.DataArray:\n        assert isinstance(self.common_mask, ty.Mapping)\n        if variable_id and variable_id in self.common_mask:\n            return self.common_mask[variable_id]\n        assert 'common_mask' in self.common_mask, self.common_mask.keys()\n        return self.common_mask['common_mask']\n\n    def _squash_variables(self, common_mask=None) -> ty.Mapping:\n        common_mask = common_mask or self.common_mask\n\n        new_ds: ty.Mapping[str, ty.Any] = defaultdict(dict)\n        if isinstance(common_mask, xr.DataArray):\n            new_ds['data_vars']['common_mask'] = common_mask\n        else:\n            assert isinstance(common_mask, ty.Mapping), type(common_mask)\n            shared_mask = None\n            for var, mask in common_mask.items():\n                new_ds['data_vars'][f'global_mask_{var}'] = mask\n                if shared_mask is None:\n                    shared_mask = mask.astype(np.bool_).copy()\n                    continue\n                shared_mask = mask.astype(np.bool_) | shared_mask\n            new_ds['data_vars']['common_mask'] = shared_mask\n        for var, path in self.source_files.items():\n            _ds = oet.load_glob(path, load=self.load)\n            _ds['time'] = [int(d.year) for d in _ds['time'].values]\n            for sub_variable in list(_ds.data_vars):\n                if var not in sub_variable:\n                    continue\n                data = _ds[sub_variable].values\n                mask = self.get_common_mask(var).values.astype(bool)\n                weights = _ds['cell_area'].values\n                if len(data.shape) == 3 and all(\n                    k in _ds[sub_variable].coords\n                    for k in oet.config.config['analyze']['lon_lat_dim'].split(',')\n                ):\n                    data[:, ~mask] = np.nan\n                    mean_values = oet.analyze.tools._weighted_mean_array_numba(\n                        data=data,\n                        weights=weights,\n                        has_time_dim=True,\n                    )\n                    new_ds['data_vars'][sub_variable] = xr.DataArray(\n                        mean_values,\n                        dims=['time'],\n                        attrs=_ds[sub_variable].attrs,\n                    )\n                elif len(data.shape) == 2 and all(\n                    k in _ds[sub_variable].coords\n                    for k in oet.config.config['analyze']['lon_lat_dim'].split(',')\n                ):\n                    data[~mask] = np.nan\n                    mean_values = oet.analyze.tools._weighted_mean_array_numba(\n                        data=data,\n                        weights=weights,\n                        has_time_dim=False,\n                    )\n                    new_ds['data_vars'][sub_variable] = xr.DataArray(\n                        mean_values,\n                        attrs=_ds[sub_variable].attrs,\n                    )\n                else:\n                    raise ValueError(\n                        f'sub_variable: {data.shape} {_ds[sub_variable].coords}',\n                    )\n\n        # Make one copy - just use the last dataset\n        new_ds['data_vars']['cell_area'] = _ds['cell_area']\n        new_ds['data_vars']['time'] = _ds['time']\n        keys = sorted(list(self.source_files.keys()))\n        new_ds['attrs'] = dict(  # type: ignore\n            variables=keys,\n            source_files=[self.source_files[k] for k in keys],\n            mask_files=sorted(self.mask_paths),  # type: ignore\n            paths=self.mask_paths,\n            other_paths=self.other_paths,\n        )\n        return new_ds\n\n    def _merge_squash(self, new_ds_kw: ty.Dict[str, ty.Any]) -> xr.Dataset:\n        try:\n            new_ds = xr.Dataset(**new_ds_kw)\n        except TypeError as e:  # pragma: no cover\n            oet.get_logger().warning(f'Ran into {e} fallback method because of cftime')\n            # Stupid cftime can't compare it's own formats\n            # But xarray can fudge something along the way!\n            data_vars = new_ds_kw.pop('data_vars')\n            new_ds = xr.Dataset(**new_ds_kw)\n\n            for k, v in data_vars.items():\n                if 'time' in new_ds.coords and 'time' in v.coords:\n                    new_ds[k] = ('time', v.values)\n                    new_ds[k].attrs = v.attrs\n                else:\n                    new_ds[k] = v\n        except ValueError as e:  # pragma: no cover\n            oet.get_logger().warning(\n                f'Ran into {e} fallback method because of duplicated time stamps',\n            )\n            data_vars = new_ds_kw.pop('data_vars')\n            new_ds = xr.Dataset(**new_ds_kw)\n\n            for k, v in data_vars.items():\n                if 'time' in new_ds.coords and 'time' in v.coords:\n                    v = v.drop_duplicates('time')\n                    new_ds[k] = ('time', v.values)\n                    new_ds[k].attrs = v.attrs\n                else:\n                    new_ds[k] = v\n        if self.use_cftime:\n            import cftime\n\n            new_ds['time'] = [cftime.DatetimeNoLeap(y, 7, 1) for y in new_ds['time']]\n            dt = new_ds['time'].values[-1].year - new_ds['time'].values[0].year\n            if len(new_ds['time']) > dt + 1:\n                raise ValueError('Got more years than dates.')\n        return new_ds\n\n    def make_fig(\n        self,\n        ds: ty.Optional[xr.Dataset] = None,\n        fig_kw: ty.Optional[ty.Mapping] = None,\n        add_histograms: bool = False,\n        add_history: bool = True,\n        add_summary: bool = True,\n        _historical_ds: ty.Optional[xr.Dataset] = None,\n        **kw,\n    ) -> plt.Axes:\n        # sourcery skip: merge-repeated-ifs, move-assign\n        ds = ds or self.squash_sources()\n        if add_history:\n            kw.setdefault('set_y_lim', False)\n\n        axes = self._make_fig(\n            ds,\n            fig_kw=fig_kw,\n            add_histograms=add_histograms,\n            add_summary=add_summary,\n            **kw,\n        )\n        if add_history:\n            kw.pop('add_summary', None)\n            self._add_historical_period(axes, _historical_ds=_historical_ds, **kw)\n        if self.merge_method == 'logical_or':\n            axes = self._continue_global_map(axes, ds=ds)\n        if self.merge_method == 'independent':\n            axes = self._continue_indepentent_var_figure(axes, ds=ds)\n        if add_summary:\n            summary = self.summarize_stats(ds)\n            res_f, tips = result_table(\n                summary,\n                thresholds=self.tipping_thresholds,\n                formats=self.table_formats,\n            )\n            self.add_table(\n                res_f=res_f,\n                tips=tips,\n                summary=summary,\n                ax=axes['t'],  # type: ignore\n                ha='center' if add_histograms else 'bottom',\n            )\n        return axes\n\n    @staticmethod\n    def _guess_fig_kw(keys: ty.List, add_histograms=False) -> ty.Dict[str, ty.Any]:\n        if add_histograms:\n            return dict(\n                mosaic=''.join(f'{k}.\\n' for k in keys),\n                figsize=(17, 5 * ((2 + len(keys)) / 3)),\n                gridspec_kw=dict(width_ratios=[1, 0.5, 1.5], wspace=0.1, hspace=0.4),\n            )\n        return dict(\n            mosaic=''.join(f'{k}.\\n' for k in keys),\n            figsize=(17, 4 * ((2 + len(keys)) / 3)),\n            gridspec_kw=dict(width_ratios=[1, 1], wspace=0.1, hspace=0.05),\n        )\n\n    def _make_fig(\n        self,\n        ds: xr.Dataset,\n        fig_kw: ty.Optional[ty.Mapping] = None,\n        add_histograms: bool = False,\n        add_summary: bool = False,\n        **kw,\n    ) -> plt.Axes:\n        variables = list(oet.utils.to_str_tuple(ds.attrs['variables']))\n        mapping = {string.ascii_lowercase[i]: v for i, v in enumerate(variables)}\n        keys = (\n            [f'{k}{k.upper()}' for k in mapping] + (['tt'] if add_summary else [])\n            if add_histograms\n            else list(mapping) + (['t'] if add_summary else [])\n        )\n        fig_kw = fig_kw or self._guess_fig_kw(keys, add_histograms)\n\n        _, axes = plt.subplot_mosaic(**fig_kw)\n\n        for old_key, new_key in mapping.items():\n            axes[new_key] = axes.pop(old_key)\n\n        if len(variables) > 1:\n            for k in variables[1:]:\n                axes[k].sharex(axes[variables[0]])  # type: ignore\n\n        for var in variables:\n            plt.sca(axes[var])  # type: ignore\n            plot_kw = dict(label=var, **kw)\n            rm_kw = {\n                k: v\n                for k, v in {\n                    **plot_kw,\n                    **dict(alpha=0.5, add_label=False, set_y_lim=False),\n                }.items()\n                if k != 'label'\n            }\n            var_rm = (\n                var\n                + '_run_mean_'\n                + oet.config.config['analyze']['moving_average_years']\n            )\n            oet.plotting.map_maker.plot_simple(ds, var_rm, **rm_kw)\n            oet.plotting.map_maker.plot_simple(ds, var, **plot_kw)  # type: ignore\n            plt.legend(loc='center left')\n            if add_histograms:\n                plt.sca(axes[var.upper()])  # type: ignore\n                hist_kw = dict(bins=25, range=[np.nanmin(ds[var]), np.nanmax(ds[var])])\n                self.simple_hist(ds, var, hist_kw=hist_kw)\n                self.simple_hist(ds, var_rm, hist_kw=hist_kw, add_label=False)\n\n        return axes\n\n    def _continue_global_map(\n        self,\n        axes: ty.Dict[str, plt.Axes],\n        ds: xr.Dataset,\n    ) -> ty.Dict[str, plt.Axes]:\n        ax = plt.gcf().add_subplot(\n            1,\n            2,\n            2,\n            projection=oet.plotting.plot.get_cartopy_projection(),\n        )\n        oet.plotting.map_maker.overlay_area_mask(\n            ds.where(self.get_common_mask()).copy(),\n            ax=ax,\n        )\n        axes['global_map'] = ax  # type: ignore\n        return axes\n\n    def _continue_indepentent_var_figure(\n        self,\n        axes: ty.Dict[str, plt.Axes],\n        ds: xr.Dataset,\n        ax: ty.Optional[plt.Axes] = None,\n        skip_common: bool = True,\n    ) -> ty.Dict[str, plt.Axes]:\n        ax = ax or plt.gcf().add_subplot(\n            1,\n            2,\n            2,\n            projection=oet.plotting.plot.get_cartopy_projection(),\n        )\n        plt.gca().coastlines()\n        gl = ax.gridlines(draw_labels=True)\n        gl.top_labels = False\n\n        legend_args = []\n        for variable, mask in self.common_mask.items():\n            if skip_common and variable == 'common_mask':\n                continue\n            artists = mask.astype(int).plot.contour(\n                cmap=self._independent_cmaps.get(variable, 'viridis'),\n                transform=oet.plotting.plot.get_cartopy_transform(),\n                **self._contour_f_kw,\n            )\n            artists, _ = artists.legend_elements()\n            for line in artists:\n                line.set_linewidth(10)\n            legend_args.append(tuple(artists))\n\n        def get_area(k):\n            area = float(ds['cell_area'].where(ds[f'global_mask_{k}']).sum() / 1e6)\n            exp = int(np.log10(area))  # type: ignore\n            return f'{k} -- ${area/(10**exp):.1f}\\\\times10^{{{exp}}}$ km$^2$'\n\n        labels = [get_area(k) for k in self.common_mask.keys() if k != 'common_mask']\n        plt.legend(\n            legend_args,\n            labels,\n            **self._independent_legend_kw,\n        )\n        axes['global_map'] = ax\n        return axes\n\n    @staticmethod\n    def simple_hist(\n        ds,\n        var: str,\n        hist_kw: ty.Optional[ty.Dict[str, ty.Any]] = None,\n        add_label: bool = True,\n        **plot_kw,\n    ) -> None:\n        da = ds[var]\n        hist_kw = hist_kw or dict(\n            bins=25,\n            range=[np.nanmin(da.values), np.nanmax(da.values)],\n        )\n        x, y, _ = histogram(da, **hist_kw)\n        for k, v in dict(ls='-', drawstyle='steps-mid').items():\n            plot_kw.setdefault(k, v)\n        plt.errorbar(x, y, **plot_kw)\n        if add_label:\n            plt.xlabel(\n                f'{oet.plotting.plot.default_variable_labels().get(var, var)} [{oet.plotting.plot.get_unit_da(da)}]',\n            )\n\n    def summarize_stats(self, ds: xr.Dataset) -> ty.Dict[str, ty.Dict[str, ty.Any]]:\n        return {\n            field: summarize_stats(ds=ds, field=field, path=path)\n            for field, path in zip(\n                oet.utils.to_str_tuple(ds.attrs['variables']),\n                oet.utils.to_str_tuple(ds.attrs['source_files']),\n            )\n        }\n\n    def _check_mask_coord_names(\n        self,\n        mask: ty.Union[ty.Mapping, np.ndarray, xr.DataArray],\n    ) -> ty.Union[ty.Mapping, np.ndarray, xr.DataArray]:\n        if isinstance(mask, ty.Mapping):\n            return {k: self._check_mask_coord_names(v) for k, v in mask.items()}\n        if isinstance(mask, xr.DataArray) and mask.dims != ('lat', 'lon'):\n            return oet.analyze.xarray_tools.reverse_name_mask_coords(mask)\n        return mask\n\n    def process_masks(self) -> ty.Tuple[dict, ty.Union[ty.Mapping, xr.DataArray]]:\n        source_files = {}\n        variable_masks: dict = {}\n        for path in self.mask_paths:  # type: ignore\n            ds = oet.load_glob(path, load=self.load)\n            variable_id = ds.attrs['variable_id']\n            # Source files may be non-unique!\n            source_files[variable_id] = ds.attrs['file']\n\n            variable_masks[variable_id] = self.combine_masks(\n                variable_masks.get(variable_id),\n                ds,\n                dtype=np.int64,\n            )\n\n        shared_mask = None\n        for var_mask in variable_masks.values():\n            if shared_mask is None:\n                shared_mask = var_mask.copy()\n            else:\n                shared_mask |= var_mask\n        variable_masks['common_mask'] = shared_mask\n        if self.merge_method == 'logical_or':\n            # Each variable did get it's own mask - but that is not what we want.\n            variable_masks = dict(common_mask=shared_mask)\n        for other_path in self.other_paths:\n            if other_path == '':  # pragma: no cover\n                continue\n            ds = oet.load_glob(other_path, load=self.load)\n            # Source files may be non-unique!\n            var = ds.attrs['variable_id']\n            if var not in source_files:\n                source_files[var] = ds.attrs['file']\n        assert isinstance(variable_masks, ty.Mapping)\n        return source_files, variable_masks\n\n    def combine_masks(\n        self,\n        common_mask: ty.Optional[xr.DataArray],\n        other_dataset: xr.Dataset,\n        field: ty.Optional[str] = None,\n        dtype: type = np.bool_,\n    ) -> xr.DataArray:\n        field = field or (\n            'global_mask' if 'global_mask' in other_dataset else 'cell_area'\n        )\n        is_the_first_instance = common_mask is None\n        other_mask = self._check_mask_coord_names(other_dataset[field])\n        if not isinstance(other_mask, xr.DataArray):\n            raise TypeError(f'Expected xr.DataArray, got {type(other_mask)}')\n        if is_the_first_instance:\n            return other_mask.astype(dtype)\n\n        if not isinstance(common_mask, xr.DataArray):\n            raise TypeError(f'Expected xr.DataArray, got {type(common_mask)}')\n\n        if self.merge_method == 'logical_or':\n            return common_mask | other_mask.astype(dtype)\n        elif self.merge_method == 'independent':\n            return common_mask.astype(dtype) + other_mask.astype(dtype)\n        raise NotImplementedError(\n            f'No such method as {self.merge_method}',\n        )  # pragma: no cover\n\n    def add_table(self, *a, **kw):\n        return add_table(*a, **kw)\n\n    def _add_historical_period(\n        self,\n        axes: ty.Mapping[str, plt.Axes],\n        match_to: str = 'historical',\n        read_ds_kw: ty.Optional[ty.Mapping] = None,\n        _historical_ds: ty.Optional[xr.Dataset] = None,\n        **plot_kw,\n    ) -> None:\n        if _historical_ds is None:\n            raise NotImplementedError\n        plot_kw.setdefault('lw', 1)\n        plot_kw.setdefault('add_label', False)\n        read_ds_kw = read_ds_kw or {}\n\n        for var, path in self.source_files.items():\n            historical_ds = (\n                _historical_ds\n                or oet.analyze.time_statistics.get_historical_ds(\n                    oet.load_glob(path, load=self.load),\n                    match_to=match_to,\n                )\n            )\n            common_mask = self.get_common_mask(var)\n            historical_ds = historical_ds.where(common_mask)\n\n            plt.sca(axes[var])\n            rm_kw = {\n                k: v\n                for k, v in {\n                    **plot_kw,\n                    **dict(alpha=0.5, set_y_lim=False),\n                }.items()\n                if k != 'label'\n            }\n            var_rm = (\n                var\n                + '_run_mean_'\n                + oet.config.config['analyze']['moving_average_years']\n            )\n            oet.plotting.map_maker.plot_simple(historical_ds, var_rm, **rm_kw)\n            oet.plotting.map_maker.plot_simple(historical_ds, var, **plot_kw)\n\n\ndef histogram(d: np.ndarray, **kw) -> ty.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    c, be = np.histogram(d, **kw)\n    return (be[1:] + be[:-1]) / 2, c, be[1] - be[0]\n\n\ndef change_plt_table_height(increase_by: float = 1.5) -> None:\n    \"\"\"Increase the height of rows in plt.table.\n\n    Unfortunately, the options that you can pass to plt.table are insufficient to render a table\n    that has rows with sufficient heights that work with a font that is not the default. From the\n    plt.table implementation, I figured I could change these (rather patchy) lines in the source\n    code:\n    https://github.com/matplotlib/matplotlib/blob/b7dfdc5c97510733770429f38870a623426d0cdc/lib/matplotlib/table.py#L391\n\n    Matplotlib version matplotlib==3.7.2\n    \"\"\"\n    import matplotlib\n\n    print('Change default plt.table row height')\n\n    def _approx_text_height(self):\n        return increase_by * (\n            self.FONTSIZE / 72.0 * self.figure.dpi / self._axes.bbox.height * 1.2\n        )\n\n    matplotlib.table.Table._approx_text_height = _approx_text_height  # type: ignore\n\n\ndef _always_false(*a):\n    return False\n\n\ndef add_table(\n    res_f: pd.DataFrame,\n    tips: pd.Series,\n    ax: ty.Optional[plt.Axes] = None,\n    fontsize: int = 16,\n    pass_color: ty.Tuple[float, ...] = (0.75, 1.0, 0.75),\n    ha: str = 'bottom',\n    summary: None = None,\n):\n    ax = ax or plt.gcf().add_subplot(2, 2, 4)\n    ax.axis('off')\n    ax.axis('tight')\n\n    table = ax.table(\n        cellText=res_f.values,\n        rowLabels=res_f.index,\n        colLabels=res_f.columns,\n        cellColours=[\n            [(pass_color if v else [1, 1, 1]) for v in row] for row in tips.values\n        ],\n        loc=ha,\n        colLoc='center',\n        rowLoc='center',\n        cellLoc='center',\n    )\n    table.set_fontsize(fontsize)\n\n\ndef result_table(\n    res: ty.Mapping[str, ty.Any],\n    thresholds: ty.Optional[ty.Mapping] = None,\n    formats: ty.Optional[ty.Mapping] = None,\n):\n    thresholds = thresholds or {}\n    is_tip = pd.DataFrame(\n        {\n            k: {\n                t: (\n                    thresholds.get(t, [_always_false])[0](\n                        v,\n                        thresholds.get(t, [None, None])[1],\n                    )\n                    if v is not None\n                    else False\n                )\n                for t, v in d.items()\n            }\n            for k, d in res.items()\n        },\n    ).T\n\n    formats = formats or dict(\n        n_breaks='.0f',\n        p_symmetry='.2%',\n        p_dip='.1%',\n        max_jump='.1f',\n        n_std_global='.1f',\n    )\n    res_f = pd.DataFrame(res).T\n    for k, f in formats.items():\n        if k not in res_f.keys():\n            continue\n        res_f[k] = res_f[k].map(f'{{:,{f}}}'.format)\n\n    order = [o for o in formats.keys() if o in res_f.keys()]\n    return res_f[order], is_tip[order]\n\n\ndef summarize_stats(\n    ds: xr.Dataset,\n    field: str,\n    path: str,\n) -> ty.Dict[str, ty.Union[int, float]]:\n    return {\n        'n_breaks': oet.analyze.time_statistics.calculate_n_breaks(ds, field=field),\n        'p_symmetry': oet.analyze.time_statistics.calculate_symmetry_test(\n            ds,\n            field=field,\n        ),\n        'p_dip': oet.analyze.time_statistics.calculate_dip_test(ds, field=field),\n    }\n\n\nif __name__ == '__main__':\n    change_plt_table_height()\n", "coverage": [1, 1, 1, null, 1, 1, 1, 1, 1, 1, null, 1, null, null, 1, null, null, null, 1, 1, 1, null, 1, null, null, null, null, null, 1, null, null, null, null, 1, null, 1, null, null, null, null, null, null, null, null, null, null, 1, 1, null, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, null, null, null, null, null, null, 1, null, null, null, null, 1, null, 1, 1, 1, null, 1, 1, null, 1, 1, 1, 1, null, 1, 1, 1, 0, 1, 1, null, 1, 1, null, 1, 1, 0, null, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, null, 1, 1, null, null, null, null, 1, null, null, null, null, 1, null, null, null, 1, 1, null, null, null, null, 1, null, null, null, null, 0, null, null, null, null, 1, 1, 1, 1, null, null, null, null, null, null, 1, null, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, 0, 1, null, 1, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, 1, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, null, null, 1, null, null, null, null, null, null, 1, null, 1, 1, 1, 1, null, null, null, null, 1, null, null, null, null, null, 1, null, null, null, null, null, null, null, 1, 1, 1, null, null, null, null, 1, null, 1, null, 1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, null, null, null, null, null, null, null, 1, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, null, 1, null, null, null, null, 1, null, null, null, null, null, 1, null, null, null, 1, 1, null, 1, null, null, null, null, null, null, 1, null, null, null, null, null, 1, 1, 1, null, 1, 1, 1, 1, 0, null, null, null, null, 0, 0, 0, 0, null, 1, 0, 0, 0, null, 1, 1, null, null, null, null, 1, 1, null, 1, 1, null, null, null, null, null, null, 1, 1, null, null, null, 1, 1, 1, 1, 1, 1, null, null, null, 1, 1, null, null, null, null, null, null, null, 1, null, null, null, 1, 0, 1, 0, 1, null, 1, 1, 1, 1, 1, 1, null, 1, null, 1, null, null, null, null, null, 1, 1, 1, 1, null, 1, 1, 1, null, 1, 1, null, null, 1, null, 1, 1, 1, 1, 1, null, 1, null, null, null, null, null, null, 1, null, null, 1, 1, 1, 0, 1, 1, null, 0, 0, null, 0, 0, 0, 0, null, null, null, null, 1, 1, null, 1, null, null, null, null, null, null, null, 1, 0, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, 1, 1, null, 1, 1, null, null, null, null, null, null, null, 1, null, null, null, null, 1, 1, null, null, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, 1, null, 1, null, 1, 1, null, null, null, 1, null, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, 1, null, null, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, 1, 1, null, null, 1, null, null, null, null, 1, null, null, null, null, null, null, null, null, null, null, null]}, {"name": "optim_esm_tools/analyze/concise_dataframe.py", "source": "import typing as ty\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nimport optim_esm_tools as oet\n\n\nclass ConciseDataFrame:\n    delimiter: str = ', '\n    merge_postfix: str = '(s)'\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        group: ty.Optional[ty.Iterable] = None,\n        tqdm: bool = False,\n        match_overlap: bool = True,\n        sort_by: ty.Union[str, ty.Tuple] = (\n            'tips',\n            'institution_id',\n            'source_id',\n            'experiment_id',\n        ),\n        match_by: ty.Iterable = ('institution_id', 'source_id', 'experiment_id'),\n        min_frac_overlap: float = 0.33,\n        eager_mode=True,\n        disable_doubles: ty.Optional[ty.Iterable[str]] = None,\n    ) -> None:\n        # important to sort by tips == True first! As in match_rows there is a line that assumes\n        # that all tipping rows are already merged!\n\n        self.df = df.copy().sort_values(\n            by=list(oet.utils.to_str_tuple(sort_by)),\n            ascending=False,\n        )\n        self.group = group or (set(self.df.columns) - set(match_by))\n        self.match_overlap = match_overlap\n        self.tqdm = tqdm\n        self.min_frac_overlap = min_frac_overlap\n        self.eager_mode = eager_mode\n        self.disable_doubles = disable_doubles\n\n    def concise(self) -> pd.DataFrame:\n        rows = [row.to_dict() for _, row in self.df.iterrows()]\n        matched_rows = self.match_rows(rows)  # type: ignore\n        combined_rows = [self.combine_rows(r, self.delimiter) for r in matched_rows]\n        df_ret = pd.DataFrame(combined_rows)\n        return self.rename_columns_with_plural(df_ret)\n\n    def rename_columns_with_plural(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add postfix to columns from the dataframe.\"\"\"\n        rename_dict = {k: f'{k}{self.merge_postfix}' for k in self.group}\n        return df.rename(columns=rename_dict)\n\n    @staticmethod\n    def combine_rows(rows: ty.Mapping, delimiter: str) -> ty.Dict[str, str]:\n        ret = {}\n        for k in rows[0].keys():\n            try:\n                vals = list({r[k] for r in rows})\n                val = sorted(vals)\n            except TypeError:\n                val = sorted({str(v) for v in vals})\n            ret[k] = val[0] if len(val) == 1 else delimiter.join([str(v) for v in val])\n        return ret\n\n    _mask_cache = None\n\n    def overlaps_enough(self, path1, path2, use_field='global_mask'):\n        self._mask_cache = self._mask_cache or {}\n        for path in path1, path2:\n            if path not in self._mask_cache:\n                self._mask_cache[path] = oet.load_glob(path)[use_field].values\n        result = (\n            self.overlaps_percent(self._mask_cache[path1], self._mask_cache[path2])\n            >= self.min_frac_overlap\n        )\n        if not self.eager_mode:\n            self._mask_cache = None\n        return result\n\n    @staticmethod\n    def overlaps_percent(arr1, arr2):\n        return np.sum(arr1 & arr2) / min(np.sum(arr1), np.sum(arr2))\n\n    def _row_is_double(self, row: pd.Series, other_row: pd.Series) -> bool:\n        if not self.disable_doubles:\n            return False\n\n        return any(\n            row.get(d, 'no d?') == other_row.get(d, 'also no?')\n            for d in oet.utils.to_str_tuple(self.disable_doubles)\n        )\n\n    def _should_append_to_group(\n        self,\n        group: ty.List[pd.Series],\n        other_row: pd.Series,\n    ) -> bool:\n        return (not self.match_overlap) or (\n            any(\n                self.overlaps_enough(r['path'], other_row['path'])\n                for r in group\n                if r['tips']\n            )\n        )\n\n    def match_rows(self, rows):\n        df = pd.DataFrame(rows)\n        match = sorted(set(df.columns) - set(self.group))\n\n        groups = []\n        for row in oet.utils.tqdm(rows, desc='rows', disable=self.tqdm):\n            if any(row in g for g in groups):\n                continue\n\n            groups.append([row])\n            mask = np.ones(len(df), dtype=np.bool_)\n            for m in match:\n                mask &= df[m] == row[m]\n            for other_row in oet.utils.tqdm(\n                [row for m, row in zip(mask, rows) if m],\n                desc='subrows',\n                disable=self.tqdm,\n            ):\n                if row == other_row:\n                    continue\n\n                if self._row_is_double(row, other_row):\n                    continue\n\n                if self._should_append_to_group(groups[-1], other_row):\n                    groups[-1].append(other_row)\n        return groups\n", "coverage": [1, null, 1, 1, 1, null, 1, null, null, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, null, 1, null, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, null, 1, null, 1, 1, 1, 1, 1, 1, null, null, null, 1, 0, 1, null, 1, 1, 1, null, 1, 1, 1, null, 0, null, null, null, null, 1, null, null, null, null, 1, null, null, null, null, null, null, null, 1, 1, 1, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, null, null, null, null, 1, 1, null, 1, 0, null, 1, 1, 1]}, {"name": "optim_esm_tools/analyze/discontinuous_grid_patcher.py", "source": "import typing as ty\n\nimport numpy as np\nimport optim_esm_tools as oet\nimport xarray as xr\nfrom statsmodels.stats.weightstats import DescrStatsW\n\n\nclass DiscontinuousGridPatcher:\n    min_samples_for_issue: int = 75\n    max_lon_weighted_std: ty.Union[float, int] = 4  # degrees, longitude\n\n    def __init__(\n        self,\n        ds: xr.Dataset,\n        should_have_data_mask: xr.DataArray,\n        iter_time: bool = False,\n        build_cluster_kw: ty.Optional[dict] = None,\n        split_cluster_kw: ty.Optional[dict] = None,\n    ):\n        \"\"\"Patch holes in regridded data to allow continuous region finding.\n\n        Args:\n            ds (xr.Dataset): dataset to patch\n            should_have_data_mask (xr.DataArray): a lat/lon mask of where there should be data\n            iter_time (bool, optional): Iteratate over the time field. Defaults to False.\n            build_cluster_kw (ty.Optional[dict], optional): Optional arguments passed to\n                optim_esm_tools.analyze.clustering.build_cluster_mask. Defaults to None.\n            split_cluster_kw (ty.Optional[dict], optional): Optional arguments passed to\n            optim_esm_tools.analyze.clustering._split_to_continuous. Defaults to None.\n        \"\"\"\n        self.ds = ds.copy()\n        self.should_have_data_mask: xr.DataArray = should_have_data_mask\n        self.iter_time: bool = iter_time\n        self.build_cluster_kw: ty.Dict = build_cluster_kw or dict(\n            min_samples=4,\n            max_distance_km=120,\n        )\n        self.split_cluster_kw: ty.Dict = split_cluster_kw or dict(\n            add_diagonal=False,\n            add_double_lon=False,\n        )\n\n    def find_issues(self) -> ty.List[np.ndarray]:\n        \"\"\"Find issues in the dataset, if any return a list of masks where\n        issues exist.\n\n        Returns:\n            ty.List[np.ndarray]: List of issues\n        \"\"\"\n        year_0 = self.ds[self.ds.variable_id].isel(time=0)\n        res = oet.analyze.clustering.build_cluster_mask(\n            (self.should_have_data_mask & year_0.isnull()).values.astype(bool),\n            year_0.lat.values,\n            year_0.lon.values,\n            **self.build_cluster_kw,\n        )\n        _, lon = np.meshgrid(self.ds.lat.values, self.ds.lon.values)\n        cell_a = self.ds[\"cell_area\"].values\n\n        continous_masks = oet.analyze.clustering._split_to_continuous(\n            res[0],\n            **self.split_cluster_kw,\n        )\n        large_enough_masks = [\n            r for r in continous_masks if r.sum() >= self.min_samples_for_issue\n        ]\n\n        located_enough_masks = [\n            m\n            for m in large_enough_masks\n            if DescrStatsW(lon.T[m], weights=cell_a[m], ddof=0).std\n            < self.max_lon_weighted_std\n        ]\n        if len(continous_masks):\n            oet.get_logger().info(\n                f\"Started with {len(continous_masks)} of which {len(large_enough_masks)} are >= {self.min_samples_for_issue} cells large. \"\n                f\"Finally, {len(located_enough_masks)} are actually also located enough (std < {self.max_lon_weighted_std}).\",\n            )\n        return located_enough_masks\n\n    @staticmethod\n    def find_adjacency(mask: np.ndarray):\n        \"\"\"Get cells that are one around the mask.\"\"\"\n        import scipy.ndimage as ndimage\n\n        blurred = (\n            ndimage.gaussian_filter(\n                (mask > 0).astype(np.float64),\n                sigma=(0, 1),\n                order=0,\n            )\n            > 0.2\n        ).astype(np.int16)\n\n        return (blurred - mask.astype(np.int16)).astype(np.bool_)\n\n    def execute_patches(self, issues: ty.List[np.ndarray]) -> None:\n        for issue in issues:\n            self.execute_patch(issue)\n\n    def patch_one_variable(\n        self,\n        variable: str,\n        issue: np.ndarray,\n        adjacency_mask: np.ndarray,\n        iter_time: ty.Optional[bool] = None,\n    ) -> None:\n        \"\"\"Patch the data in the dataset stored under \"variable\".\n\n        There are only a limited number of datastructures supported, if the NotImplementedError is raised,\n        one should add the appropriate handling proceidure for the datastructure.\n\n        Args:\n            variable (str): the data-array in the dataset to fix\n            issue (np.ndarray): the 2d-boolean mask for the issue to fix\n            adjacency_mask (np.ndarray): the result of self.find_adjacency(issue)\n            iter_time (ty.Optional[bool], optional): Iteratate over the time field. Defaults to None.\n\n        Raises:\n            NotImplementedError: If the data structure is unrecognized\n        \"\"\"\n        iter_time = iter_time if iter_time is not None else self.iter_time\n        da = self.ds[variable].copy()\n        del self.ds[variable]\n        idx_lon = np.argwhere(np.array(da.dims) == \"lon\").squeeze()\n        idx_lat = np.argwhere(np.array(da.dims) == \"lat\").squeeze()\n\n        if idx_lat == 0 and idx_lon == 1 and len(da.shape) == 2:\n            values = da.values\n            temp = values.copy()\n            temp[~adjacency_mask] = np.nan\n            patch_data = np.nanmean(temp, axis=idx_lon)\n            values[issue] = np.repeat(patch_data, values.shape[idx_lon]).reshape(\n                values.shape,\n            )[issue]\n\n            da.data = values\n        elif idx_lat == 1 and idx_lon == 2 and len(da.shape) == 3 and not iter_time:\n            values = da.values\n            temp = values.copy()\n            temp[:, ~adjacency_mask] = np.nan\n            patch_data = np.nanmean(temp, axis=idx_lon)\n            values[:, issue] = np.repeat(patch_data, values.shape[idx_lon]).reshape(\n                values.shape,\n            )[:, issue]\n\n            da.data = values\n        elif idx_lat == 1 and idx_lon == 2 and len(da.shape) == 3 and iter_time:\n            for t_idx in oet.utils.tqdm(list(range(len(self.ds[\"time\"])))):\n                da_t = da.isel(time=t_idx)\n                values = da_t.values\n                temp = values.copy()\n                temp[~adjacency_mask] = np.nan\n                patch_data = np.nanmean(temp, axis=idx_lon - 1)\n                values[issue] = np.repeat(\n                    patch_data,\n                    values.shape[idx_lon - 1],\n                ).reshape(values.shape)[issue]\n\n                da.data[t_idx] = values\n\n        else:\n            raise NotImplementedError(\n                idx_lat == 1,\n                idx_lon == 2,\n                len(da.shape) == 3,\n                not iter_time,\n            )\n\n        self.ds[variable] = da\n\n    def execute_patch(self, issue: np.ndarray) -> None:\n        \"\"\"For an issue, find the adjacency mask and then execute the patch for\n        each DataArray in the Dataset one by one.\n\n        Args:\n            issue (np.ndarray): 2d boolean array of the issue\n        \"\"\"\n        adjacency_mask = self.find_adjacency(issue)\n        patch_variables = [\n            v\n            for v in list(self.ds)\n            if all(\n                dim in self.ds[v].dims\n                for dim in oet.config.config[\"analyze\"][\"lon_lat_dim\"].split(\",\")\n            )\n        ]\n        for variable in patch_variables:\n            self.patch_one_variable(variable, issue, adjacency_mask)\n\n    def patch_all_issues(self) -> xr.Dataset:\n        \"\"\"Find all issues in the Dataset. If there are any, then iterate over\n        the issues and patch them one by one. If there are no issues, just\n        return the dataset as it is.\n\n        Returns:\n            xr.Dataset: A copy of the dataset with patches included, or if there are no issues found, the\n            original dataset.\n        \"\"\"\n        issues = self.find_issues()\n        oet.get_logger().warning(\n            f\"Finding {len(issues)} issues. N={[s.sum() for s in issues]}.\",\n        )\n        if len(issues):\n            self.ds = self.ds.copy()\n            for issue in oet.utils.tqdm(\n                issues,\n                desc=\"patching issues\",\n                disable=len(issues) <= 1,\n            ):\n                self.execute_patch(issue)\n            buffer = xr.zeros_like(self.ds['cell_area']).astype(bool)\n            for issue_mask in issues:\n                buffer.data = buffer.data | issue_mask\n            self.ds['patched_data'] = buffer\n            self.ds['patched_data'].attrs.update(\n                description=f'{self.__class__.__name__} added data for {len(issues)} in these lat/lon places.',\n            )\n        self.ds.attrs.update({\"Grid filler\": f\"Fixed {len(issues)}\"})\n        return self.ds\n", "coverage": [1, null, 1, 1, 1, 1, null, null, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, null, null, null, 1, null, null, null, null, 1, null, null, null, null, null, null, 1, 1, null, null, null, null, null, 1, 1, null, 1, null, null, null, 1, null, null, null, 1, null, null, null, null, null, 1, 1, null, null, null, 1, null, 1, 1, null, 1, null, 1, null, null, null, null, null, null, null, null, 1, null, 1, 0, 0, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, null, null, null, 1, 1, 1, 1, 1, 1, 1, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, null, null, 1, null, null, 1, null, null, null, null, null, null, 1, null, 1, null, null, null, null, null, null, 1, 1, null, null, null, null, null, null, null, 1, 1, null, 1, null, null, null, null, null, null, null, null, 1, 1, null, null, 1, 1, 1, null, null, null, null, 1, 1, 1, 1, 1, 1, null, null, 1, 1]}, {"name": "optim_esm_tools/analyze/find_matches.py", "source": "import glob\nimport os\nfrom collections import defaultdict\n\nfrom optim_esm_tools.config import config\nfrom optim_esm_tools.config import get_logger\nfrom optim_esm_tools.utils import check_accepts\nfrom optim_esm_tools.utils import deprecated\nfrom optim_esm_tools.utils import timed\nimport xarray as xr\nimport logging\nimport typing as ty\n\n\n@timed\n@check_accepts(\n    accepts=dict(\n        activity_id=('AerChemMIP', 'ScenarioMIP', 'CMIP', '*'),\n        experiment_id=(\n            'piControl',\n            'historical',\n            'ssp119',\n            'ssp126',\n            'ssp245',\n            'ssp370',\n            'ssp585',\n            '*',\n        ),\n    ),\n)\ndef find_matches(\n    base: str,\n    activity_id: str = 'ScenarioMIP',\n    institution_id: str = '*',\n    source_id: str = '*',\n    experiment_id: str = '*',\n    variant_label: str = '*',\n    domain: str = '*',\n    variable_id: str = '*',\n    grid_label: str = '*',\n    version: str = '*',\n    max_versions: int = 1,\n    max_members: int = 1,\n    required_file: str = 'merged.nc',\n    # Deprecated arg\n    grid=None,\n) -> ty.List[str]:\n    \"\"\"Follow synda folder format to find matches.\n\n    Args:\n        base (str): where start looking for matches\n        activity_id (str, optional): As synda convention. Defaults to 'ScenarioMIP'.\n        institution_id (str, optional): As synda convention. Defaults to '*'.\n        source_id (str, optional): As synda convention. Defaults to '*'.\n        experiment_id (str, optional): As synda convention. Defaults to 'ssp585'.\n        variant_label (str, optional): As synda convention. Defaults to '*'.\n        domain (str, optional): As synda convention. Defaults to 'Ayear'.\n        variable_id (str, optional): As synda convention. Defaults to 'tas'.\n        grid_label (str, optional): As synda convention. Defaults to '*'.\n        version (str, optional): As synda convention. Defaults to '*'.\n        max_versions (int, optional): Max number of different versions that match. Defaults to 1.\n        max_members (int, optional): Max number of different members that match. Defaults to 1.\n        required_file (str, optional): Filename to match. Defaults to 'merged.nc'.\n\n    Returns:\n        list: of matches corresponding to the query\n    \"\"\"\n    log = get_logger()\n    if grid is not None:  # pragma: no cover\n        log.error('grid argument for find_matches is deprecated, use grid_label')\n        grid_label = grid\n    if max_versions is None:\n        max_versions = int(9e9)  # pragma: no cover\n    if max_members is None:\n        max_members = int(9e9)  # pragma: no cover\n    variants = sorted(\n        glob.glob(\n            os.path.join(\n                base,\n                activity_id,\n                institution_id,\n                source_id,\n                experiment_id,\n                variant_label,\n                domain,\n                variable_id,\n                grid_label,\n                version,\n            ),\n        ),\n        key=_variant_label_id_and_version,\n    )\n    seen: dict = {}\n    for candidate in variants:\n        folders = candidate.split(os.sep)\n        group = folders[-7]\n        version = folders[-1]\n\n        if group not in seen:\n            seen[group] = defaultdict(list)\n        seen_members = seen[group]\n\n        if (len(seen_members) == max_versions and version not in seen_members) or len(\n            seen_members.get(version, []),\n        ) == max_members:\n            continue  # pragma: no cover\n        if required_file and required_file not in os.listdir(\n            candidate,\n        ):  # pragma: no cover\n            log.warning(f'{required_file} not in {candidate}')\n            continue\n        if is_excluded(candidate):  # pragma: no cover\n            log.info(f'{candidate} is excluded')\n            continue\n        seen_members[version].append(candidate)\n    assert all(len(group) <= max_versions for group in seen.values())\n    assert all(\n        len(members) <= max_members\n        for group in seen.values()\n        for members in group.values()\n    )\n    return [\n        folder\n        for group_dict in seen.values()\n        for versions in group_dict.values()\n        for folder in versions\n    ]\n\n\ndef _get_head(path: str) -> str:\n    log = get_logger()\n    if path.endswith(os.sep):\n        log.debug(f'Stripping tailing \"/\" from {path}')\n        path = path[: -len(os.sep)]\n\n    if os.path.isfile(path):\n        log.debug(f'Splitting file from {path}')\n        path = os.path.split(path)[0]\n    return path\n\n\ndef is_excluded(path: str, exclude_too_short: bool = True) -> bool:\n    from fnmatch import fnmatch\n\n    path = _get_head(path)\n    exlusion_list = config['CMIP_files']['excluded'].split('\\n')\n    if exclude_too_short:\n        exlusion_list += config['CMIP_files']['too_short'].split('\\n')\n    for excluded in exlusion_list:\n        if not excluded:\n            continue\n        folders = excluded.split()\n\n        path_ends_with = os.path.join(*path.split(os.sep)[-len(folders) :])\n        match_to = os.path.join(*folders)\n        if fnmatch(path_ends_with, match_to):\n            return True  # pragma: no cover\n    return False\n\n\ndef _variant_label_id_and_version(full_path: str) -> ty.Tuple[int, int]:\n    run_variant_number = None\n    grid_version = None\n    for folder in full_path.split(os.sep):\n        if len(folder):\n            if folder[0] == 'r' and run_variant_number is None:\n                index = folder.split('i')\n                if len(index) == 2:\n                    run_variant_number = int(index[0][1:])\n            if (\n                folder[0] == 'v'\n                and len(folder) == len('v20190731')\n                and grid_version is None\n            ):\n                grid_version = int(folder[1:])\n    if run_variant_number is None or grid_version is None:\n        return int(1e9), int(1e9)  # pragma: no cover\n    return -grid_version, run_variant_number\n\n\ndef folder_to_dict(path: str, strict: bool = True) -> ty.Optional[ty.Dict[str, str]]:\n    path = _get_head(path)\n    folders = path.split(os.sep)\n    if folders[-1][0] == 'v' and len(folders[-1]) == len('v20190731'):\n        return {\n            k: folders[-i - 1]\n            for i, k in enumerate(config['CMIP_files']['folder_fmt'].split()[::-1])\n        }\n        # great\n    if strict:\n        raise NotImplementedError(\n            f'folder {path} does not end with a version',\n        )  # pragma: no cover\n    return None\n\n\ndef base_from_path(path: str, look_back_extra: int = 0) -> str:\n    path = _get_head(path)\n    return os.path.join(\n        os.sep,\n        *path.split(os.sep)[\n            : -len(config['CMIP_files']['folder_fmt'].split()) - look_back_extra\n        ],\n    )\n\n\n@deprecated\ndef associate_historical(*a, **kw):\n    return associate_parent(*a, **kw)\n\n\ndef _get_search_kw(\n    data_set: xr.Dataset,\n    keep_keys: tuple = tuple(\n        'parent_activity_id parent_experiment_id parent_source_id parent_variant_label'.split(),\n    ),\n    required_file: str = 'merged.nc',\n) -> ty.Dict[str, str]:\n    query = {\n        k.replace('parent_', ''):\n        # Filter out some institutes that ended up adding a bunch of spaces here?!\n        data_set.attrs.get(k, '*').replace(' ', '')\n        for k in keep_keys\n    }\n    query.update(dict(required_file=required_file))\n    return query\n\n\ndef _check_search_kw(\n    search: dict,\n    data_set: xr.Dataset,\n    log: logging.Logger,\n    path: str,\n) -> dict:\n    if (\n        search['source_id'] == 'GISS-E2-1-G'\n        and data_set.attrs['source_id'] == 'GISS-E2-2-G'\n    ):\n        # I'm quite sure there has been some mixup here.\n        log.error(f'Hacking GISS-E2-1-G -> GISS-E2-2-G ?!!?!')\n        search['source_id'] = 'GISS-E2-2-G'\n\n    if search['source_id'] != data_set.attrs['source_id']:\n        log.critical(\n            f\"Misalignment in source-ids for {path} got {search['source_id']} and {data_set.attrs['source_id']}\",\n        )\n\n    if search['activity_id'] not in ['ScenarioMIP', 'CMIP']:\n        log.warning(\n            f\"{search['activity_id']} seems invalid for {path}, trying wildcard!\",\n        )\n        search['activity_id'] = '*'\n\n    return search\n\n\ndef _read_dataset(\n    data_set: ty.Optional[xr.Dataset],\n    required_file: ty.Optional[str],\n    path: ty.Optional[str],\n) -> xr.Dataset:\n    if data_set is None and path is None:\n        raise ValueError(\n            'No dataset, no path, can\\'t match if I don\\'t know what I\\'m looking for',\n        )  # pragma: no cover\n    if path is None:\n        assert data_set is not None\n        path = data_set.attrs['path']\n\n    assert isinstance(path, str) and os.path.exists(path), path\n    if data_set is None:\n        from optim_esm_tools import load_glob\n\n        assert required_file is not None\n        file = (\n            os.path.join(path, required_file)\n            if not path.endswith(required_file)\n            else path\n        )\n        data_set = load_glob(file)\n    return data_set\n\n\ndef associate_parent(\n    data_set: ty.Optional[xr.Dataset] = None,\n    path: ty.Optional[str] = None,\n    match_to: str = 'piControl',\n    look_back_extra: int = 0,\n    query_updates: ty.Optional[ty.List[dict]] = None,\n    search_kw: ty.Optional[dict] = None,\n    strict: bool = True,\n    required_file: ty.Optional[str] = 'merged.nc',\n):\n\n    log = get_logger()\n\n    data_set = _read_dataset(data_set=data_set, required_file=required_file, path=path)\n    path = path or data_set.attrs['path']\n    base = base_from_path(path, look_back_extra=look_back_extra)\n    search = _get_search_kw(data_set, required_file=required_file)\n    search = _check_search_kw(search=search, data_set=data_set, log=log, path=path)\n\n    if all(v == '*' for v in search.values()) or search['source_id'] == '*':\n        raise ValueError(f'Unclear search for {path} - attributes are missing')\n\n    search.update(dict(variable_id=data_set.attrs['variable_id']))\n\n    if search_kw:\n        raise ValueError(f'Not used any more, got {search_kw}')\n\n    if query_updates is None:\n        # It's important to match the variant-label last, otherwise we get mismatched simulations\n        # from completely different ensamble members. We used to accept such cases, but disabled\n        # this later as it gave funky results\n        query_updates = [\n            {},\n            dict(version='*'),\n            dict(grid_label='*'),\n            dict(variant_label='*'),\n        ]\n\n    for try_n, update_query in enumerate(query_updates):\n        if try_n:\n            if not strict:\n                message = f'No results after {try_n} try, retrying with {update_query}'\n                log.info(message)\n            else:\n                break\n        search.update(update_query)\n        if this_try := find_matches(base, **search):\n            if match_to == 'piControl' and search.get('experiment_id') == 'historical':\n                log.debug(\n                    f'Found historical, but we need to match to PiControl, recursively returning!',\n                )\n                results = [\n                    associate_parent(\n                        path=t,\n                        match_to=match_to,\n                        look_back_extra=look_back_extra,\n                        query_updates=query_updates,\n                        search_kw=search_kw,\n                        strict=strict,\n                    )\n                    for t in this_try\n                ]\n                # Return a flat array!\n                return [rr for r in results if r for rr in r]\n\n            return this_try\n    message = f'Looked for {search}, in {base} found nothing'\n    if strict:\n        raise RuntimeError(message)\n    log.warning(message)  # pragma: no cover\n", "coverage": [1, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, 1, null, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, null, null, 1, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, null, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, null, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, null, null, 1, 1, null, 1, null, null, 1, 1, 1, 1, 1, null, null, null, null, 0, null, null, null, 0, null, null, 1, 1, 1, null, null, null, null, null, null, null, 1, 1, 1, null, null, 1, null, null, null, null, null, null, 1, null, null, null, null, null, 1, 1, null, null, 1, null, null, null, null, null, 1, null, null, null, null, 0, 0, null, 1, 0, null, null, null, 1, 0, null, null, 0, null, 1, null, null, 1, null, null, null, null, 1, null, null, null, 1, 0, 0, null, 1, 1, 1, null, 1, 1, null, null, null, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, 1, null, 1, 1, 1, 1, 1, null, 1, 0, null, 1, null, 1, 0, null, 1, null, null, null, 1, null, null, null, null, null, null, 1, 1, 0, 0, 0, null, 0, 1, 1, 1, 0, null, null, 0, null, null, null, null, null, null, null, null, null, null, null, 0, null, 1, 0, 0, 0, null]}, {"name": "optim_esm_tools/analyze/globals.py", "source": "import typing as ty\nfrom optim_esm_tools.config import config\n\n_SECONDS_TO_YEAR: int = int(config['constants']['seconds_to_year'])\n_FOLDER_FMT: ty.List[str] = config['CMIP_files']['folder_fmt'].split()\n_CMIP_HANDLER_VERSION: str = config['versions']['cmip_handler']\n_DEFAULT_MAX_TIME: ty.Tuple[int, ...] = tuple(\n    int(s) for s in config['analyze']['max_time'].split()\n)\n", "coverage": [1, 1, null, 1, 1, 1, 1, null, null]}, {"name": "optim_esm_tools/analyze/io.py", "source": "import os\n\nimport xarray as xr\n\nfrom optim_esm_tools.utils import add_load_kw\n\n\n@add_load_kw\ndef load_glob(\n    pattern: str,\n    **kw,\n) -> xr.Dataset:\n    \"\"\"Load cmip dataset from provided pattern.\n\n    Args:\n        pattern (str): Path where to load the data from\n\n    Returns:\n        xr.Dataset: loaded from pattern\n    \"\"\"\n    if not os.path.exists(pattern):\n        raise FileNotFoundError(f'{pattern} does not exists')  # pragma: no cover\n    for k, v in dict(\n        use_cftime=True,\n        concat_dim='time',\n        combine='nested',\n        data_vars='minimal',\n        coords='minimal',\n        compat='override',\n        decode_times=True,\n    ).items():\n        kw.setdefault(k, v)\n    try:\n        return xr.open_mfdataset(pattern, **kw)\n    except ValueError as e:  # pragma: no cover\n        raise ValueError(f'Fatal error while reading {pattern}') from e\n", "coverage": [1, null, 1, null, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1, null, null, null, null, null, null, null, null, 1, 1, 1, null, null]}, {"name": "optim_esm_tools/analyze/merge_candidate_regions.py", "source": "import os\nimport typing as ty\n\nimport numba\nimport numpy as np\nimport optim_esm_tools as oet\nimport pandas as pd\nimport xarray as xr\n\n\ndef should_merge(\n    ds1: ty.Union[xr.DataArray, np.ndarray],\n    ds2: ty.Union[xr.DataArray, np.ndarray],\n    min_frac_overlap: float = 0.5,\n    min_border_frac: float = 0.05,\n    min_n_adjacent: int = 100,\n) -> bool:\n    \"\"\"Based on two 2d-grids (ds1 and ds2) decide if we should merge the grids\n    since they either overlap or are adjacent.\n\n    Merge if either is true:\n        - The two regions overlap by a fraction of at least <min_frac_overlap>\n        - The two regions border, where a fraction of at least <min_border_frac> of the borders are adjacent\n        - The two regions border, with at least <min_n_adjacent> grid cells\n    \"\"\"\n\n    if isinstance(ds1, xr.DataArray) and isinstance(ds2, xr.DataArray):\n        ar1 = ds1.values\n        ar2 = ds2.values\n    elif isinstance(ds1, np.ndarray) and isinstance(ds2, np.ndarray):\n        ar1 = ds1\n        ar2 = ds2\n    else:\n        raise TypeError(f\"Bad type {type(ds1)} {type(ds2)}\")\n\n    if _frac_overlap_nb(ar1, ar2) > min_frac_overlap:\n        return True\n    merge_a = _should_merge_adjacent(\n        ar1,\n        ar2,\n        min_border_frac=min_border_frac,\n        min_n_adjacent=min_n_adjacent,\n    )\n    if merge_a:\n        return True\n\n    # Also check the other way around one area is usually larger. For a small area, the fraction of overlap requirement is easier to satisfy.\n    merge_b = _should_merge_adjacent(\n        ar2,\n        ar1,\n        min_border_frac=min_border_frac,\n        min_n_adjacent=min_n_adjacent,\n    )\n    if merge_b:\n        oet.get_logger().debug(\n            f\"Got that we should not merge a to b, but b to a should be merged!\",\n        )\n    return merge_b\n\n\ndef _frac_overlap_nb(ar1: np.ndarray, ar2: np.ndarray):\n    \"\"\"Fraction of ar1 and ar2 that overlap.\"\"\"\n    assert ar1.shape == ar2.shape\n    return _frac_overlap_numba(ar1, ar2, ar1.shape)\n\n\n@numba.njit\ndef _frac_overlap_numba(ar1, ar2, shape):\n    x, y = shape\n    both = np.int64(0)\n    tot_1 = np.int64(0)\n    tot_2 = np.int64(0)\n    for i in range(x):\n        for j in range(y):\n            if ar1[i][j]:\n                tot_1 += 1\n                if ar2[i][j]:\n                    tot_2 += 1\n                    both += 1\n            elif ar2[i][j]:\n                tot_2 += 1\n    lowest = min(tot_1, tot_2)\n    return both / lowest\n\n\ndef _should_merge_adjacent(\n    ar1: np.ndarray,\n    ar2: np.ndarray,\n    min_border_frac=0.05,\n    min_n_adjacent=100,\n):\n    n_ad, n_border = _n_adjacent(ar1, ar2)\n    return (n_ad / n_border) > min_border_frac or n_ad > min_n_adjacent\n\n\n@numba.njit\ndef _n_adjacent(ar1: np.ndarray, ar2: np.ndarray):\n    _n_adjacent = 0\n    _n_border = 0\n    x, y = ar1.shape\n\n    for i in range(x):\n        for j in range(y):\n            if ar1[i][j] == False:\n                continue\n\n            _this_is_border = False\n\n            left = np.mod(j + 1, y - 1)\n            right = j - 1\n\n            up = min(i + 1, x - 1)\n            down = max(i - 1, 0)\n\n            for altx, alty in ((up, j), (down, j), (i, left), (i, right)):\n                if (altx, alty) == (i, j):\n                    # Can raise this error, but for performance disabled, if it's working once, it's always true\n                    # if (altx, alty) == (i, left) or (altx, alty) == (i, right):\n                    #     raise ValueError((altx, alty) , (i,j))\n                    # For up and down, we don't assume it's circular, so here the min and max can crop it to the same coordinate as i,j, let's skip those cases.\n                    continue\n\n                if not ar1[altx][alty]:\n                    _this_is_border = True\n                    if ar2[altx][alty]:\n                        _n_adjacent += 1\n            if _this_is_border:\n                _n_border += 1\n    return _n_adjacent, _n_border\n\n\nclass Merger:\n    _log = None\n    _sorted = False\n\n    def __init__(\n        self,\n        pass_criteria: ty.Callable,\n        summary_calculation: ty.Callable,\n        data_sets: ty.Optional[ty.List[xr.Dataset]] = None,\n        common_mother: ty.Optional[xr.Dataset] = None,\n        common_pi: ty.Optional[xr.Dataset] = None,\n        merge_options: ty.Optional[dict] = None,\n        merge_method: str = \"independent\",\n    ):\n        assert data_sets, 'datasets'\n        assert isinstance(pass_criteria, ty.Callable)\n        assert isinstance(summary_calculation, ty.Callable)\n        self.summary_calculation = summary_calculation\n        self.pass_criteria = pass_criteria\n        self.data_sets = data_sets\n\n        common_mother, common_pi = self.get_mother_and_pi(common_mother, common_pi)\n        self.common_mother = common_mother\n        self.common_pi = common_pi\n        self.merge_options = merge_options or dict(\n            min_frac_overlap=0.5,\n            min_border_frac=0.05,\n            min_n_adjacent=100,\n        )\n        self.merge_method = merge_method\n\n    def get_mother_and_pi(\n        self,\n        common_mother,\n        common_pi,\n    ) -> ty.Tuple[xr.Dataset, xr.Dataset]:\n        common_mother = common_mother or oet.load_glob(self.data_sets[0]['file'])\n        common_pi = common_pi or oet.analyze.time_statistics.get_historical_ds(\n            common_mother,\n        )\n        assert isinstance(\n            common_mother,\n            xr.Dataset,\n        ), f\"Got wrong type {type(common_mother)}\"\n        assert isinstance(common_pi, xr.Dataset), f\"Got wrong type {type(common_pi)}\"\n        return common_mother, common_pi\n\n    def set_passing_largest_data_sets_first(self) -> None:\n        candidates_info = [\n            dict(\n                passes=self.pass_criteria(\n                    **self.summary_calculation(\n                        **self.summary_kw,\n                        mask=c['global_mask'],\n                    ),\n                ),\n                cells=int(c['global_mask'].sum()),\n                candidate=c,\n            )\n            for c in self.data_sets\n        ]\n        _max_number_of_cells = max(c['cells'] for c in candidates_info)\n\n        candidates_sorted = sorted(\n            candidates_info,\n            key=lambda x: -int(x['passes'] * 2) * _max_number_of_cells\n            - int(x['cells']),\n        )\n\n        self.data_sets = [c['candidate'] for c in candidates_sorted]\n        self._sorted = True\n\n    def merge_datasets(self):\n        if not self._sorted:\n            self.set_passing_largest_data_sets_first()\n        candidates = self.data_sets\n        groups = []\n        pbar = oet.utils.tqdm(total=len(candidates), desc='Looping candidates')\n        while candidates:\n            pbar.n = len(self.data_sets) - len(candidates)\n            pbar.display()\n            self.log.info(pbar)\n            doc = self._group_to_first(candidates)\n\n            if not self.pass_criteria(**doc['stats']):\n                self.log.info(\n                    f'Discarding group {doc[\"merged\"]} because {doc[\"stats\"]} does not pass',\n                )\n                self.log.debug(f\"Discarded doc {doc}\")\n            else:\n                self.log.info(f'Adding {doc[\"merged\"]} because {doc[\"stats\"]} passes')\n                doc['ds'].attrs.update(\n                    dict(\n                        mask_paths=[\n                            c.attrs.get('mask_path', 'path=?')\n                            for i, c in enumerate(candidates)\n                            if i in doc['merged']\n                        ],\n                    ),\n                )\n                groups.append(doc)\n            candidates = [c for i, c in enumerate(candidates) if i not in doc['merged']]\n\n            if doc.get('force_break', False):\n                self.log.warning('Breaking forcefully')\n                candidates = []\n        pbar.n = pbar.total\n        pbar.close()\n        pbar.display()\n        self.log.info(pbar)\n        return groups\n\n    @property\n    def summary_kw(self):\n        return dict(\n            ds_global=self.common_mother,\n            ds_pi=self.common_pi,\n            field=self.common_mother.attrs['variable_id'],\n        )\n\n    def _group_to_first(\n        self,\n        candidates: ty.List[xr.Dataset],\n    ) -> ty.Dict[str, ty.Union[ty.Mapping, xr.Dataset, ty.List]]:\n        something_merged = True\n        global_masks = {\n            i: ds['global_mask'].load().copy() for i, ds in enumerate(candidates)\n        }\n        current_global_mask: xr.DataArray = global_masks.pop(0)\n\n        merge_to_current: ty.List[int] = []\n\n        summary_kw = self.summary_kw\n        first_doc = self.summary_calculation(\n            **self.summary_kw,\n            mask=candidates[0]['global_mask'],\n        )\n        if not self.pass_criteria(**first_doc):\n            # The first candidate is not passing anything, so we should stop here.\n            # Because _set_passing_largest_data_sets_first set the passing sets first, there\n            # Should be no reason to continue here.\n            self.log.info(\n                f\"Exhausted passing regions, so next candidates are ignored ({len(candidates)-1} remaining)\",\n            )\n            # We are going to pass one additional argument that allows us to break the overencompasing loop\n            return dict(stats=first_doc, ds=candidates[0], merged=[0], force_break=True)\n        while something_merged:\n            something_merged = False\n            for i, ds_alt in global_masks.items():\n                if i in merge_to_current:\n                    continue\n                if should_merge(current_global_mask, ds_alt, **self.merge_options):\n                    merge_to_current += [i]\n                    current_global_mask = current_global_mask | ds_alt\n                    something_merged = True\n        self.log.info(f\"Merging items {merge_to_current} to [0]\")\n        if not merge_to_current:\n            return dict(stats=first_doc, ds=candidates[0], merged=[0])\n\n        ds_merged = oet.analyze.xarray_tools.mask_to_reduced_dataset(\n            self.common_mother.load().copy(),\n            oet.analyze.xarray_tools.reverse_name_mask_coords(current_global_mask),\n        )\n        stat = self.summary_calculation(**summary_kw, mask=current_global_mask)\n        if self.pass_criteria(**stat):\n            self.log.info(f\"After merging everything, we pass\")\n            return dict(stats=stat, ds=ds_merged, merged=[0] + merge_to_current)\n\n        for it, candidate_i in enumerate(\n            oet.utils.tqdm([0] + merge_to_current, desc='checking at least one passes'),\n        ):\n            m = candidates[candidate_i]['global_mask']\n            if self.pass_criteria(\n                **self.summary_calculation(**summary_kw, mask=m),\n            ):\n                if it == 0:\n                    self.log.info('First dataset passes, stop check')\n                    break\n                # Set the passing candidate as first in the list to merge to item 0.\n                # Account for [0] being it 0\n                it -= 1\n                self.log.info(\n                    f\"Merging {candidate_i} of {merge_to_current} (it = {it})\",\n                )\n                passing_first = merge_to_current[it:] + merge_to_current[:it]\n                assert all(np.in1d(passing_first, merge_to_current)) and all(\n                    np.in1d(merge_to_current, passing_first),\n                ), f\"{passing_first} {merge_to_current} are not equal!\"\n                merge_to_current = passing_first\n                self.log.info(\n                    f\"Changed order of merge_to_current to {merge_to_current}\",\n                )\n                break\n        else:\n            self.log.warning(\n                f\"All the datasets that are adjacent to current mask are not passing the criteria. Not trying to merge this group {merge_to_current}\",\n            )\n            return dict(stats=stat, ds=ds_merged, merged=[0] + merge_to_current)\n\n        self.log.warning(\n            f\"Merging would lead to failed test, going over items one by one\",\n        )\n        if self.merge_method != 'independent':\n            raise NotImplementedError(f'{self.merge_method} is not implemented')\n\n        return self._iter_mergable_candidates(\n            candidates,\n            merge_to_current,\n            summary_kw,\n        )\n\n    def _iter_mergable_candidates(\n        self,\n        candidates: ty.List[xr.Dataset],\n        merge_to_current: ty.List[int],\n        summary_kw: ty.Mapping,\n    ) -> ty.Dict[str, ty.Union[ty.Mapping, xr.Dataset, ty.List]]:\n        global_masks = {\n            i: ds['global_mask'].load().copy() for i, ds in enumerate(candidates)\n        }\n        current_global_mask = global_masks.pop(0)\n        do_merge: ty.List[int] = []\n\n        do_a_merge = True\n        it = 0\n        pbar = oet.utils.tqdm(total=len(global_masks), desc='merging iteratively')\n        while do_a_merge:\n            # Keep looping over the candidates until we can loop over all the remaining candidates and conclude that they cannot be merged to the current mask\n            it += 1\n            pbar.n = len(do_merge)\n            self.log.info(pbar)\n            self.log.info(f\"{it}: Remaining {merge_to_current} done {do_merge}\")\n            do_a_merge = False\n            # Try this many times. We always pop the first element of the remaining candidates.\n            for _ in merge_to_current:\n                merge = merge_to_current.pop(0)\n                ds_alt = global_masks[merge]\n                passes_crit = False\n                if should_merge(current_global_mask, ds_alt, **self.merge_options):\n                    self.log.info(f\"Try merging {merge}\")\n                    candidate_mask = current_global_mask.copy() | ds_alt\n                    candidate_ds = oet.analyze.xarray_tools.mask_to_reduced_dataset(\n                        self.common_mother.load().copy(),\n                        oet.analyze.xarray_tools.reverse_name_mask_coords(\n                            candidate_mask,\n                        ),\n                    )\n                    candidate_stat = self.summary_calculation(\n                        **summary_kw,\n                        mask=candidate_mask,\n                    )\n                    passes_crit = self.pass_criteria(**candidate_stat)\n                else:\n                    self.log.info(f\"No reason to merge {merge}\")\n                    continue\n\n                if passes_crit:\n                    self.log.info(\n                        f\"Merging {merge}. Currently do_a_merge={do_a_merge} remaining={merge_to_current}\",\n                    )\n                    # Now break\n                    do_a_merge = True\n                    current_global_mask = candidate_mask\n                    ds_merged = candidate_ds\n                    stat = candidate_stat\n                    do_merge += [merge]\n                    break\n                else:\n                    # Now let's add i back into the list of things that we could merge\n                    merge_to_current += [merge]\n                    self.log.info(f\"Merge {merge} failed, lead to {candidate_stat}\")\n        pbar.close()\n        pbar.display()\n        self.log.info(pbar)\n        self.log.info(f\"Merging {do_merge} from {merge_to_current}\")\n        if not do_merge:\n            single_stat = self.summary_calculation(\n                **summary_kw,\n                mask=candidates[0]['global_mask'],\n            )\n            return dict(stats=single_stat, ds=candidates[0], merged=[0])\n        return dict(stats=stat, ds=ds_merged, merged=[0] + do_merge)\n\n    @property\n    def log(self):\n        self._log = self._log or oet.get_logger()\n        return self._log\n\n\nclass MergerCached(Merger):\n    def __init__(self, *a, **kw):\n        super().__init__(*a, **kw)\n        del self.summary_calculation\n\n        self._cache = {}\n        self._summary_calculation = kw['summary_calculation']\n        self.summary_calculation = self.cache_summary\n\n    @staticmethod\n    def hash_for_mask(mask: ty.Union[np.ndarray, xr.DataArray]) -> str:\n        if isinstance(mask, xr.DataArray):\n            return hash(mask.values.tobytes())\n        return hash(mask.tobytes())\n\n    def cache_summary(self, mask, *a, **kw) -> str:\n        key = self.hash_for_mask(mask)\n        if key not in self._cache:\n            self.log.warning(f\"Loading {key} new. Tot size {len(self._cache.keys())}\")\n            self._cache[key] = self._summary_calculation(*a, **kw, mask=mask)\n        return self._cache[key]\n\n    def set_all_caches_as_false(\n        self,\n        masks: ty.List[np.ndarray],\n        fill_doc: ty.Optional[dict] = None,\n    ) -> None:\n        \"\"\"A method to set all masks to not fullfull the summay calculation\"\"\"\n        if fill_doc is None:\n            doc_0 = self._summary_calculation(**self.summary_kw, mask=masks[0])\n            fill_doc = {\n                k: np.nan if isinstance(v, (float, int, np.number)) else v\n                for k, v in doc_0.items()\n            }\n        for mask in masks:\n            key = self.hash_for_mask(mask)\n            self._cache[key] = fill_doc\n", "coverage": [1, 1, null, 1, 1, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, null, 1, null, 1, 1, 1, null, null, null, null, null, 1, 1, null, null, 1, null, null, null, null, null, 1, 1, null, null, 1, null, null, 1, null, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, null, null, null, null, null, 1, 1, null, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, 1, null, 1, 1, null, 1, 1, null, 1, 1, null, null, null, null, 1, null, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, null, null, null, 1, null, 1, null, null, null, null, 1, 1, null, null, 1, null, null, null, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1, null, null, null, null, null, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, null, null, 1, null, 1, 1, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, null, null, null, null, null, 1, null, null, null, 1, 1, null, null, 1, null, 1, null, 1, 1, null, null, null, 1, null, null, null, 1, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, null, null, null, 1, 1, 1, 1, null, 1, null, null, 1, 1, null, null, 1, 1, 1, null, null, 0, 0, null, null, 0, 0, null, null, 0, 0, null, null, 0, null, 0, null, null, 0, null, 1, null, null, 1, 0, null, 1, null, null, null, null, null, 1, null, null, null, null, null, 1, null, null, 1, 1, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, null, null, null, null, null, 1, null, null, null, 1, null, 1, 1, null, 1, 1, null, null, null, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 0, null, null, null, 0, 1, null, 1, 1, 1, 1, null, null, 1, 1, 1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, null, 1, null, null, null, null, null, 1, 1, 1, null, null, null, 1, 1, 1]}, {"name": "optim_esm_tools/analyze/pre_process.py", "source": "import os\nimport shutil\nimport tempfile\nimport typing as ty\n\nimport numpy as np\n\nfrom optim_esm_tools.analyze.globals import _DEFAULT_MAX_TIME\nfrom optim_esm_tools.analyze.io import load_glob\nfrom optim_esm_tools.analyze.xarray_tools import _native_date_fmt\nfrom optim_esm_tools.config import config\nfrom optim_esm_tools.config import get_logger\nfrom optim_esm_tools.utils import timed, check_accepts, to_str_tuple\nfrom pandas.util._decorators import deprecate_kwarg\n\n\n@deprecate_kwarg('source', 'sources')\n@check_accepts(dict(return_type=('path', 'data_set')))\ndef get_preprocessed_ds(\n    sources: ty.Union[str, tuple, list],\n    year_mean=False,\n    _check_duplicate_years=True,\n    return_type='data_set',\n    skip_compression=False,\n    temp_dir_location=None,\n    **kw,\n):\n    \"\"\"Create a temporary working directory for pre-process and delete all\n    intermediate files.\"\"\"\n    if 'working_dir' in kw:  # pragma: no cover\n        message = (\n            f'Calling get_preprocessed_ds with working_dir={kw.get(\"working_dir\")} is not '\n            'intended, as this function is meant to open a temporary directory, load the '\n            'dataset, and remove all local files.'\n        )\n        get_logger().warning(message)\n    if return_type == 'path':\n        assert 'save_as' in kw\n        store_final = kw.pop('save_as')\n    if temp_dir_location is None and os.path.exists('/data/volume_2/temp'):\n        temp_dir_location = '/data/volume_2/temp'\n    with tempfile.TemporaryDirectory(dir=temp_dir_location) as temp_dir:\n        if year_mean:\n            old_sources = to_str_tuple(sources)\n            new_sources = [\n                os.path.join(temp_dir, os.path.split(p)[1]) for p in old_sources\n            ]\n            for o, n in zip(old_sources, new_sources):\n                _year_mon_mean(o, n)\n                assert os.path.exists(n)\n            sources = new_sources\n        if isinstance(sources, (list, tuple)) and len(sources) > 1:\n            source_tmp = os.path.join(temp_dir, 'sources_merged.nc')\n            _merge_sources(list(sources), source_tmp)\n            source = source_tmp\n        elif len(sources) == 1:\n            source = sources[0]\n        else:\n            assert isinstance(sources, str)\n            source = sources\n        defaults = dict(\n            source=source,\n            working_dir=temp_dir,\n            clean_up=False,\n            save_as=None,\n        )\n        for k, v in defaults.items():\n            kw.setdefault(k, v)\n        intermediate_file = pre_process(\n            **kw,\n            _check_duplicate_years=_check_duplicate_years,\n        )\n        ds = load_glob(intermediate_file)\n        if return_type == 'data_set':\n            # After with close this \"with\", we lose the file, so load it just to be sure we have all we need\n            ds = ds.load()  # type: ignore\n            ret = ds\n        elif skip_compression:\n            shutil.move(intermediate_file, store_final)\n            ret = store_final\n        else:\n            save_nc(ds, store_final)\n            ret = store_final\n\n    if _check_duplicate_years:\n        sanity_check(ds)\n    return ret\n\n\ndef _merge_sources(source_files: ty.List[str], f_tmp: str) -> None:  # pragma: no cover\n    import cdo\n\n    cdo_int = cdo.Cdo()\n    cdo_int.mergetime(input=source_files, output=f_tmp)\n\n\ndef _year_mon_mean(input_file, output_file):\n    import cdo\n\n    cdo_int = cdo.Cdo()\n    cdo_int.yearmonmean(input=input_file, output=output_file)\n\n\ndef save_nc(ds, path):\n    comp_kw = dict(\n        format='NETCDF4',\n        engine='netcdf4',\n        encoding={k: {'zlib': True, 'complevel': 1} for k in ds.data_vars},\n    )\n    ds.to_netcdf(path, **comp_kw)\n\n\ndef sanity_check(ds):\n    t_prev = None\n\n    for i, t in enumerate(ds['time'].values):\n        t_cur = getattr(t, 'year', None)\n        if t_prev is not None and t_cur <= t_prev:  # pragma: no cover\n            m = f'Got at least one overlapping year on index {i} {t_cur} {t_prev}'\n            raise ValueError(m)\n        t_prev = t_cur\n\n\ndef pre_process(\n    source: str,\n    historical_path: ty.Optional[str] = None,\n    target_grid: ty.Union[None, str, bool] = None,\n    max_time: ty.Optional[ty.Tuple[int, ...]] = _DEFAULT_MAX_TIME,\n    min_time: ty.Optional[ty.Tuple[int, int, int]] = None,\n    save_as: ty.Optional[str] = None,\n    clean_up: bool = True,\n    _ma_window: ty.Union[int, str, None] = None,\n    variable_id: ty.Optional[str] = None,\n    working_dir: ty.Optional[str] = None,\n    _check_duplicate_years=True,\n    do_detrend=True,\n    do_running_mean=True,\n) -> str:  # type: ignore\n    \"\"\"Apply several preprocessing steps to the file located at <source>:\n\n      - Slice the data to desired time range\n      - regrid to simple grid\n      - calculate corresponding area\n      - calculate running mean, detrended and not-detrended\n      - merge all files into one\n\n    Args:\n        source (str): path of file to parse\n        historical_path (str, None): if specified, first merge this file to the source.\n        target_grid (str, optional): Grid specification (like n64, n90 etc.). Defaults to None and\n            is taken from config.\n        max_time (ty.Optional[ty.Tuple[int, int, int]], optional): Defines time range in which to\n            load data. Defaults to (2100, 12, 30).\n        min_time (ty.Optional[ty.Tuple[int, int, int]], optional): Defines time range in which to\n            load data. Defaults to None.\n        save_as (str, optional): path where to store the pre-processed folder. Defaults to None.\n        clean_up (bool, optional): delete intermediate files. Defaults to True.\n        _ma_window (int, optional): moving average window (assumed 10 years). Defaults to None.\n        variable_id (str, optional): Name of the variable of interest. Defaults to None.\n\n    Raises:\n        ValueError: If source and dest are the same, we'll run into problems\n\n    Returns:\n        str: path of the dest file (same provided, if any)\n    \"\"\"\n\n    import cdo\n\n    _remove_bad_vars(source)\n    if historical_path is not None:\n        _remove_bad_vars(historical_path)\n    variable_id = variable_id or _read_variable_id(source)\n    use_max_time = max_time or (9999, 12, 30)  # unreasonably far away\n    use_min_time = min_time or (0, 1, 1)  # unreasonably long ago\n\n    do_regrid = target_grid != False\n    target_grid = target_grid or config['analyze']['regrid_to']\n\n    _ma_window = _ma_window or config['analyze']['moving_average_years']\n    _check_time_range(source, use_max_time, use_min_time, _ma_window)\n\n    cdo_int = cdo.Cdo()\n    head, _ = os.path.split(source)\n    working_dir = working_dir or head\n\n    # Several intermediate_files\n    f_time = os.path.join(working_dir, 'time_sel.nc')\n    f_tmp = os.path.join(working_dir, 'tmp.nc')\n    f_regrid = os.path.join(working_dir, 'regrid.nc')\n    f_area = os.path.join(working_dir, 'area.nc')\n    f_det = os.path.join(working_dir, 'detrend.nc')\n    f_rm = os.path.join(working_dir, f'rm_{_ma_window}.nc')\n    f_det_rm = os.path.join(working_dir, f'detrend_rm_{_ma_window}.nc')\n    files = [f_time, f_det, f_det_rm, f_rm, f_tmp, f_regrid, f_area]\n\n    save_as = save_as or os.path.join(working_dir, 'result.nc')\n\n    # Several names:\n    var = variable_id\n\n    for p in files + [save_as]:\n        if p == source:\n            raise ValueError(f'source equals other path {p}')  # pragma: no cover\n        if os.path.exists(p):  # pragma: no cover\n            get_logger().warning(f'Removing {p}!')\n            os.remove(p)\n    if historical_path:\n        _remap_and_merge(\n            cdo_int,\n            cdo,\n            historical_path,\n            source,\n            target_grid,\n            working_dir,\n            f_tmp,\n        )\n        source = f_tmp\n    if _check_duplicate_years:\n        _remove_duplicate_time_stamps(source)\n\n    next_source = source\n    if min_time is not None or max_time is not None:\n        time_range = f'{_fmt_date(use_min_time)},{_fmt_date(use_max_time)}'\n        cdo_int.seldate(time_range, input=next_source, output=f_time)  # type: ignore\n        next_source = f_time\n\n    if do_regrid:\n        cdo_int.remapbil(target_grid, input=next_source, output=f_regrid)  # type: ignore\n        cdo_int.gridarea(input=f_regrid, output=f_area)  # type: ignore\n        input_files = [f_regrid, f_area]\n    else:\n        input_files = [f_regrid]\n        os.rename(next_source, f_regrid)\n    next_source = f_regrid\n\n    if do_detrend:\n        var_det = f'{var}_detrend'\n        cdo_int.detrend(input=next_source, output=f_tmp)  # type: ignore\n        cdo_int.chname(f'{var},{var_det}', input=f_tmp, output=f_det)  # type: ignore\n        os.remove(f_tmp)\n        input_files += [f_det]\n\n    if do_running_mean:\n        var_rm = f'{var}_run_mean_{_ma_window}'\n        cdo_int.runmean(_ma_window, input=f_regrid, output=f_tmp)  # type: ignore\n        _run_mean_patch(\n            f_start=f_regrid,\n            f_rm=f_tmp,\n            f_out=f_rm,\n            ma_window=_ma_window,\n            var_name=var,\n            var_rm_name=var_rm,\n        )\n        os.remove(f_tmp)\n        input_files += [f_rm]\n\n    if do_running_mean and do_detrend:\n        var_det_rm = f'{var_det}_run_mean_{_ma_window}'\n        cdo_int.detrend(input=f_rm, output=f_tmp)  # type: ignore\n        cdo_int.chname(f'{var_rm},{var_det_rm}', input=f_tmp, output=f_det_rm)  # type: ignore\n        input_files += [f_det_rm]\n    get_logger().info(f'Join {input_files} to {save_as}')\n    cdo_int.merge(input=' '.join(input_files), output=save_as)  # type: ignore\n\n    if clean_up:  # pragma: no cover\n        for p in files:\n            if os.path.exists(p):\n                os.remove(p)\n    return save_as\n\n\ndef _quick_drop_duplicates(ds, t_span, t_len, path):\n    ds = ds.drop_duplicates('time')\n    if (t_new_len := len(ds['time'])) > t_span + 1:\n        raise ValueError(f'{t_new_len} too long! Started with {t_len} and {t_span}')\n    get_logger().warning('Timestamp issue solved')\n    with tempfile.TemporaryDirectory() as temp_dir:\n        save_as = os.path.join(temp_dir, 'temp.nc')\n        ds.to_netcdf(save_as)\n        # move the old file\n        os.rename(path, os.path.join(os.path.split(path)[0], 'faulty_merged.nc'))\n        shutil.copy2(save_as, path)\n\n\ndef _drop_duplicates_carefully(ds, t_span, t_len, path):\n    from tqdm import tqdm\n\n    # As we only do this for huge datasets, it might be that /tmp doesn't allow storing sufficient data.\n    work_dir = os.path.split(path)[0]\n    with tempfile.TemporaryDirectory(dir=work_dir) as temp_dir:\n        keep_years = np.argwhere(np.diff([t.year for t in ds['time'].values]) == 1)[\n            :,\n            0,\n        ]\n        keep_years = [0] + [int(i) + 1 for i in keep_years]\n        saves = []\n        for i in tqdm(keep_years):\n            save_as = os.path.join(temp_dir, f'temp_{i}.nc')\n            saves.append(save_as)\n            ds.isel(time=slice(i, i + 1)).load().to_netcdf(save_as)\n        # move the old file\n        _tempf = os.path.join(temp_dir, 'temp_merge.nc')\n        get_logger().warning(f'Merging {saves} -> {_tempf}')\n        _merge_sources(saves, _tempf)\n        ds = load_glob(_tempf)\n        if (t_new_len := len(ds['time'])) > t_span + 1:\n            raise ValueError(\n                f'{t_new_len} too long! Started with {t_len} and {t_span}',\n            )\n        get_logger().warning('Timestamp issue solved')\n\n        os.rename(path, os.path.join(os.path.split(path)[0], 'faulty_merged.nc'))\n        os.rename(_tempf, path)\n    get_logger().warning(f'_remove_duplicate_time_stamps - > Fixed!')\n\n\ndef _remove_duplicate_time_stamps(path):  # pragma: no cover\n    ds = load_glob(path)\n    t_len = len(ds['time'])\n    if t_len <= 1:\n        raise ValueError(f'No time length in {path}')\n    if t_len > (t_span := (ds['time'].values[-1].year - ds['time'].values[0].year)) + 1:\n        get_logger().warning(\n            f'Finding {t_len} timestamps in {t_span} years - removing duplicates',\n        )\n        if ds.nbytes / 1e6 < 1_000:\n            _quick_drop_duplicates(ds, t_span, t_len, path)\n        else:\n            _drop_duplicates_carefully(ds, t_span, t_len, path)\n\n\ndef _remap_and_merge(\n    cdo_int,\n    cdo,\n    historical_path: str,\n    source: str,\n    target_grid: ty.Union[bool, str],\n    working_dir: str,\n    f_tmp: str,\n) -> None:  # pragma: no cover\n    \"\"\"The function `_remap_and_merge` merges two input files,\n    `historical_path` and `source`, and if an exception occurs, it regrids the\n    files and tries again.\n\n    :param cdo_int: The `cdo_int` parameter is an instance of the `cdo` module, which is used for\n    executing CDO commands in Python\n    :param cdo: The `cdo` parameter is an instance of the `cdo.Cdo` class. It is used to interact with\n    the Climate Data Operators (CDO) library, which provides a collection of command-line tools for\n    working with climate and weather data\n    :param historical_path: The `historical_path` parameter is a string that represents the path to a\n    file containing historical data\n    :type historical_path: str\n    :param source: The `source` parameter is a string that represents the path to the source file that\n    needs to be merged with the `historical_path` file\n    :type source: str\n    :param target_grid: The `target_grid` parameter is a string that specifies the path to the target\n    grid file. It is used in the `remapbil` function to regrid the input data to the target grid before\n    merging\n    :type target_grid: str\n    :param working_dir: The `working_dir` parameter is a string that represents the directory where\n    temporary files will be stored during the execution of the `_remap_and_merge` function\n    :type working_dir: str\n    :param f_tmp: The `f_tmp` parameter is a string that represents the path to the output file where\n    the merged data will be saved\n    :type f_tmp: str\n    \"\"\"\n    try:\n        cdo_int.mergetime(input=[historical_path, source], output=f_tmp)\n    except cdo.CDOException as e:  # pragma: no cover\n        get_logger().error(f\"Ran into {e}, let's regrid first and retry\")\n        if target_grid == False:\n            raise ValueError(\n                f'Cannot merge {historical_path} and {source} since target grid is False and we ran into {e}',\n            ) from e\n        cdo_int.remapbil(\n            target_grid,\n            input=historical_path,\n            output=(_a := os.path.join(working_dir, '_a.nc')),\n        )\n        cdo_int.remapbil(\n            target_grid,\n            input=source,\n            output=(_b := os.path.join(working_dir, '_b.nc')),\n        )\n        cdo_int.mergetime(input=[_a, _b], output=f_tmp)\n\n\ndef _remove_bad_vars(path):\n    \"\"\"The function `_remove_bad_vars` removes specified variables from a\n    dataset and replaces the original file with the modified dataset.\n\n    :param path: The `path` parameter is a string that represents the file path to the dataset that\n    needs to be processed\n    \"\"\"\n    log = get_logger()\n    to_delete = config['analyze']['remove_vars'].split()\n    ds = load_glob(path)\n    drop_any = False\n    for var in to_delete:  # pragma: no cover\n        if var in ds.data_vars:\n            log.warning(f'{var} in dataset from {path}')\n            drop_any = True\n            ds = ds.load()\n            ds = ds.drop_vars(var)\n    if drop_any:  # pragma: no cover\n        log.error(f'Replacing {path} after dropping at least one of {to_delete}')\n        os.remove(path)\n        ds.to_netcdf(path)\n        if not os.path.exists(path):\n            raise RuntimeError(f'Data loss, somehow {path} got removed!')\n\n\ndef _check_time_range(path, max_time, min_time, ma_window):\n    \"\"\"The function `_check_time_range` checks if the time stamps in a dataset\n    fall within a specified time range and raises an error if the number of\n    time stamps is less than a specified moving average window size.\n\n    :param path: The `path` parameter is the file path or file pattern that specifies the location of\n    the data files to be loaded\n    :param max_time: The maximum time value to consider in the time range\n    :param min_time: The `min_time` parameter represents the minimum time or date range for the data. It\n    is a tuple with three elements representing the year, month, and day respectively\n    :param ma_window: The `ma_window` parameter represents the moving average window size. It is used to\n    determine the minimum number of time stamps required within the specified time range (`[min_time,\n    max_time]`) in order to proceed with the calculation of the moving average. If the number of time\n    stamps within the time\n    \"\"\"\n    ds = load_glob(path)\n    times = ds['time'].values\n    time_mask = times < _native_date_fmt(times, max_time)\n    if min_time != (0, 1, 1):\n        # CF time does not always support year 0\n        time_mask &= times > _native_date_fmt(times, min_time)\n    if time_mask.sum() < float(ma_window):\n        message = f'Data from {path} has {time_mask.sum()} time stamps in [{min_time}, {max_time}]'\n        raise NoDataInTimeRangeError(message)\n\n\ndef _run_mean_patch(f_start, f_rm, f_out, ma_window, var_name, var_rm_name):\n    \"\"\"Patch running mean file, since cdo decreases the length of the file by\n    the ma_window, merging two files of different durations results in bad\n    data.\n\n    As a solution, we take the original file (f_start) and use it's\n    shape (like the number of timestamps) and fill those timestamps\n    where the value of the running mean is defined. Everything else is\n    set to zero.\n    \"\"\"\n    ds_base = load_glob(f_start)\n    ds_rm = load_glob(f_rm)\n    ds_out = ds_base.copy()\n\n    # Replace timestamps with the CDO computed running mean\n    data = np.zeros(ds_base[var_name].shape, ds_base[var_name].dtype)\n    data[:] = np.nan\n    ma_window = int(ma_window)\n    data[ma_window // 2 : 1 - ma_window // 2] = ds_rm[var_name].values\n    ds_out[var_name].data = data\n\n    # Patch variables and save\n    ds_out = ds_out.rename({var_name: var_rm_name})\n    ds_out.attrs = ds_rm.attrs\n    ds_out.to_netcdf(f_out)\n\n\nclass NoDataInTimeRangeError(Exception):\n    pass\n\n\ndef _fmt_date(date: tuple) -> str:\n    \"\"\"The function `_fmt_date` takes a tuple representing a date and returns a\n    formatted string in the format 'YYYY-MM-DD'.\n\n    :param date: The `date` parameter is a tuple containing three elements: year, month, and day\n    :type date: tuple\n    :return: a formatted string representing the date in the format \"YYYY-MM-DD\".\n    \"\"\"\n    assert len(date) == 3\n    y, m, d = date\n    return f'{y:04}-{m:02}-{d:02}'\n\n\ndef _read_variable_id(path):\n    \"\"\"The function `_read_variable_id` reads the variable ID from a given path\n    and raises an error if the information is not available.\n\n    :param path: The `path` parameter is a string that represents the file path from which the variable\n    ID needs to be read\n    :return: the value of the 'variable_id' attribute from the file located at the given path.\n    \"\"\"\n    try:\n        return load_glob(path).attrs['variable_id']\n    except KeyError as e:  # pragma: no cover\n        message = f'When reading the variable_id from {path}, it appears no such information is available'\n        raise KeyError(message) from e\n", "coverage": [1, 1, 1, 1, null, 1, null, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 0, 0, 1, 1, 1, 1, 0, 0, null, null, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, null, 1, 1, 1, null, null, null, null, null, 1, 1, 1, null, null, null, 1, 1, null, 1, 1, 0, 0, 0, null, 0, 0, null, 1, 1, 1, null, null, null, null, null, null, null, null, null, 1, 0, null, 0, 0, null, null, 1, 1, null, null, null, null, 1, null, null, 1, 1, null, 1, 1, null, null, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1, 1, 1, 1, 1, 1, null, 1, 1, null, 1, 1, null, 1, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, null, null, 1, null, 1, 1, null, null, null, null, 1, 1, null, null, null, null, null, null, null, null, 1, 1, 1, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, 0, 0, 1, null, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, null, null, null, null, null, 1, null, null, 1, 0, 0, 0, 0, 0, 0, 0, null, 0, 0, null, null, 1, 0, null, null, 0, 0, 0, null, null, null, 0, 0, 0, 0, 0, 0, null, 0, 0, 0, 0, 0, 0, null, null, 0, null, 0, 0, 0, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, 1, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, null, 0, 1, 0, 0, null, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, 1, 1, 1, 1, 1, null, null, 1, 1, 1, null, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, 1, 1, null, null, null]}, {"name": "optim_esm_tools/analyze/region_calculation.py", "source": "import optim_esm_tools as oet\nimport numpy as np\n\nimport typing as ty\n\n\nimport xarray as xr\nimport functools\nfrom optim_esm_tools.analyze.tools import (\n    weighted_mean_array,\n    _weighted_mean_array_numba,\n    running_mean,\n    running_mean_array,\n)\nimport scipy\n\n\nclass RegionPropertyCalculator:\n    \"\"\"\n    For a given region, calculate properties that can be used to assess how special a given region is.\n\n    For this purpose, three essential elements are required:\n     - The dataset of interest (ds_global)\n     - The pi-control dataset of interest (ds_pi)\n     - The region of interest (mask)\n    \"\"\"\n\n    def __init__(\n        self,\n        ds_global: xr.Dataset,\n        ds_pi: xr.Dataset,\n        mask: ty.Union[np.ndarray, xr.DataArray],\n        field: ty.Optional[str] = None,\n        ds_local: ty.Optional[xr.Dataset] = None,\n        ds_pi_local: ty.Optional[xr.Dataset] = None,\n        _tropic_lat: ty.Union[int, float] = float(\n            oet.config.config[\"analyze\"][\"tropics_latitude\"],\n        ),\n        _rm_years: int = int(oet.config.config[\"analyze\"][\"moving_average_years\"]),\n    ):\n        \"\"\"\n\n        Args:\n            ds_global (xr.Dataset): dataset of interest\n            ds_pi (xr.Dataset): pi-control dataset corresponding to ds_global\n            mask (ty.Union[np.ndarray, xr.DataArray]): region of interest in the dataset of interest\n            field (ty.Optional[str], optional): variable_id in the dataset to extract. Defaults to None in which case we read it from ds_global.\n            ds_local (ty.Optional[xr.Dataset], optional): Masked ds_global. Defaults to None, and we compute the masked dataset using ds_global and mask.\n            ds_pi_local (ty.Optional[xr.Dataset], optional): Masked ds_pi. Defaults to, similar to ds_local for the pi-control dataset.\n            _tropic_lat (ty.Union[int, float], optional): Value of tropics to use. Defaults to float( oet.config.config[\"analyze\"][\"tropics_latitude\"], ).\n            _rm_years (int, optional): Number of years for running mean-calculations. Defaults to int(oet.config.config[\"analyze\"][\"moving_average_years\"]).\n        \"\"\"\n        self._tropic_lat = _tropic_lat\n        self._rm_years = _rm_years\n        self.ds_global = ds_global\n        self.ds_pi = ds_pi\n        self.mask = mask\n        assert mask.dtype == \"bool\"\n        self.field = field or ds_global.variable_id\n\n        da_mask = self.mask_to_da(mask)\n        self.da_mask: xr.DataArray = da_mask\n        kw = dict(mask=da_mask, add_global_mask=False)\n        _common_keys = [\n            self.field_rm,\n            \"time\",\n            \"cell_area\",\n            *oet.config.config[\"analyze\"][\"lon_lat_dim\"].split(\",\"),\n        ]\n        keep_keys_sc = [self.field, *_common_keys]\n        keep_keys_pi = [self.field_detrend_rm, f\"{self.field}_detrend\", *_common_keys]\n\n        self.ds_local = ds_local or oet.analyze.xarray_tools.mask_to_reduced_dataset(\n            self.ds_global,\n            keep_keys=keep_keys_sc,\n            **kw,\n        )\n        self.ds_pi_local = (\n            ds_pi_local\n            or oet.analyze.xarray_tools.mask_to_reduced_dataset(\n                self.ds_pi,\n                keep_keys=keep_keys_pi,\n                **kw,\n            )\n        )\n        self._cache: ty.Dict[str, ty.Any] = dict()\n\n    @property\n    def field_detrend_rm(self) -> str:\n        return f\"{self.field}_detrend_run_mean_{self._rm_years}\"\n\n    def mask_to_da(\n        self,\n        mask: ty.Union[np.ndarray, xr.DataArray],\n        fall_back_field: str = \"cell_area\",\n    ) -> xr.DataArray:\n        \"\"\"Handle masks of either numpy or xarray types and make sure an xarray.DataArray is returned\n\n        Args:\n            mask (ty.Union[np.ndarray, xr.DataArray]): _description_\n            fall_back_field (str, optional): _description_. Defaults to \"cell_area\".\n\n        Raises:\n            ValueError: _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        if isinstance(mask, xr.DataArray):\n            return mask\n        if isinstance(mask, np.ndarray):\n            mask_da = self.ds_global[fall_back_field].astype(np.bool_)\n            mask_da.data = mask\n            return mask_da\n        raise ValueError\n\n    @property\n    def field_rm(self) -> str:\n        return f\"{self.field}_run_mean_{self._rm_years}\"\n\n    def weigthed_mean_cached(\n        self,\n        field: str,\n        data_set: str = \"ds_local\",\n    ) -> np.ndarray:\n        k = f\"{data_set}-weighted_mean_time-{field}\"\n        if k not in self._cache:\n            _ds = getattr(self, data_set, \"NO DATA\")\n            assert isinstance(\n                _ds,\n                xr.Dataset,\n            ), f\"{_ds} is not an xr.Dataset or not available at {data_set}\"\n\n            self._cache[k] = weighted_mean_array(_ds, field=field)\n        return self._cache[k]\n\n    def _calc_max_end(self) -> float:\n        \"\"\"Calculte the difference between the end and the maximum calue in the default moving average filtered time series\"\"\"\n        _m = self.weigthed_mean_cached(self.field_rm, \"ds_local\")\n        _rm_years = int(self._rm_years)\n        end = _m[-_rm_years // 2]\n        return max(np.abs(np.nanmax(_m) - end), np.abs(np.nanmin(_m) - end))\n\n    def _calc_std_trop(self, **kw) -> float:\n        \"\"\"Calculte the average standard deviation in the scenario dataset\"\"\"\n        return self._calc_std_trop_inner(**kw, values_from=\"scenario\")\n\n    def _calc_pi_std_trop(self, **kw) -> float:\n        \"\"\"Calculte the average standard deviation in the pi-control dataset\"\"\"\n        return self._calc_std_trop_inner(**kw, values_from=\"pi\")\n\n    @oet.utils.check_accepts(accepts=dict(values_from=[\"scenario\", \"pi\"]))\n    def _calc_std_zone(\n        self,\n        std_field: str = \"std detrended\",\n        values_from: str = \"scenario\",\n        remove_zero_std: bool = False,\n        zone_name: str = \"tropics\",\n    ) -> float:\n        ds_values = dict(pi=self.ds_pi, scenario=self.ds_global)[values_from]\n        mask = self.mask\n\n        _, lat = np.meshgrid(self.ds_global.lon.values, self.ds_global.lat.values)\n        if not isinstance(mask, np.ndarray):\n            mask = mask.values\n        assert np.all(\n            np.diff(self.ds_global.lat.values) < 0,\n        ), \"Lats should be in descending order\"\n\n        lats = lat[mask.astype(np.bool_)].flatten()\n        use_mask = self._mask_by_zone(lats, zone_name=zone_name)\n        if remove_zero_std:\n            log = oet.get_logger()\n            log.debug(f\"Use {use_mask.sum()} datapoints, remove zero std\")\n            use_mask &= ds_values[std_field].values > 0\n            log.debug(f\"{use_mask.sum()} datapoints left\")\n\n        data = ds_values[std_field].values[use_mask]\n        weights = ds_values[\"cell_area\"].values[use_mask]\n\n        dinom = _weighted_mean_array_numba(\n            data=data,\n            weights=weights,\n            has_time_dim=False,\n        )\n\n        return float(dinom)\n\n    @oet.utils.check_accepts(accepts=dict(values_from=[\"scenario\", \"pi\"]))\n    def _calc_std_trop_inner(\n        self,\n        min_frac: float = 0.5,\n        std_field: str = \"std detrended\",\n        values_from: str = \"scenario\",\n        remove_zero_std: bool = False,\n    ) -> float:\n        _tropic_lat = self._tropic_lat\n        ds_global = self.ds_global\n        ds_values = dict(pi=self.ds_pi, scenario=ds_global)[values_from]\n        mask = self.mask\n        log = oet.get_logger()\n        _, lat = np.meshgrid(ds_global.lon.values, ds_global.lat.values)\n        if not isinstance(mask, np.ndarray):\n            mask = mask.values\n        assert np.all(\n            np.diff(ds_global.lat.values) < 0,\n        ), \"Lats should be in descending order\"\n\n        lats = lat[mask.astype(np.bool_)].flatten()\n        mask_trop_2d = (lat < _tropic_lat) & (lat > -_tropic_lat)\n\n        if (\n            np.sum([(lats > -_tropic_lat) & (lats < _tropic_lat)]) / len(lats)\n            > min_frac\n        ):\n            log.debug(\"Is tropical\")\n            is_trop = np.zeros_like(mask)\n            is_trop[mask_trop_2d] = True\n            use_mask = is_trop\n        else:\n            log.debug(\"Is extra tropical en een annanas\")\n            extra_trop = np.ones_like(mask)\n            extra_trop[mask_trop_2d] = False\n            use_mask = extra_trop\n        if remove_zero_std:\n            log.debug(f\"Use {use_mask.sum()} datapoints, remove zero std\")\n            use_mask &= ds_values[std_field].values > 0\n            log.debug(f\"{use_mask.sum()} datapoints left\")\n        dinom = _weighted_mean_array_numba(\n            data=ds_values[std_field].values[use_mask],\n            weights=ds_values[\"cell_area\"].values[use_mask],\n            has_time_dim=False,\n        )\n\n        return float(dinom)\n\n    def _calc_start_end(self, offset: ty.Optional[int] = None) -> float:\n        offset = offset or self._rm_years // 2\n        assert isinstance(offset, int)\n        data = self.ds_local[self.field_rm].values\n        weights = self.ds_local[\"cell_area\"].values\n        se = data[-offset] - data[offset]\n        # # make sure that there is at least one point in time that is not the same\n        # mask = np.any(data[1:]!=data[0], axis=0)\n        res = _weighted_mean_array_numba(data=se, weights=weights, has_time_dim=False)\n        assert isinstance(res, float)\n        return res\n\n    def _calc_siconc_norm(self, calculate_vars=(\"siconc\", \"siconca\")) -> float:\n        return float(\n            (\n                calculate_norm(\n                    ds_global=self.ds_global,\n                    mask=self.da_mask,\n                    field=self.field,\n                )\n                if self.field in calculate_vars\n                else np.nan\n            ),\n        )\n\n    def _calc_rho_frac(self, rm_0: int, rm_1: ty.Optional[int] = None) -> float:\n        ds = self.ds_local\n        field = self.field\n        ar = ds[field].values\n        if len(ar.shape) > 1:\n            ar = ds[field].mean(\"lat lon\".split()).values\n        ar_0 = running_mean(ar, rm_0)\n        ar_1 = ar if rm_1 is None else running_mean(ar, rm_1)\n        return np.nanstd(ar_1 - ar_0)\n\n    def _calc_pi_std(self) -> float:\n        return float(\n            np.nanstd(\n                self.weigthed_mean_cached(\n                    f\"{self.field}_detrend_run_mean_{self._rm_years}\",\n                    \"ds_pi_local\",\n                ),\n            ),\n        )\n\n    def _calc_pi_std_rmx(self, rm_years) -> float:\n        return float(\n            np.nanstd(\n                running_mean(\n                    self.weigthed_mean_cached(\n                        f\"{self.field}_detrend\",\n                        \"ds_pi_local\",\n                    ),\n                    rm_years,\n                ),\n            ),\n        )\n\n    def _calc_psymmetry(self) -> np.float64:\n        values = self.weigthed_mean_cached(self.field, \"ds_local\")\n        return oet.analyze.time_statistics.calculate_symmetry_test(values=values)\n\n    def _calc_psymmetry_cached(self) -> np.float64:\n        values = self.weigthed_mean_cached(self.field, \"ds_local\")\n        values_as_tuple = tuple(values)\n        return cache_psym_test(values=values_as_tuple)\n\n    def _calc_pdip(self) -> float:\n        values = self.weigthed_mean_cached(self.field, \"ds_local\")\n        return oet.analyze.time_statistics.calculate_dip_test(values=values)\n\n    def _calc_j2(self, *a, do_raise=False, **k) -> float:\n        values = self.weigthed_mean_cached(self.field_rm, \"ds_local\")\n        try:\n            return np.divide(*_max_and_second_jump(values, *a, **k))\n        except ValueError as e:\n            if do_raise:\n                raise e\n            return np.nan\n\n    def _calc_jump_n_years(\n        self,\n        n_years: int = 10,\n        rm_years: ty.Optional[int] = None,\n    ) -> float:\n        a = self.weigthed_mean_cached(self.field, data_set=\"ds_local\")\n        rm_years = rm_years or self._rm_years\n        a_rm = running_mean(a, rm_years)\n        return np.nanmax(np.abs(a_rm[n_years:] - a_rm[:-n_years]))\n\n    def _calc_max_end_rmx(self, rm_alt=50, apply_max=True) -> float:\n        assert rm_alt % 2 == 0, rm_alt\n        _m = self.weigthed_mean_cached(self.field, \"ds_local\")\n        rm_x = running_mean(_m, rm_alt)\n        end = rm_x[-rm_alt // 2]\n\n        if not apply_max:\n            return np.nanmax(rm_x) - end\n        return max(np.abs(np.nanmax(rm_x) - end), np.abs(np.nanmin(rm_x) - end))\n\n    def _add_std_rm_alt_to_ds(\n        self,\n        ds_values: xr.Dataset,\n        set_field: str,\n        rm_alt: int,\n    ) -> None:\n        yearly_means = ds_values[f\"{self.field}_detrend\"].values\n        vals_2d = ds_values[\"cell_area\"].copy()\n        vals_2d.attrs[\"units\"] = \"std\"\n\n        rm_50_3d = running_mean_array(yearly_means, rm_alt)\n\n        vals_2d.data = np.nanstd(rm_50_3d, axis=0)\n        ds_values[set_field] = vals_2d\n\n    def _sigma_trop_rmx(self, values_from: str = \"scenario\", rm_alt: int = 50) -> float:\n        cache_to_field = f\"std_{self.field}_detrend_rm_{rm_alt}\"\n        ds_values = dict(pi=self.ds_pi, scenario=self.ds_global)[values_from]\n        if cache_to_field not in ds_values:\n            self._add_std_rm_alt_to_ds(ds_values, cache_to_field, rm_alt)\n\n        return self._calc_std_trop_inner(\n            remove_zero_std=True,\n            values_from=values_from,\n            std_field=cache_to_field,\n        )\n\n    def _mask_to_lats(self) -> np.ndarray:\n        assert np.all(\n            np.diff(self.ds_global.lat.values) < 0,\n        ), \"Lats should be in descending order\"\n        _, lat = np.meshgrid(self.ds_global.lon.values, self.ds_global.lat.values)\n        lats = lat[self.mask.astype(np.bool_)].flatten()\n        return lats\n\n    def _get_named_zone(self, zone_name: str) -> np.ndarray:\n        lats = self._mask_to_lats()\n        return self._mask_by_zone(zone_name=zone_name, lats=lats)\n\n    def _mask_by_zone(self, lats: np.ndarray, zone_name: str, **kw) -> np.ndarray:\n        zone_kw = dict(\n            tropics=dict(zone_bounds=np.array([-90, self._tropic_lat]), reflect=True),\n        )\n        if zone_name == \"tropics_land_or_water\":\n            oet.get_logger().warning(f\"Replaced {zone_name} with tropics\")\n            zone_name = \"tropics\"\n        return self._mask_by_zone_inner(lats, **zone_kw[zone_name])  # type: ignore\n\n    def _mask_by_zone_inner(\n        self,\n        lats: np.ndarray,\n        zone_bounds: np.ndarray,\n        reflect: bool = False,\n    ) -> np.ndarray:\n        _, lat = np.meshgrid(self.ds_global.lon.values, self.ds_global.lat.values)\n        if reflect:\n            zone_bounds = np.concatenate([-zone_bounds, zone_bounds])\n        assert np.all(\n            np.diff(self.ds_global.lat.values) < 0,\n        ), \"Lats should be in descending order\"\n        zone_bounds = np.sort(zone_bounds)[::-1]\n        zone_ranges = np.vstack([zone_bounds[:-1], zone_bounds[1:]])\n        overlaps = []\n        for z_left, z_right in zip(*zone_ranges):\n            this_n = np.sum((lats <= z_left) & (lats > z_right))\n            if reflect:\n                this_n += np.sum((lats >= -z_left) & (lats < -z_right))\n            overlaps.append(this_n)\n        zone_i = np.argmax(overlaps)\n        use_mask = (lat <= zone_ranges[0][zone_i]) & (lat > zone_ranges[1][zone_i])\n        if reflect:\n            use_mask |= (lat >= -zone_ranges[0][zone_i]) & (\n                lat < -zone_ranges[1][zone_i]\n            )\n        return use_mask\n\n    def calculate_mse_trop_rmx(\n        self,\n        start_or_max: str = \"start\",\n        zone_name: str = \"tropics\",\n        rm: int = 10,\n        field: ty.Optional[str] = None,\n    ):\n        mask = self._get_named_zone(zone_name)\n        data = self.ds_global[field or self.field].where(mask, drop=False).values\n        weights = self.ds_global[\"cell_area\"].values\n\n        trop_avg = _weighted_mean_array_numba(data, weights)\n        if rm is not None:\n            trop_avg = running_mean(trop_avg, rm)\n        offset = int(rm // 2 if rm else 0)\n        if start_or_max == \"start\":\n            se = trop_avg[-1 - offset] - trop_avg[offset]\n            return se\n        elif start_or_max == \"max\":\n            end = trop_avg[offset]\n            return max(\n                np.abs(np.nanmax(trop_avg) - end),\n                np.abs(np.nanmin(trop_avg) - end),\n            )\n        raise NotImplementedError(f\"Method {start_or_max} not available\")\n\n    def max_rmx(self, rm_alt=50, field=None, values_from=\"ds_local\"):\n        _m = self.weigthed_mean_cached(field or self.field, values_from)\n        rm_x = running_mean(_m, rm_alt)\n        return np.nanmax(rm_x)\n\n    def calculate(self) -> ty.Dict[str, ty.Union[str, int, float, bool]]:  # type: ignore\n        doc = dict(\n            p_sym_mi=self._calc_psymmetry_cached(),\n            p_dip=self._calc_pdip(),\n            se=abs(self._calc_start_end()),\n            J2_rm=self._calc_j2(),\n            J2_min0_rm=self._calc_j2(force_same_sign=False, min_distance=0),\n            area_sum=float(self.ds_local[\"cell_area\"].sum()),\n            j=self._calc_jump_n_years(),\n            j_50=self._calc_jump_n_years(50),\n            variable_id=self.field,\n            _pi_std=self._calc_pi_std(),\n            _pi_std_rm50=self._calc_pi_std_rmx(50),\n            _frac_norm=self._calc_siconc_norm(),\n            _frac_rho10_50=self._calc_rho_frac(rm_0=50, rm_1=10),\n            me_rm50=self._calc_max_end_rmx(rm_alt=50),\n            me_rm50_signed=self._calc_max_end_rmx(rm_alt=50, apply_max=False),\n            std_pi_trop_rm50=self._sigma_trop_rmx(values_from=\"pi\", rm_alt=50),\n        )\n        doc.update(\n            dict(\n                max_jump=np.divide(doc[\"j\"], doc[\"_pi_std\"]),\n                max_jump_rm50=np.divide(doc[\"j_50\"], doc[\"_pi_std_rm50\"]),\n                e_s=np.divide(doc[\"se\"], doc[\"_frac_norm\"]),\n                rho10_50=np.divide(doc[\"se\"], doc[\"_frac_rho10_50\"]),\n                me_std_pi_trop_rm50=np.divide(doc[\"me_rm50\"], doc[\"std_pi_trop_rm50\"]),\n            ),\n        )\n        zone = 'tropics'\n        doc[\"se_std_trop_domain\"] = np.divide(\n            doc[\"se\"],\n            self._calc_std_zone(remove_zero_std=True, zone_name=zone),\n        )\n        doc[\"se_pi_std_trop_domain\"] = np.divide(\n            doc[\"se\"],\n            self._calc_std_zone(remove_zero_std=True, zone_name=zone, values_from=\"pi\"),\n        )\n        doc[\"mj_std_trop_domain\"] = np.divide(\n            doc[\"j\"],\n            self._calc_std_zone(remove_zero_std=True, zone_name=zone),\n        )\n        doc[\"mj_pi_std_trop_domain\"] = np.divide(\n            doc[\"j\"],\n            self._calc_std_zone(remove_zero_std=True, zone_name=zone, values_from=\"pi\"),\n        )\n        doc[\"_se_trop\"] = self.calculate_mse_trop_rmx(\"start\", rm=10)\n        doc[\"_me_trop_rm50\"] = self.calculate_mse_trop_rmx(\"max\", rm=50)\n\n        doc[\"se_vs_se_trop\"] = np.divide(self._calc_start_end(), doc[\"_se_trop\"])\n        doc[\"me_vs_me_trop_rm50\"] = np.divide(doc[\"me_rm50\"], doc[\"_me_trop_rm50\"])\n        doc[\"me_max_rm50\"] = np.divide(doc[\"me_rm50\"], self.max_rmx(50))\n        doc['me_pi_std_rm50'] = np.divide(doc['me_rm50'], doc['_pi_std_rm50'])\n        doc['se_pi_std'] = np.divide(doc['se'], doc['_pi_std'])\n        return {k: (v if v is not None else np.nan) for k, v in doc.items()}\n\n\ndef summarize_stats(\n    ds_global: xr.Dataset,\n    ds_pi: xr.Dataset,\n    mask: ty.Union[np.ndarray, xr.DataArray],\n    field: str,\n) -> ty.Dict[str, ty.Union[str, int, float, bool]]:\n    return RegionPropertyCalculator(\n        ds_global=ds_global,\n        ds_pi=ds_pi,\n        mask=mask,\n        field=field,\n    ).calculate()\n\n\ndef _max_and_second_jump(\n    values,\n    n_years_difference: int = 10,\n    min_distance: int = 10,\n    force_same_sign: bool = True,\n):\n    difference = values[n_years_difference:] - values[:-n_years_difference]\n\n    max_index = np.nanargmax(np.abs(difference))\n\n    slice_disabled = slice(\n        np.max(\n            [int(max_index) - min_distance, 0],\n        ),\n        np.min([int(max_index) + min_distance, len(difference)]),\n    )\n\n    delta_nanned = difference.copy()\n    delta_nanned[slice_disabled] = np.nan\n    if (n_nan := np.sum(np.isnan(delta_nanned))) < min_distance:\n        raise RuntimeError(\n            f\"Should have at least {min_distance} nans, got {n_nan}\",\n        )\n    sign = np.sign(difference[max_index])\n    sign = sign if force_same_sign else -sign\n    second_jump_index = np.nanargmax(sign * delta_nanned)\n\n    return difference[max_index], difference[second_jump_index]\n\n\ndef find_max_in_equal_area(\n    ds,\n    target_area,\n    time_index=0,\n    field=None,\n    tqdm=False,\n    step_size=1.0,\n    max_multiplier=1000,\n):\n    field = field or ds.attrs[\"variable_id\"]\n    field = f\"{field}_run_mean_10\"\n\n    # Shift running mean\n    time_index = 5 if time_index == 0 else time_index\n    time_index = -5 if time_index == -1 else time_index\n\n    data = ds[field].isel(time=time_index)\n    area = ds[\"cell_area\"].load()\n    vals = data.values.copy()\n    nona_vals = vals.copy()\n    nona_vals[np.isnan(vals)] = np.nanmin(vals)\n    if not target_area:\n        return dict(\n            area_m2=target_area,\n            max_in_sel=nona_vals.max(),\n        )\n    da_m = data.copy()\n    da_m.data[:] = np.nan\n\n    max_distance_km = oet.analyze.clustering.infer_max_step_size(\n        data.lat.values,\n        data.lon.values,\n    )\n\n    area_perc = target_area / float(area.sum())\n    iter_perc = 1 - area_perc * step_size * (\n        np.linspace(1, max_multiplier, max_multiplier)\n    )\n    iter_perc = iter_perc[(iter_perc > 0) & (iter_perc < 1)]\n    prev_mask = None\n    n_mask = 1\n    for i, threshold in enumerate(oet.utils.tqdm(iter_perc, disable=not tqdm)):\n        mask = vals > np.percentile(nona_vals, threshold * 100)\n        if np.sum(mask) / n_mask < 1.03 and i < len(iter_perc) - 1:\n            # Skip steps that are too close together\n            continue\n        n_mask = np.sum(mask)\n        masks, _ = oet.analyze.clustering.build_cluster_mask(\n            mask,\n            max_distance_km=max_distance_km,\n            lat_coord=data.lat.values,\n            lon_coord=data.lon.values,\n        )\n        for m in masks[:1]:\n            da_m.data = m\n\n            area_m2 = float(area.where(da_m).sum())\n            if area_m2 < target_area and i < len(iter_perc) - 1:\n                if prev_mask is None:\n                    prev_mask = da_m.copy()\n                elif da_m.sum() > prev_mask.sum():\n                    prev_mask = da_m.copy()\n                continue\n\n            da_sel = data.where(da_m)\n\n            max_in_sel = oet.analyze.tools._weighted_mean_2d_numba(\n                da_sel.where(da_m).values,\n                weights=area.values,\n            )\n            if prev_mask is None or target_area > area_m2:\n                return dict(\n                    area_m2=area_m2,\n                    max_in_sel=max_in_sel,\n                )\n\n            prev_in_sel = oet.analyze.tools._weighted_mean_2d_numba(\n                da_sel.where(prev_mask).values,\n                weights=area.values,\n            )\n            prev_area = float(area.where(prev_mask).sum())\n\n            itp_max = scipy.interpolate.interp1d(\n                [prev_area, area_m2],\n                [prev_in_sel, max_in_sel],\n            )(target_area)\n            return dict(\n                area_m2=target_area,\n                max_in_sel=itp_max,\n            )\n    # At some point, you just are going to try finding larger areas than possible with this kind of dataset. E.g. when you require half of the area surface of the earth for sea-ice. Which will fail because only a fraction is coverered by a portion of sea ice\n    return dict(\n        area_m2=1e-10,\n        max_in_sel=1e-10,\n    )\n\n\n@functools.lru_cache(maxsize=int(1e9))\ndef cache_psym_test(values: np.ndarray, **kw) -> np.float64:\n    if not isinstance(values, np.ndarray):\n        values = np.array(values)\n    return oet.analyze.time_statistics.calculate_symmetry_test(values=values, **kw)\n\n\ndef calculate_norm(\n    ds_global: xr.Dataset,\n    mask: ty.Union[np.ndarray, xr.DataArray],\n    field: str,\n) -> float:\n    if hasattr(mask, \"values\"):\n        mask = mask.values\n    use_mask = mask.copy()\n    use_mask &= np.any(ds_global[f\"{field}_run_mean_10\"].values > 0, axis=0)\n    target = float(ds_global[\"cell_area\"].where(use_mask).sum())\n    kw = dict(\n        ds=ds_global,\n        target_area=target,\n        field=field,\n    )\n    t0 = find_max_in_equal_area(\n        **kw,\n        time_index=0,\n    )\n    t1 = find_max_in_equal_area(\n        **kw,\n        time_index=-1,\n    )\n    return max(t0[\"max_in_sel\"], t1[\"max_in_sel\"])\n\n\ndef jump_n_years(\n    field: str,\n    ds_local: xr.Dataset,\n    n_years: int = 10,\n    moving_average_years: ty.Optional[int] = None,\n) -> np.float64:\n    ma = moving_average_years or int(\n        oet.config.config[\"analyze\"][\"moving_average_years\"],\n    )\n    use_field = f\"{field}_run_mean_{ma}\"\n    a = ds_local[use_field].mean(\"lat lon\".split()).values\n\n    return np.nanmax(np.abs(a[n_years:] - a[:-n_years]))\n", "coverage": [1, 1, null, 1, null, null, 1, 1, 1, null, null, null, null, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, null, null, null, null, 1, 1, null, 1, null, null, null, null, 1, null, null, null, null, null, null, null, 1, null, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 0, null, 1, 1, 1, null, 1, null, null, null, null, 1, 1, 1, 1, null, null, null, null, 1, 1, null, 1, null, 0, 0, 0, 0, null, 1, null, 0, null, 1, null, 0, null, 1, 1, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, 1, null, null, null, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, null, 1, null, null, null, null, null, 1, null, 1, 1, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, null, 1, 1, null, 1, null, null, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, null, null, null, 1, null, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, 1, 0, 0, null, 1, 1, 1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, 0, 0, 0, 0, null, 1, null, null, null, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, null, 1, 1, 1, null, 1, null, null, null, null, null, 1, 1, 1, null, 1, null, 1, 1, null, 1, 1, 1, 1, 1, null, 1, null, null, null, null, null, 1, 1, null, null, 1, 1, 1, null, 1, 1, 1, null, 1, 1, null, null, 1, 0, 0, 1, null, 1, null, null, null, null, null, 1, 1, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, null, 1, null, null, null, null, null, null, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, null, 0, null, 1, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, 1, 1, null, null, null, 1, null, null, null, 1, null, null, null, 1, null, null, null, 1, 1, null, 1, 1, 1, 1, 1, 1, null, null, 1, null, null, null, null, null, 1, null, null, null, null, null, null, null, 1, null, null, null, null, null, 1, null, 1, null, 1, null, null, null, null, null, null, 1, 1, 1, 0, null, null, 1, 1, 1, null, 1, null, null, 1, null, null, null, null, null, null, null, null, 0, 0, null, null, 0, 0, null, 0, 0, 0, 0, 0, 0, 0, null, null, null, 0, 0, null, 0, null, null, null, null, 0, 0, null, null, 0, 0, 0, 0, 0, 0, null, 0, 0, 0, null, null, null, null, null, 0, 0, null, 0, 0, 0, 0, 0, 0, 0, null, 0, null, 0, null, null, null, 0, 0, null, null, null, null, 0, null, null, null, 0, null, 0, null, null, null, 0, null, null, null, null, 0, null, null, null, null, null, 1, 1, 1, 1, 1, null, null, 1, null, null, null, null, 0, 0, 0, 0, 0, 0, null, null, null, null, 0, null, null, null, 0, null, null, null, 0, null, null, 1, null, null, null, null, null, 0, null, null, 0, 0, null, 0]}, {"name": "optim_esm_tools/analyze/region_finding.py", "source": "from optim_esm_tools.analyze.xarray_tools import mask_xr_ds\nfrom optim_esm_tools.region_finding import *\nfrom optim_esm_tools.utils import deprecated\n\nmask_xr_ds = deprecated(mask_xr_ds)\n", "coverage": [1, 1, 1, null, 1]}, {"name": "optim_esm_tools/analyze/time_statistics.py", "source": "import operator\nimport os\nimport typing as ty\nfrom functools import partial\n\nimport numpy as np\nimport scipy\nimport xarray as xr\n\nimport optim_esm_tools as oet\n\n\ndef calculate_dip_test(\n    ds: ty.Optional[xr.Dataset] = None,\n    field: ty.Optional[str] = None,\n    values: ty.Optional[np.ndarray] = None,\n    nan_policy: str = 'omit',\n):\n    \"\"\"[citation]:\n\n    Hartigan, P. M. (1985). Computation of the Dip Statistic to Test for Unimodality.\n    Journal of the Royal Statistical Society. Series C (Applied Statistics), 34(3),\n    320-325.\n    Code from:\n    https://pypi.org/project/diptest/\n    \"\"\"\n    values = _extract_values_from_sym_args(values, ds, field, nan_policy)\n    import diptest\n\n    if len(values) < 3:  # pragma: no cover\n        # At least 3 samples are needed\n        oet.config.get_logger().error('Dataset too short for diptest')\n        return None\n    _, pval = diptest.diptest(values, boot_pval=False)\n    return pval\n\n\ndef calculate_skewtest(\n    ds: ty.Optional[xr.Dataset] = None,\n    field: ty.Optional[str] = None,\n    values: ty.Optional[np.ndarray] = None,\n    nan_policy: str = 'omit',\n):\n    \"\"\"[citation] R.\n\n    B. D'Agostino, A. J. Belanger and R. B. D'Agostino Jr., \"A suggestion for using\n    powerful and informative tests of normality\", American Statistician 44, pp.\n    316-321, 1990.\n    Code from:\n    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skewtest.html\n    \"\"\"\n    values = _extract_values_from_sym_args(values, ds, field, nan_policy)\n    if sum(~np.isnan(values)) < 8:  # pragma: no cover\n        # At least 8 samples are needed\n        oet.config.get_logger().error('Dataset too short for skewtest')\n        return None\n    return scipy.stats.skewtest(values, nan_policy=nan_policy).pvalue\n\n\ndef _extract_values_from_sym_args(\n    values: ty.Optional[np.ndarray] = None,\n    ds: ty.Optional[xr.Dataset] = None,\n    field: ty.Optional[str] = None,\n    nan_policy: str = 'omit',\n) -> np.ndarray:\n    _ds_args_are_none = ds is None and field is None\n    if values is not None:\n        if not _ds_args_are_none:\n            raise TypeError(\n                f'Got both values, dataset and field. Either provide values or ds and field.',\n            )\n        if not isinstance(values, np.ndarray):\n            try:\n                values = values.values\n            except Exception as error:\n                raise TypeError(\n                    f'values should be np.ndarray, got {type(values)}',\n                ) from error\n        if not len(values.shape) == 1:\n            raise TypeError(f'values have wrong shape {values.shape}, should be 1D')\n\n    else:\n        if _ds_args_are_none:\n            raise TypeError('No ds is provided or field is missing!')\n\n        da = ds[field]\n        da = da.mean(set(da.dims) - {'time'})\n        values = da.values\n\n    if nan_policy == 'omit':\n        values = values[~np.isnan(values)]\n    else:  # pragma: no cover\n        message = 'Not sure how to deal with nans other than omit'\n        raise NotImplementedError(message)\n\n    return values\n\n\ndef calculate_symmetry_test(\n    ds: ty.Optional[xr.Dataset] = None,\n    field: ty.Optional[str] = None,\n    values: ty.Optional[np.ndarray] = None,\n    nan_policy: str = 'omit',\n    test_statistic: str = 'MI',\n    n_repeat: int = int(oet.config.config['analyze']['n_repeat_sym_test']),\n    _fast_mode: bool = True,\n    _fast_above: float = 0.05,\n    _fast_min_repeat: int = 2,\n    **kw,\n) -> np.float64:\n    \"\"\"The function `calculate_symmetry_test` calculates the symmetry test\n    statistic for a given dataset and field using the R package `rpy_symmetry`.\n\n    :param values: A numpy array with the values to calculate the diptest on. Should be 1D, and `ds` and\n    `field` should be None\n    :param ds: An xarray Dataset containing the data\n    :type ds: xr.Dataset\n    :param field: The `field` parameter is an optional string that specifies the field or variable from\n    the dataset (`ds`) that you want to calculate symmetry test for. If `field` is not provided, the\n    function will calculate the symmetry test for all variables in the dataset\n    :type field: ty.Optional[str]\n    :param nan_policy: The `nan_policy` parameter determines how to handle missing values (NaNs) in the\n    data. The default value is 'omit', which means that any NaN values will be excluded from the\n    calculation, defaults to omit\n    :type nan_policy: str (optional)\n    :param test_statistic: The `test_statistic` parameter is a string that specifies the test statistic\n    to be used in the symmetry test. It determines how the symmetry of the data will be measured,\n    defaults to MI\n    :type test_statistic: str (optional)\n    :param n_repeat: The parameter `n_repeat` specifies the number of times the symmetry test should be\n    repeated. The symmetry test in R does give non-deterministic results. As such repeat a test this\n    many times and take the average\n    :type n_repeat: int\n    :param _fast_mode: if `_fast_mode` is activated only run the test `_fast_min_repeat` times if the first try is above `_fast_above`\n    :param _fast_above: if `_fast_mode` is activated only run the test `_fast_min_repeat` times if the first try is above `_fast_above`\n    :param _fast_min_repeat: if `_fast_mode` is activated only run the test `_fast_min_repeat` times if the first try is above `_fast_above`\n\n    :return: The function `calculate_symmetry_test` returns a `np.float64` value.\n\n    [citation]:\n        Mira A (1999) Distribution-free test for symmetry based on Bonferroni's measure.\n        J Appl Stat 26(8):959\u2013972. https://doi.org/10.1080/02664769921963\n        Code from:\n        https://cran.r-project.org/web/packages/symmetry\n        Code at:\n        https://github.com/JoranAngevaare/rpy_symmetry\n    \"\"\"\n    import rpy_symmetry as rsym\n\n    values = _extract_values_from_sym_args(values, ds, field, nan_policy)\n\n    results = [rsym.p_symmetry(values, test_statistic=test_statistic, **kw)]\n    if _fast_mode:\n        n_repeat = n_repeat - 1 if results[0] < _fast_above else _fast_min_repeat - 1\n    for _ in range(n_repeat):\n        if len(results) > 3 and np.std(results) <= np.mean(results) / 10:\n            break\n        results.append(rsym.p_symmetry(values, test_statistic=test_statistic, **kw))\n    oet.get_logger().debug(\n        f'Evaluated {test_statistic} {len(results)} times: {results}',\n    )\n    return np.mean(results)\n\n\ndef calculate_n_breaks(\n    ds: ty.Optional[xr.Dataset] = None,\n    field: ty.Optional[str] = None,\n    values: ty.Optional[np.ndarray] = None,\n    nan_policy: str = 'omit',\n    penalty: ty.Optional[float] = None,\n    min_size: ty.Optional[int] = None,\n    jump: ty.Optional[int] = None,\n    model: ty.Optional[str] = None,\n    method: ty.Optional[str] = None,\n):\n    \"\"\"[citation] C. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection\n    methods. Signal Processing, 167:107299, 2020.\n    Code from:\n    https://centre-borelli.github.io/ruptures-docs/\n    \"\"\"\n    import ruptures as rpt\n\n    values = _extract_values_from_sym_args(values, ds, field, nan_policy)\n\n    penalty = penalty or float(oet.config.config['analyze']['rpt_penalty'])\n    min_size = min_size or int(oet.config.config['analyze']['rpt_min_size'])\n    jump = jump or int(oet.config.config['analyze']['rpt_jump'])\n    model = model or oet.config.config['analyze']['rpt_model']\n    method = method or oet.config.config['analyze']['rpt_method']\n\n    if len(values) < min_size:  # pragma: no cover\n        return None\n\n    algorithm = getattr(rpt, method)(model=model, min_size=min_size, jump=jump)\n    fit = algorithm.fit(values)\n\n    return len(fit.predict(pen=penalty)) - 1\n", "coverage": [1, 1, 1, 1, null, 1, 1, 1, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, null, null, null, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, 1, null, null, 1, null, null, null, null, null, 1, 1, 1, 0, null, null, 1, 0, 0, 0, 0, null, null, 1, 0, null, null, 1, 0, null, 1, 1, 1, null, 1, 1, null, null, null, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1, null, 1, 1, 1, 1, 1, null, null, null, null, 1, 1, null, 1]}, {"name": "optim_esm_tools/analyze/tipping_criteria.py", "source": "import abc\nimport typing as ty\n\nimport numpy as np\nimport xarray as xr\nfrom immutabledict import immutabledict\n\nfrom .globals import _SECONDS_TO_YEAR\nfrom .tools import rank2d\nfrom .xarray_tools import _native_date_fmt\nfrom .xarray_tools import _remove_any_none_times\nfrom .xarray_tools import apply_abs\nfrom optim_esm_tools.utils import check_accepts\nfrom optim_esm_tools.utils import deprecated\nfrom optim_esm_tools.utils import timed\n\n\nclass _Condition(abc.ABC):\n    short_description: str\n    defaults: immutabledict = immutabledict(\n        rename_to='long_name',\n        unit='absolute',\n        apply_abs=True,\n    )\n\n    def __init__(\n        self,\n        variable: str,\n        running_mean: int = 10,\n        time_var: str = 'time',\n        **kwargs,\n    ):\n        self.variable = variable\n        self.running_mean = running_mean\n        self.time_var = time_var\n        if kwargs:\n            for k, v in self.defaults.items():\n                kwargs.setdefault(k, v)\n            self.defaults = immutabledict(kwargs)\n\n    def calculate(self, *arg, **kwarg):\n        raise NotImplementedError  # pragma: no cover\n\n    @property\n    def long_description(self):\n        raise NotImplementedError  # pragma: no cover\n\n\nclass StartEndDifference(_Condition):\n    short_description: str = 'start end difference'\n\n    @property\n    def long_description(self) -> str:\n        return f'Difference of running mean ({self.running_mean} yr) between start and end of time series. Not detrended'\n\n    def calculate(self, data_set: xr.Dataset):\n        return running_mean_diff(\n            data_set,\n            variable=self.variable,  # type: ignore\n            time_var=self.time_var,  # type: ignore\n            naming='{variable}_run_mean_{running_mean}',  # type: ignore\n            running_mean=self.running_mean,  # type: ignore\n            **self.defaults,\n        )\n\n\nclass StdDetrended(_Condition):\n    short_description: str = 'std detrended'\n\n    @property\n    def long_description(self) -> str:\n        return f'Standard deviation of running mean ({self.running_mean} yr). Detrended'\n\n    @property\n    def use_variable(self) -> str:\n        return '{variable}_detrend_run_mean_{running_mean}'\n\n    def calculate(self, data_set: xr.Dataset):\n        return running_mean_std(\n            data_set,\n            variable=self.variable,  # type: ignore\n            time_var=self.time_var,  # type: ignore\n            naming=self.use_variable,  # type: ignore\n            running_mean=self.running_mean,  # type: ignore\n            **self.defaults,\n        )\n\n\nclass MaxJump(_Condition):\n    short_description: str = 'max jump'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.number_of_years = 10\n\n    @property\n    def long_description(self) -> str:\n        return f'Max change in {self.number_of_years} yr in the running mean ({self.running_mean} yr). Not detrended'\n\n    @property\n    def use_variable(self) -> str:\n        return '{variable}_run_mean_{running_mean}'\n\n    def calculate(self, data_set: xr.Dataset):\n        return max_change_xyr(\n            data_set,\n            variable=self.variable,  # type: ignore\n            time_var=self.time_var,  # type: ignore\n            naming=self.use_variable,  # type: ignore\n            x_yr=self.number_of_years,  # type: ignore\n            running_mean=self.running_mean,  # type: ignore\n            **self.defaults,\n        )\n\n\nclass MaxJumpYearly(MaxJump):\n    short_description: str = 'max jump yearly'\n\n    def __init__(self, *args, **kwargs):\n        kwargs['running_mean'] = 1\n        super().__init__(*args, **kwargs)\n\n    @property\n    def use_variable(self) -> str:\n        assert self.running_mean == 1\n        return '{variable}'\n\n\nclass StdDetrendedYearly(StdDetrended):\n    short_description: str = 'std detrended yearly'\n\n    @property\n    def long_description(self) -> str:\n        return 'Standard deviation. Detrended'\n\n    @property\n    def use_variable(self) -> str:\n        return '{variable}_detrend'\n\n\nclass MaxDerivitive(_Condition):\n    short_description: str = 'max derivative'\n\n    @property\n    def long_description(self) -> str:\n        return f'Max value of the first order derivative of the running mean ({self.running_mean} yr). Not deterended'\n\n    def calculate(self, data_set: xr.Dataset):\n        return max_derivative(\n            data_set,\n            variable=self.variable,  # type: ignore\n            time_var=self.time_var,  # type: ignore\n            naming='{variable}_run_mean_{running_mean}',  # type: ignore\n            running_mean=self.running_mean,  # type: ignore\n            **self.defaults,\n        )\n\n\nclass MaxJumpAndStd(_Condition):\n    short_description: str = 'percentile score std and max jump'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.number_of_years = 10\n\n    @staticmethod\n    def parents():\n        return MaxJump, StdDetrended\n\n    @property\n    def long_description(self) -> str:\n        p1, p2 = self.parents()\n        return f'Product of {p1.short_description} and {p2.short_description}'\n\n    def get_parents_init(self) -> ty.List[_Condition]:\n        return [\n            p(\n                variable=self.variable,\n                running_mean=self.running_mean,\n                time_var=self.time_var,\n                **self.defaults,\n            )\n            for p in self.parents()\n        ]\n\n    def get_parent_results(self, data_set: xr.Dataset) -> ty.Dict[str, float]:\n        super_1, super_2 = self.get_parents_init()\n        da_1 = super_1.calculate(data_set)\n        da_2 = super_2.calculate(data_set)\n        assert super_1.short_description != super_2.short_description, (\n            super_1.short_description,\n            super_2.short_description,\n        )\n        return {super_1: da_1, super_2: da_2}\n\n    def calculate(self, data_set: xr.Dataset):\n        da_1, da_2 = self.get_parent_results(data_set).values()\n        combined_score = np.ones_like(da_1.values, dtype=np.float64)\n        for da in [da_1, da_2]:\n            combined_score *= rank2d(da.values)\n        return xr.DataArray(\n            combined_score,\n            coords=da_1.coords,\n            name=self.short_description,\n        )\n\n\nclass SNR(MaxJumpAndStd):\n    short_description: str = 'max jump div. std'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    @staticmethod\n    def parents():\n        return MaxJump, StdDetrended\n\n    @property\n    def long_description(self) -> str:\n        p1, p2 = self.parents()\n        return f'Signal to noise ratio of {p1.short_description}/{p2.short_description}'\n\n    def calculate(self, data_set: xr.Dataset):\n        da_1, da_2 = self.get_parent_results(data_set).values()\n        res = da_1 / da_2\n        res.name = self.short_description\n        return res\n\n\n@timed\n@apply_abs()\n@check_accepts(accepts=dict(unit=('absolute', 'relative', 'std')))\ndef running_mean_diff(\n    data_set: xr.Dataset,\n    variable: str,\n    time_var: str = 'time',\n    naming: str = '{variable}_run_mean_{running_mean}',\n    running_mean: int = 10,\n    rename_to: str = 'long_name',\n    unit: str = 'absolute',\n    apply_abs: bool = True,\n) -> xr.DataArray:  # type: ignore\n    \"\"\"Return difference in running mean of data set.\n\n    Args:\n        data_set (xr.Dataset):\n        variable (str, optional): . Defaults to 'tas'.\n        time_var (str, optional): . Defaults to 'time'.\n        naming (str, optional): . Defaults to '{variable}_run_mean_{running_mean}'.\n        running_mean (int, optional): . Defaults to 10.\n        rename_to (str, optional): . Defaults to 'long_name'.\n        unit (str, optional): . Defaults to 'absolute'.\n        apply_abs (bool, optional): . Defaults to True.\n    Raises:\n        ValueError: when no timestamps are not none?\n\n    Returns:\n        xr.Dataset:\n    \"\"\"\n    var_name = naming.format(variable=variable, running_mean=running_mean)\n    _time_values = data_set[time_var].dropna(time_var)\n\n    if not len(_time_values):\n        raise ValueError(f'No values for {time_var} in data_set?')  # pragma: no cover\n\n    data_var = _remove_any_none_times(data_set[var_name], time_var)\n\n    data_t_0 = data_var.isel(time=0)\n    data_t_1 = data_var.isel(time=-1)\n    result = data_t_1 - data_t_0\n    result = result.copy()\n    var_unit = data_var.attrs.get('units', '{units}').replace('%', r'\\%')\n    name = data_var.attrs.get(rename_to, variable)\n\n    if unit == 'absolute':\n        result.name = f't[-1] - t[0] for {name} [{var_unit}]'\n        return result\n\n    if unit == 'relative':\n        result = 100 * result / data_t_0\n        result.name = fr't[-1] - t[0] / t[0] for {name} $\\%$'\n        return result\n\n    # Redundant if just for clarity\n    if unit == 'std':\n        result = result / result.std()\n        result.name = fr't[-1] - t[0] for {name} [$\\sigma$]'\n        return result\n    raise ValueError(f'{unit} is invalid')\n\n\n@timed\n@apply_abs()\n@check_accepts(accepts=dict(unit=('absolute', 'relative', 'std')))\ndef running_mean_std(\n    data_set: xr.Dataset,\n    variable: str,\n    time_var: str = 'time',\n    naming: str = '{variable}_detrend_run_mean_{running_mean}',\n    running_mean: int = 10,\n    rename_to: str = 'long_name',\n    apply_abs: bool = True,\n    unit: str = 'absolute',\n) -> xr.DataArray:  # type: ignore\n    data_var = naming.format(variable=variable, running_mean=running_mean)\n    result = data_set[data_var].std(dim=time_var)\n    result = result.copy()\n    var_unit = data_set[data_var].attrs.get('units', '{units}').replace('%', r'\\%')\n    name = data_set[data_var].attrs.get(rename_to, variable)\n\n    if unit == 'absolute':\n        result.name = f'Std. {name} [{var_unit}]'\n        return result\n\n    if unit == 'relative':\n        result = 100 * result / data_set[data_var].mean(dim=time_var)\n        result.name = fr'Relative Std. {name} [$\\%$]'\n        return result\n\n    if unit == 'std':\n        result = result / data_set[data_var].std()\n        result.name = fr'Std. {name} [$\\sigma$]'\n        return result\n\n\n@timed\n@apply_abs()\n@check_accepts(accepts=dict(unit=('absolute', 'relative', 'std')))\ndef max_change_xyr(\n    data_set: xr.Dataset,\n    variable: str,\n    time_var: str = 'time',\n    naming: str = '{variable}_run_mean_{running_mean}',\n    x_yr: ty.Union[int, float] = 10,\n    running_mean: int = 10,\n    rename_to: str = 'long_name',\n    apply_abs: bool = True,\n    unit: str = 'absolute',\n) -> xr.DataArray:  # type: ignore\n    data_var = naming.format(variable=variable, running_mean=running_mean)\n    plus_x_yr = data_set.isel({time_var: slice(x_yr, None)})[data_var]\n    to_min_x_yr = data_set.isel({time_var: slice(None, -x_yr)})[data_var]\n\n    # Keep the metadata (and time stamps of the to_min_x_yr)\n    result = to_min_x_yr.copy(data=plus_x_yr.values - to_min_x_yr.values)\n    result.data = np.abs(result.values)\n    result = result.max(dim=time_var)\n    var_unit = data_set[data_var].attrs.get('units', '{units}').replace('%', r'\\%')\n    name = data_set[data_var].attrs.get(rename_to, variable)\n\n    if unit == 'absolute':\n        result.name = f'{x_yr} yr diff. {name} [{var_unit}]'  # type: ignore\n        return result  # type: ignore\n\n    if unit == 'relative':\n        result = 100 * result / to_min_x_yr.mean(dim=time_var)\n        result.name = fr'{x_yr} yr diff. {name} [$\\%$]'  # type: ignore\n        return result  # type: ignore\n\n    if unit == 'std':\n        result = result / result.std()\n        result.name = fr'{x_yr} yr diff. {name} [$\\sigma$]'  # type: ignore\n        return result  # type: ignore\n\n\n@timed\n@apply_abs()\n@check_accepts(accepts=dict(unit=('absolute', 'relative', 'std')))\ndef max_derivative(\n    data_set: xr.Dataset,\n    variable: str,\n    time_var: str = 'time',\n    naming: str = '{variable}_run_mean_{running_mean}',\n    running_mean: int = 10,\n    rename_to: str = 'long_name',\n    apply_abs: bool = True,\n    unit: str = 'absolute',\n) -> xr.Dataset:  # type: ignore\n    var_name = naming.format(variable=variable, running_mean=running_mean)\n\n    data_array = _remove_any_none_times(data_set[var_name], time_var)\n    result = (\n        np.abs(data_array.differentiate(time_var)).max(dim=time_var) * _SECONDS_TO_YEAR\n    )\n\n    var_unit = data_array.attrs.get('units', '{units}').replace('%', r'\\%')\n    name = data_array.attrs.get(rename_to, variable)\n\n    if unit == 'absolute':\n        result.name = fr'Max $\\partial/\\partial t$ {name} [{var_unit}/yr]'\n        return result\n\n    if unit == 'relative':\n        result = 100 * result / data_array.mean(dim=time_var)\n        result.name = fr'Max $\\partial/\\partial t$ {name} [$\\%$/yr]'\n        return result\n\n    if unit == 'std':\n        # A local unit of sigma might be better X.std(dim=time_var)\n        result = result / data_array.std()\n        result.name = fr'Max $\\partial/\\partial t$ {name} [$\\sigma$/yr]'\n        return result\n\n\nrank2d = deprecated(rank2d, 'Call rank2d from optim_esm_tools.analyze.tools')\n", "coverage": [1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, null, null, null, null, null, 1, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, null, 1, null, null, 1, 1, null, null, null, 1, 1, null, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, null, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, null, 1, 1, 1, null, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, null, null, 1, 1, null, 1, 1, 1, null, 1, 1, 1, null, null, 1, 1, null, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, null, null, 1, null, 1, 1, 1, 1, 1, 1, null, null, null, null, null, null, 1, 1, null, 1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, null, null, 1, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, null, null, 1, null, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, null, 0, 0, 0, 0, null, null, 0, 0, 0, 0, 0, null, null, 1, 1, 1, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, 1, 1, 1, null, 0, 0, 0, 0, null, 0, 0, 0, 0, null, null, 1, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, 1, 1, 1, 1, 1, null, 1, 1, 1, null, 0, 0, 0, 0, null, 0, 0, 0, 0, null, null, 1, 1, 1, 1, null, null, null, null, null, null, null, null, null, 1, null, 1, 1, null, null, null, 1, 1, null, 1, 1, 1, null, 0, 0, 0, 0, null, 0, null, 0, 0, 0, null, null, 1]}, {"name": "optim_esm_tools/analyze/tools.py", "source": "import numpy as np\nfrom scipy.interpolate import interp1d\nimport statsmodels.api as sm\nimport typing as ty\nimport xarray as xr\nimport numba\nimport json\n\n\ndef _dinfo(a):\n    try:\n        return np.iinfo(a.dtype)\n    except ValueError:\n        return np.finfo(a.dtype)\n\n\ndef rank2d(a):\n    nan_mask = np.isnan(a)\n    a_flat = a[~nan_mask].flatten().astype(np.float64)\n    dtype_info = _dinfo(a_flat)\n    # Clip infinite from values - they will get ~0 or ~1 for -np.inf and np.inf respectively\n    a_flat = np.clip(a_flat, dtype_info.min, dtype_info.max)\n\n    # This is equivalent to (but much faster than)\n    # from scipy.stats import percentileofscore\n    # import optim_esm_tools as oet\n    # pcts = [[percentileofscore(a_flat, i, kind='mean') / 100 for i in aa]\n    #         for aa in oet.utils.tqdm(a)]\n    # return pcts\n    a_sorted, count = np.unique(a_flat, return_counts=True)\n    # One value can occur more than once, get the center x value for that case\n    cumsum_high = (np.cumsum(count) / len(a_flat)).astype(np.float64)\n    cumsum_low = np.zeros_like(cumsum_high)\n    cumsum_low[1:] = cumsum_high[:-1]\n    cumsum = (cumsum_high + cumsum_low) / 2\n    itp = interp1d(a_sorted, cumsum, bounds_error=True, kind='linear')\n\n    result = np.empty_like(a, dtype=np.float32)\n    result[:] = np.nan\n    result[~nan_mask] = itp(a_flat)\n    return result\n\n\ndef smooth_lowess(\n    *a: ty.Union[\n        ty.Tuple[np.ndarray, np.ndarray],\n        ty.Tuple[np.ndarray,],\n        ty.Tuple[xr.DataArray, xr.DataArray],\n        ty.Tuple[xr.DataArray,],\n    ],\n    **kw,\n) -> ty.Union[xr.DataArray, np.ndarray]:\n\n    if len(a) == 2:\n        x, y = a\n        ret_slice = slice(None, None)\n    elif len(a) == 1:\n        y = a[0]\n        x = np.arange(len(y))\n\n        ret_slice = slice(1, None)\n    else:\n        raise ValueError(len(a), a)\n    input_type = 'xr' if isinstance(y, xr.DataArray) else 'np'\n    assert isinstance(y, (xr.DataArray, np.ndarray)), f'{type(x)} not supported'\n    if input_type == 'xr':\n        _y = y.values\n        _x = x if isinstance(x, np.ndarray) else x.values\n    else:\n        _x, _y = x, y\n    assert isinstance(_y, type(_x)), f'{type(_x)} is not {type(_y)}'\n\n    res = _smooth_lowess(_x, _y, ret_slice, **kw)\n    if input_type == 'np':\n        return res\n\n    ret_y = y.copy()\n    if len(a) == 1:\n        ret_y.data = res\n        return ret_y\n\n    ret_x = x.copy()\n    ret_x.data, ret_y.data = res\n    return ret_x, ret_y\n\n\nsmooth_lowess.__doc__ = \"\"\"wrapper for statsmodels.api.nonparametric.lowess. For kwargs read\\n\\n: {doc}\"\"\".format(\n    doc=sm.nonparametric.lowess.__doc__,\n)\n\n\ndef _smooth_lowess(x: np.ndarray, y: np.ndarray, ret_slice: slice, **kw) -> np.ndarray:\n    kw = kw.copy()\n    if 'window' in kw:\n        assert 'frac' not in kw, 'Provide either frac or window, not both!'\n        window = kw.pop('window')\n        assert window > 0 and window <= len(y)\n        kw['frac'] = window / len(y)\n\n    kw.setdefault('frac', 0.1)\n    kw.setdefault('missing', 'raise')\n\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, **kw)\n    return smoothed.T[ret_slice].squeeze()\n\n\n@numba.njit\ndef running_mean(a: np.ndarray, window: int) -> np.ndarray:\n    res = np.zeros_like(a)\n    res[:] = np.nan\n    half_win = window // 2\n    mean = 0\n    for i, v in enumerate(a):\n        mean += v\n        if i >= window:\n            mean -= a[i - window]\n        if i >= (window - 1):\n            res[i - half_win + 1] = mean / window\n    return res\n\n\n@numba.njit\ndef running_mean_array(a: np.ndarray, window: int) -> np.ndarray:\n    _, len_x, len_y = a.shape\n    res = np.zeros_like(a)\n    res[:] = np.nan\n    for i in range(len_x):\n        for j in range(len_y):\n            res[:, i, j] = running_mean(a[:, i, j], window)\n    return res\n\n\ndef _weighted_mean_array_xarray(\n    data: xr.DataArray,\n    weights: xr.DataArray,\n) -> xr.DataArray:\n    res = data * weights\n    if \"time\" in res.dims:\n        res = res.sum(\"lat lon\".split())\n        mask_time = data.isnull().all(dim=\"lat\").all(dim=\"lon\")\n        res[mask_time] = np.nan\n\n        mask_lat_lon = ~data.isnull().all(dim=\"time\")\n        area_mask = weights.where(mask_lat_lon)\n    else:\n        mask_lat_lon = ~data.isnull()\n        res = res.values[mask_lat_lon].sum()\n        area_mask = weights.where(mask_lat_lon)\n\n    return res / (area_mask.sum())\n\n\ndef _weighted_mean_array_numpy(\n    data: np.ndarray,\n    weights: np.ndarray,\n    has_time_dim: bool = True,\n    _dtype=np.float64,\n) -> np.ndarray:\n    res = data * weights\n    if has_time_dim:\n        na_array = np.isnan(res)\n\n        mask_time = na_array.all(axis=1).all(axis=1)\n\n        # This is slightly confusing. We used to ignore any time step where there is a nan, but this is problematic if the nans pop in and out at a given grid cell\n        # So instead we remove those grid cells.\n\n        # Used to follow this logic: throw away all data where each cell is always nan in time, and keep data without any nan in time.\n        # mask_lat_lon = ~na_array.all(axis=0)\n        # no_na_vals = res[~mask_time][:, mask_lat_lon].sum(axis=1)\n\n        # However, it's better to exclude those grid cells that are nan at least somewhere in time\n        # Individual grid cells might have nans, but are at least not consistent in time.\n        mask_lat_lon = ~na_array[~mask_time].any(axis=0)\n        no_na_vals = np.nansum(res[~mask_time][:, mask_lat_lon], axis=1)\n\n        res = np.zeros(len(data), dtype=_dtype)\n        res[mask_time] = np.nan\n        res[~mask_time] = no_na_vals\n        area_mask = weights[mask_lat_lon]\n    else:\n        mask_lat_lon = ~np.isnan(data)\n        res = np.nansum(res[mask_lat_lon])\n        area_mask = weights[mask_lat_lon]\n\n    return res / (area_mask.sum())\n\n\ndef _weighted_mean_array_numba(\n    data: np.ndarray,\n    weights: np.ndarray,\n    has_time_dim: bool = True,\n) -> ty.Union[float, np.ndarray]:\n    if has_time_dim:\n        assert len(data.shape) == 3, data.shape\n\n    else:\n        if len(data.shape) == 1:\n            return _weighted_mean_1d_numba(data, weights)\n        assert len(data.shape) == 2, data.shape\n        return _weighted_mean_2d_numba(data, weights)\n    return _weighted_mean_3d_numba(data, weights)\n\n\n@numba.njit\ndef _weighted_mean_2d_numba(data, weights):\n    tot = 0.0\n    weight = 0.0\n    x, y = data.shape\n    for i in range(x):\n        for j in range(y):\n            if np.isnan(data[i][j]):\n                continue\n            tot += data[i][j] * weights[i][j]\n            weight += weights[i][j]\n    if tot == 0.0:\n        return np.nan\n    return tot / weight\n\n\n@numba.njit\ndef _weighted_mean_1d_numba(data: np.ndarray, weights: np.ndarray) -> float:\n    tot = 0.0\n    weight = 0.0\n    for i in range(len(data)):\n        if np.isnan(data[i]):\n            continue\n        tot += data[i] * weights[i]\n        weight += weights[i]\n    if tot == 0.0:\n        return np.nan\n    return tot / weight\n\n\n@numba.njit\ndef _weighted_mean_3d_numba(data, weights, _dtype=np.float64):\n    t, x, y = data.shape\n    is_nan_xy = np.zeros((x, y), dtype=np.bool_)\n    weight = 0.0\n    # Not sure if we should allow anything but np.float64 since you get overflows get quickly!\n    tot = np.zeros(t, dtype=_dtype)\n\n    # First, check which time steps are always nan\n    for k in range(t):\n        do_break = False\n        for i in range(x):\n            for j in range(y):\n                if ~np.isnan(data[k][i][j]):\n                    do_break = True\n                    break\n            if do_break:\n                break\n        is_nan_for_all_i_j = not do_break\n        if is_nan_for_all_i_j:\n            tot[k] = np.nan\n\n    # Then, check which lat,lon coords are always nan\n    for k in range(t):\n        if np.isnan(tot[k]):\n            continue\n        for i in range(x):\n            for j in range(y):\n                if np.isnan(data[k][i][j]):\n                    is_nan_xy[i][j] = True\n\n    # Now sum all gridcells which are never nan in time, or lat+lon\n    for i in range(x):\n        for j in range(y):\n            if is_nan_xy[i][j]:\n                continue\n            for k in range(t):\n                if np.isnan(tot[k]):\n                    continue\n                tot[k] = tot[k] + data[k][i][j] * weights[i][j]\n            weight += weights[i][j]\n    return tot / weight\n\n\ndef weighted_mean_array(\n    _ds: xr.Dataset,\n    field: str = \"std detrended\",\n    area_field: str = \"cell_area\",\n    return_values: bool = True,\n    method: str = \"numpy\",\n    time_field: str = \"time\",\n) -> ty.Union[np.ndarray, float, xr.DataArray]:\n    if method == \"xarray\":\n        res_da = _weighted_mean_array_xarray(_ds[field], _ds[area_field])\n        return res_da.values if return_values else res_da\n\n    da_sel = _ds[field]\n    has_time_dim = time_field in da_sel.dims\n\n    data = da_sel.values\n    weights = _ds[area_field].values\n    kw = dict(data=data, weights=weights, has_time_dim=has_time_dim)\n    if method == \"numba\":\n        res_arr = _weighted_mean_array_numba(**kw)  # type: ignore\n    elif method == \"numpy\":\n        res_arr = _weighted_mean_array_numpy(**kw)  # type: ignore\n    else:\n        raise ValueError(f\"Unknown method {method}\")\n\n    if return_values or not has_time_dim:\n        return res_arr\n    res_da = xr.DataArray(res_arr, dims=\"time\")\n    res_da.attrs.update(da_sel.attrs)\n    return res_da\n\n\nclass NumpyEncoder(json.JSONEncoder):\n    \"\"\"Special json encoder for numpy types.\"\"\"\n\n    # Thanks https://stackoverflow.com/a/49677241/18280620\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)\n", "coverage": [1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, 1, 1, null, null, 1, 1, 1, 1, null, 1, null, null, null, null, null, null, null, 1, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, null, 1, null, 0, 1, 1, 1, 1, 1, null, 1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, null, 1, 1, 1, null, null, 1, null, null, null, null, 1, 1, 1, 0, 0, 0, 0, null, 1, 1, null, 1, 1, null, null, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, null, null, 1, 1, 0, 0, 0, 0, 0, 0, 0, null, null, 1, null, null, null, 0, 0, 0, 0, 0, null, 0, 0, null, 0, 0, 0, null, 0, null, null, 1, null, null, null, null, null, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, 1, null, 0, 0, 0, null, 1, null, null, 1, null, null, null, null, 1, 1, null, null, 1, 1, 1, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, null, null, 1, 1, 1, 1, 1, null, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 0, null, null, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, 1, 0, 0, null, 1, 1, null, 1, 1, 1, 1, 0, 1, 1, null, 0, null, 1, 1, 0, 0, 0, null, null, 1, null, null, null, 1, 0, 0, 0, 0, 0, 0, 0]}, {"name": "optim_esm_tools/analyze/xarray_tools.py", "source": "import typing as ty\nfrom functools import wraps\n\nimport numpy as np\nimport xarray as xr\nimport numba\nimport cftime\nfrom optim_esm_tools.config import config\nfrom optim_esm_tools.utils import check_accepts\n\n\ndef _native_date_fmt(time_array: np.ndarray, date: ty.Tuple[int, int, int]):\n    \"\"\"Create date object using the date formatting from the time-array.\"\"\"\n\n    if isinstance(time_array, xr.DataArray):  # pragma: no cover\n        return _native_date_fmt(time_array=time_array.values, date=date)\n\n    if not len(time_array):  # pragma: no cover\n        raise ValueError('No values in dataset?')\n\n    # Support cftime.DatetimeJulian, cftime.DatetimeGregorian, cftime.DatetimeNoLeap and similar\n    _time_class = time_array[0].__class__\n    return _time_class(*date)\n\n\ndef apply_abs(apply=True, add_abs_to_name=True, _disable_kw='apply_abs') -> ty.Callable:\n    \"\"\"Apply np.max() to output of function (if apply=True) Disable in the\n    function kwargs by using the _disable_kw argument.\n\n    Example:\n        ```\n        @apply_abs(apply=True, add_abs_to_name=False)\n        def bla(a=1, **kw):\n            print(a, kw)\n            return a\n        assert bla(-1, apply_abs=True) == 1\n        assert bla(-1, apply_abs=False) == -1\n        assert bla(1) == 1\n        assert bla(1, apply_abs=False) == 1\n        ```\n    Args:\n        apply (bool, optional): apply np.abs. Defaults to True.\n        _disable_kw (str, optional): disable with this kw in the function. Defaults to 'apply_abs'.\n    \"\"\"\n\n    def somedec_outer(fn):\n        @wraps(fn)\n        def somedec_inner(*args, **kwargs):\n            response = fn(*args, **kwargs)\n            do_abs = kwargs.get(_disable_kw)\n            if do_abs or (do_abs is None and apply):\n                if add_abs_to_name and isinstance(getattr(response, 'name'), str):\n                    response.name = f'Abs. {response.name}'\n                return np.abs(response)\n            return response  # pragma: no cover\n\n        return somedec_inner\n\n    return somedec_outer\n\n\ndef _remove_any_none_times(da: xr.DataArray, time_dim: bool, drop: bool = True) -> None:\n    data_var = da.copy()\n    time_null = data_var.isnull().all(dim=set(data_var.dims) - {time_dim})\n    if np.all(time_null):\n        # If we take a running mean of 10 (the default), and the array is shorter than\n        # 10 years we will run into issues here because a the window is longer than the\n        # array. Perhaps we should raise higher up.\n        raise ValueError(\n            f'This array only has NaN values, perhaps array too short ({len(time_null)} < 10)?',\n        )  # pragma: no cover\n\n    if np.any(time_null):\n        try:\n            # For some reason only alt_calc seems to work even if it should be equivalent to the data_var\n            # I think there is some fishy indexing going on in pandas <-> dask\n            # Maybe worth raising an issue?\n            alt_calc = xr.where(~time_null, da, np.nan)\n            if drop:\n                alt_calc = alt_calc.dropna(time_dim)\n            data_var = data_var.load().where(~time_null, drop=drop)\n            assert np.all((alt_calc == data_var).values)\n        except IndexError as e:  # pragma: no cover\n            from optim_esm_tools.config import get_logger\n\n            get_logger().error(e)\n            if 'alt_calc' in locals():\n                return alt_calc  # type: ignore\n            raise e\n    return data_var\n\n\n@check_accepts(accepts=dict(drop_method=('xarray', 'numba')))\ndef mask_xr_ds(\n    data_set: xr.Dataset,\n    da_mask: xr.DataArray,\n    masked_dims: ty.Optional[ty.Iterable[str]] = None,\n    drop: bool = False,\n    keep_keys: ty.Optional[ty.Iterable[str]] = None,\n    drop_method: ty.Optional[str] = None,\n):\n    # Modify the ds in place - make a copy!\n    data_set = data_set.copy()\n    if masked_dims is None:\n        masked_dims = config['analyze']['lon_lat_dim'].split(',')[::-1]\n\n    ds_start = data_set.copy()\n    drop_true_function: ty.Callable = (\n        _drop_by_mask\n        if (\n            drop_method == 'xarray'\n            or (drop_method is None and config['analyze']['use_drop_nb'] != 'True')\n        )\n        else _drop_by_mask_nb\n    )\n\n    func_by_drop = {\n        True: drop_true_function,\n        False: _mask_xr_ds,\n    }[drop]\n    data_set = func_by_drop(\n        data_set,\n        masked_dims,\n        ds_start,\n        da_mask,\n        keep_keys=keep_keys,\n    )\n    data_set = data_set.assign_attrs(ds_start.attrs)\n    return data_set\n\n\ndef reverse_name_mask_coords(\n    da_mask: xr.DataArray,\n    rename_dict: ty.Optional[dict] = None,\n) -> xr.DataArray:\n    rename_dict = rename_dict or {\n        v: k for k, v in default_rename_mask_dims_dict().items()\n    }\n    return rename_mask_coords(da_mask, rename_dict=rename_dict)\n\n\ndef rename_mask_coords(\n    da_mask: xr.DataArray,\n    rename_dict: ty.Optional[ty.Mapping] = None,\n) -> xr.DataArray:\n    \"\"\"Get a boolean DataArray with renamed dimensionality. For some\n    applications, we want to prune a dataset of nan values along a given\n    lon/lat mask. Removing data along a given mask can greatly reduce file-size\n    and speed up data-set handling. This however makes it somewhat cumbersome\n    to later re-apply said mask (to other data) since it's shape will be\n    inconsistent with other (non-masked) data. To this end, we want to store\n    the mask separately in a dataset. To avoid dimension clashes between masked\n    data and the masked information, we rename the dimensions of the mask.\n\n    Args:\n        da_mask (xr.DataArray): Mask to be renamed.\n        rename_dict (ty.Mapping, optional): Mapping from the dims in da_mask to renamed dims.\n\n    Returns:\n        xr.DataArray: da_mask with renamed dims.\n    \"\"\"\n    rename_dict = rename_dict or default_rename_mask_dims_dict()\n    if any(dim not in da_mask.dims for dim in rename_dict.keys()):\n        raise KeyError(\n            f'Trying to rename {rename_dict}, but this DataArray has {da_mask.dims}',\n        )  # pragma: no cover\n    mask = da_mask.copy().rename(rename_dict)\n    message = (\n        'Full global mask with full lat/lon dimensionality in order to be save the masked '\n        'time series with all nan values dropped (to conserve disk space)'\n    )\n    mask.attrs.update(dict(info=message))\n    return mask\n\n\ndef mask_to_reduced_dataset(\n    data_set: xr.Dataset,\n    mask: ty.Union[xr.DataArray, np.ndarray],\n    add_global_mask: bool = True,\n    _fall_back_field: str = 'cell_area',\n    **kw,\n) -> xr.Dataset:\n    \"\"\"Reduce data_set by dropping all data where mask is False. This greatly\n    reduces the size (which is absolutely required for exporting time series\n    from global data).\n\n    Args:\n        data_set (xr.Dataset): data set to mask by mask\n        mask (ty.Union[xr.DataArray, np.ndarray]): boolean array to mask\n        add_global_mask (bool, optional): Add global mask with full dimensionality (see\n            rename_mask_coords for more info). Defaults to True.\n\n    Raises:\n        ValueError: If mask has a wrong shape\n\n    Returns:\n        xr.Dataset: Original dataset where mask is True\n    \"\"\"\n    if isinstance(mask, np.ndarray):\n        mask_da = data_set[_fall_back_field].astype(np.bool_).copy()\n        mask_da.data = mask\n        mask = mask_da\n    if mask.shape != (expected := data_set[_fall_back_field].shape):\n        raise ValueError(\n            f'Inconsistent dimensionality, expected {expected}, got {mask.shape}',\n        )  # pragma: no cover\n\n    if all(m in list(mask.coords) for m in default_rename_mask_dims_dict().values()):\n        from optim_esm_tools.config import get_logger\n\n        get_logger().debug(\n            f'Reversing coords {list(mask.coords)} != {list(data_set[_fall_back_field].coords)}',\n        )\n        mask = reverse_name_mask_coords(mask)\n    ds_masked = mask_xr_ds(data_set.copy(), mask, drop=True, **kw)\n    if add_global_mask:\n        ds_masked = add_mask_renamed(ds_masked, mask)\n    return ds_masked\n\n\ndef default_rename_mask_dims_dict() -> ty.Dict:\n    return {k: f'{k}_mask' for k in config['analyze']['lon_lat_dim'].split(',')}\n\n\ndef add_mask_renamed(\n    data_set: xr.Dataset,\n    da_mask: xr.DataArray,\n    mask_name: str = 'global_mask',\n    **kw,\n) -> xr.Dataset:\n    data_set[mask_name] = rename_mask_coords(da_mask, **kw)\n    return data_set\n\n\ndef _drop_by_mask(\n    data_set: xr.Dataset,\n    masked_dims: ty.Iterable[str],\n    ds_start: xr.Dataset,\n    da_mask: xr.DataArray,\n    keep_keys: ty.Optional[ty.Iterable[str]] = None,\n):\n    \"\"\"Drop values with masked_dims dimensions.\n\n    Unfortunately, data_set.where(da_mask, drop=True) sometimes leads to\n    bad results, for example for time_bnds (time, bnds) being dropped by\n    (lon, lat). So we have to do some funny bookkeeping of which data\n    vars we can drop with data_set.where.\n    \"\"\"\n    if keep_keys is None:\n        keep_keys = list(data_set.variables.keys())\n    dropped = [\n        k\n        for k, data_array in data_set.data_vars.items()\n        if any(dim not in list(data_array.dims) for dim in masked_dims)\n        or k not in keep_keys\n    ]\n    data_set = data_set.drop_vars(dropped)\n\n    try:\n        data_set = data_set.where(da_mask.compute(), drop=True)\n    except ValueError:\n        from optim_esm_tools.config import get_logger\n\n        get_logger().info(f'data_set {list(data_set.coords)}')\n        get_logger().info(f'da_mask {list(da_mask.coords)}')\n        raise\n\n    # Restore ignored variables and attributes\n    for k in dropped:  # pragma: no cover\n        if k not in keep_keys:\n            continue\n        data_set[k] = ds_start[k]\n    return data_set\n\n\ndef _mask_xr_ds(\n    data_set: xr.Dataset,\n    masked_dims: ty.Iterable[str],\n    ds_start: xr.Dataset,\n    da_mask: xr.DataArray,\n    keep_keys: ty.Optional[ty.Iterable[str]] = None,\n):\n    \"\"\"Rebuild data_set for each variable that has all masked_dims.\"\"\"\n    for k, data_array in data_set.data_vars.items():\n        if keep_keys is not None and k not in keep_keys:\n            continue\n        if all(dim in list(data_array.dims) for dim in masked_dims):\n            lat_lon = config['analyze']['lon_lat_dim'].split(',')[::-1]\n            dim_incorrect = tuple(data_array.dims) not in [\n                ('time', *lat_lon),\n                (*lat_lon,),\n            ]\n            shape_incorrect = data_array.shape != data_array.T.shape\n            if dim_incorrect and shape_incorrect:  # pragma: no cover\n                message = f'Please make \"{k}\" {lat_lon}, now \"{data_array.dims}\"'\n                raise ValueError(message)\n            da = data_set[k].where(da_mask, drop=False)\n            da = da.assign_attrs(ds_start[k].attrs)\n            data_set[k] = da\n\n    return data_set\n\n\ndef _prepare_dropped_dataset(\n    data_set: xr.Dataset,\n    fall_back_key: str,\n    masked_dims: ty.Iterable[str],\n    da_mask: xr.DataArray,\n    keep_keys: ty.Optional[ty.Iterable[str]] = None,\n) -> ty.Tuple[xr.Dataset, ty.List[str], ty.List[str]]:\n    assert fall_back_key in data_set\n    if keep_keys is None:\n        keep_keys = list(data_set.variables.keys())\n    else:\n        if not all(isinstance(k, str) for k in keep_keys):\n            raise TypeError(f'Got one or more non-string keys {keep_keys}')\n    dropped = [\n        str(k)\n        for k, data_array in data_set.data_vars.items()\n        if (\n            any(dim not in list(data_array.dims) for dim in masked_dims)\n            or k not in keep_keys\n        )  # and k not in no_drop\n    ]\n\n    data_set = data_set.drop_vars(\n        (set(dropped) | set(keep_keys)) - {*masked_dims, fall_back_key},\n    )\n\n    data_set = data_set.where(da_mask.compute(), drop=True)\n\n    assert fall_back_key in data_set\n    return data_set, keep_keys, dropped\n\n\ndef _drop_by_mask_nb(\n    data_set: xr.Dataset,\n    masked_dims: ty.Iterable[str],\n    ds_start: xr.Dataset,\n    da_mask: xr.DataArray,\n    keep_keys: ty.Optional[ty.Iterable[str]] = None,\n    fall_back_key='cell_area',\n) -> xr.Dataset:\n    \"\"\"Drop values with masked_dims dimensions.\n\n    Unfortunately, data_set.where(da_mask, drop=True) sometimes leads to\n    bad results, for example for time_bnds (time, bnds) being dropped by\n    (lon, lat). So we have to do some funny bookkeeping of which data\n    vars we can drop with data_set.where.\n    \"\"\"\n    data_set, keep_keys, dropped = _prepare_dropped_dataset(\n        data_set=data_set,\n        fall_back_key=fall_back_key,\n        masked_dims=masked_dims,\n        da_mask=da_mask,\n        keep_keys=keep_keys,\n    )\n    x_map = map_array_to_index_array(ds_start.lat.values, data_set.lat.values)\n    y_map = map_array_to_index_array(ds_start.lon.values, data_set.lon.values)\n    mask_np = da_mask.values\n    res_shape = data_set[fall_back_key].shape\n    from optim_esm_tools.config import get_logger\n\n    log = get_logger()\n\n    for mask_key in keep_keys:\n        if mask_key == fall_back_key:\n            continue\n\n        da = ds_start[mask_key]\n        if da.dims == tuple(masked_dims):\n            v = mapped_2d_mask(da.values, res_shape, mask_np, x_map, y_map)\n\n            try:\n                data_set[mask_key] = xr.DataArray(data=v, dims=da.dims, attrs=da.attrs)\n            except ValueError:\n                return dict(data_set=data_set, data=v, dims=da.dims, attrs=da.attrs)\n\n        elif da.dims[1:] == tuple(masked_dims):\n            v = mapped_3d_mask(da.values, res_shape, mask_np, x_map, y_map)\n            data_set[mask_key] = xr.DataArray(data=v, dims=da.dims, attrs=da.attrs)\n        elif mask_key not in masked_dims:\n            log.debug(\n                f'Skipping \"{mask_key}\" in masking as it\\'s not following the {masked_dims} but has {da.dims}',\n            )\n            if any(m in da.dims for m in masked_dims):\n                data_set[mask_key] = ds_start[mask_key].where(da_mask, drop=True)\n            else:\n                data_set[mask_key] = ds_start[mask_key]\n\n    # Restore ignored variables and attributes\n    for k in dropped:  # pragma: no cover\n        if k not in keep_keys:\n            continue\n        data_set[k] = ds_start[k]\n    return data_set\n\n\n@numba.njit\ndef map_array_to_index_array(x0, x1, nan_int=-1234):\n    \"\"\"Map the indexes from x0 to x1, if there is no index in x1 that\n    corresponds to a value of x0, fill it with the nan_int value.\n\n    Example\n    ``\n    >>> arg_map_1d(np.array([1,2,3]), np.array([1,3]), nan_int=-1234)\n    array([    0, -1234,     1])\n    ``\n    \"\"\"\n    res = np.ones(len(x0), dtype=np.int16) * nan_int\n    for i0, x0_i in enumerate(x0):\n        for i1, x1_i in enumerate(x1):\n            if not np.isclose(x0_i, x1_i):\n                continue\n            res[i0] = i1\n            break\n    return res\n\n\n@numba.njit\ndef mapped_2d_mask(\n    source_values,\n    result_shape,\n    mask_2d,\n    index_map_x,\n    index_map_y,\n    dtype=np.float64,\n    nan_int=-1234,\n):\n    x, y = source_values.shape\n    res = np.zeros(result_shape, dtype) * np.nan\n\n    for i in range(x):\n        x_fill = index_map_x[i]\n        if x_fill == nan_int:\n            continue\n        for j in range(y):\n            y_fill = index_map_y[j]\n            if y_fill == nan_int:\n                continue\n            if not mask_2d[i][j]:\n                continue\n            res[x_fill][y_fill] = source_values[i][j]\n\n    return res\n\n\n@numba.njit\ndef mapped_3d_mask(\n    source_values,\n    result_shape,\n    mask_2d,\n    index_map_x,\n    index_map_y,\n    dtype=np.float64,\n    nan_int=-1234,\n):\n    assert len(source_values.shape) == 3\n    res = (\n        np.zeros((len(source_values), result_shape[0], result_shape[1]), dtype) * np.nan\n    )\n    for ti in range(source_values.shape[0]):\n        res[ti] = mapped_2d_mask(\n            source_values[ti],\n            result_shape=result_shape,\n            mask_2d=mask_2d,\n            index_map_x=index_map_x,\n            index_map_y=index_map_y,\n            dtype=dtype,\n            nan_int=nan_int,\n        )\n    return res\n\n\ndef yearly_average(ds: xr.Dataset, time_dim='time') -> xr.Dataset:\n    \"\"\"Compute yearly averages for all variables in the dataset along the time\n    dimension, handling both datetime and cftime objects.\"\"\"\n\n    def compute_weighted_mean(data, time):\n        \"\"\"Helper function to compute weighted mean for a given array of\n        data.\"\"\"\n        if time_bounds is not None:\n            dt = np.diff(ds[time_bounds].values, axis=1).squeeze()\n        else:\n            if isinstance(time[0], cftime.datetime):\n                dt = np.array(\n                    [(time[i + 1] - time[i]).days for i in range(len(time) - 1)]\n                    + [(time[-1] - time[-2]).days],\n                )\n            else:\n                # poor man solution, let's just assume that the last time-interval is as long as the second to last interval\n                dt = np.diff(time)\n                dt = np.concatenate([dt, [dt[-1]]])\n        if len(time) != len(dt):\n            raise ValueError(f'Inconsistent time lengths {len(time)} != {len(dt)}')\n        years = [d.year for d in data[time_dim].values]\n        if isinstance(time[0], cftime.datetime):\n            keep_idx = np.array([t.year in years for t in time])\n        elif isinstance(time, xr.DataArray) and isinstance(\n            time.values[0],\n            cftime.datetime,\n        ):\n            keep_idx = np.array([t.year in years for t in time.values])\n        else:\n            raise TypeError(type(time))\n\n        dt = dt[keep_idx]\n        dt_seconds = dt * 86400  # Convert days to seconds if cftime\n        weights = dt_seconds / dt_seconds.sum()\n\n        # Apply weighted mean over time axis\n        weighted_mean = (data * weights[:, None, None]).sum(axis=0)\n        return weighted_mean\n\n    # Handle time bounds if present\n    time_bounds = next(\n        (k for k in [f'{time_dim}_bounds', f'{time_dim}_bnds'] if k in ds),\n        None,\n    )\n\n    # Initialize a new dataset to hold the yearly averages\n    ds_yearly = xr.Dataset()\n\n    # Loop through the variables in the dataset\n    for var in ds.data_vars:\n        if time_dim in ds[var].dims:\n            dtype = ds[var].dtype\n\n            # Skip non-numeric data types\n            if not np.issubdtype(dtype, np.number):\n                print(f'Skipping {var} of dtype={dtype}')\n                continue\n\n            grouped = ds[var].groupby('time.year')\n            yearly_mean = grouped.map(lambda x: compute_weighted_mean(x, ds[time_dim]))\n\n            ds_yearly[var] = yearly_mean.astype(ds[var].dtype)\n\n    return ds_yearly\n\n\ndef set_time_int(ds: xr.Dataset) -> xr.Dataset:\n    if not isinstance(ds['time'].values[0], (int, np.int_)):\n        years = [t.year for t in ds['time'].values]\n        assert (\n            np.unique(years, return_counts=True)[1].max() == 1\n        ), 'Data has one or more non-unique years!'\n        ds['time'] = years\n    return ds\n", "coverage": [1, 1, null, 1, 1, 1, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, null, 1, null, null, 1, 1, 1, 1, null, null, null, null, null, null, null, 1, 1, null, null, null, 1, 1, 1, 1, 1, null, null, null, null, null, null, null, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, 1, null, null, null, 1, null, null, null, null, null, null, 1, 1, null, null, 1, null, null, null, 1, null, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, null, null, 1, 1, null, null, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, null, null, null, 1, 1, null, 1, null, null, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, 1, null, null, null, null, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, null, null, null, 1, null, 1, 1, 0, 0, null, 0, 0, 0, null, null, null, null, null, null, 1, null, null, 1, null, null, null, null, null, null, null, 0, 0, 0, 0, 0, 0, null, null, null, 0, null, null, null, 0, 0, 0, null, 0, null, null, 1, null, null, null, null, null, null, 1, 1, 1, null, 1, 0, 1, null, null, null, null, null, null, null, null, 1, null, null, null, 1, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, 1, null, 1, 1, 1, null, 1, 1, 1, null, 1, 1, 0, 0, null, 1, 1, 1, 1, 1, null, null, 1, 0, null, 1, null, null, null, null, null, null, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, 1, null, null, 1, null, null, null, 1, null, null, 1, 1, null, 1, 0, null, null, null, null, null, 1, 1, 1, 0, 1, 1, 0, 1, null, null, null, 1, null, 0, null, 1, 1, 1, null, null, 1, 1, null, null, 1, null, null, null, null, null, 1, null, null, 1, 1, 1, null, null, 1, 1, 1, null, 1, 1, null, 1, null, 1, null, null, 1, 1, 1, 1, null, null, 1, 1]}, {"name": "optim_esm_tools/config.py", "source": "\"\"\"Shared common methods for reprocessing, not useful in itself.\"\"\"\n\nimport configparser\nimport logging\nimport os\n\nfrom optim_esm_tools.utils import root_folder\n\nif 'OPTIM_ESM_CONFIG' in os.environ:  # pragma: no cover\n    config_path = os.environ['OPTIM_ESM_CONFIG']\nelse:\n    _warn_later = True\n    config_path = os.path.join(root_folder, 'optim_esm_tools', 'optim_esm_conf.ini')\n\nconfig = configparser.ConfigParser()\nconfig.sections()\nconfig.read(config_path)\n# oet.config.config.read_dict({'boo':{'bar':'bla'}})\n_logger = {}\n\n\ndef get_logger(name='oet'):\n    if name not in _logger:\n        logging.basicConfig(\n            level=getattr(logging, config['log']['logging_level'].upper()),\n            format=(\n                '%(asctime)s '\n                '| %(name)-12s '\n                '| %(levelname)-8s '\n                '| %(message)s '\n                '| %(funcName)s (l. %(lineno)d)'\n            ),\n            datefmt='%m-%d %H:%M:%S',\n        )\n\n        log = logging.getLogger(name)\n        _logger[name] = log\n    return _logger[name]\n\n\nif _warn_later:  # type: ignore\n    get_logger().info(\n        f'Using {config_path}-config. Overwrite by setting \"OPTIM_ESM_CONFIG\" '\n        f'as an environment variable',\n    )\n", "coverage": [null, null, 1, 1, 1, null, 1, null, null, null, null, 1, 1, null, 1, 1, 1, null, 1, null, null, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, 1, 1, null, null, null]}, {"name": "optim_esm_tools/plotting/__init__.py", "source": "from . import plot_utils\nfrom . import map_maker\nfrom . import plot\n", "coverage": [1, 1, 1]}, {"name": "optim_esm_tools/plotting/map_maker.py", "source": "import collections\nimport typing as ty\nfrom warnings import warn\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport xarray as xr\nfrom immutabledict import immutabledict\nfrom matplotlib.colors import LogNorm\n\nimport optim_esm_tools as oet\nfrom .plot import *\nfrom optim_esm_tools.analyze import tipping_criteria\nfrom optim_esm_tools.analyze.globals import _SECONDS_TO_YEAR\n\n\ndef plot_simple(\n    ds,\n    var,\n    other_dim=None,\n    show_std=False,\n    std_kw=None,\n    add_label=True,\n    set_y_lim=True,\n    **kw,\n):\n    if other_dim is None:\n        other_dim = set(ds[var].dims) - {'time'}\n    mean = ds[var].mean(other_dim)\n    l = mean.plot(**kw)\n    if show_std:\n        std_kw = std_kw or {}\n        for k, v in kw.items():\n            std_kw.setdefault(k, v)\n        std_kw.setdefault('alpha', 0.4)\n        std_kw.pop('label', None)\n        std = ds[var].std(other_dim)\n        (mean - std).plot(color=l[0]._color, **std_kw)\n        (mean + std).plot(color=l[0]._color, **std_kw)\n\n    if set_y_lim:\n        set_y_lim_var(var)\n    if add_label:\n        plt.ylabel(oet.plotting.plot.get_ylabel(ds, var))\n    plt.title('')\n\n\ndef overlay_area_mask(ds_dummy, field='cell_area', ax=None):\n    ax = ax or plt.gcf().add_subplot(\n        1,\n        2,\n        2,\n        projection=oet.plotting.plot.get_cartopy_projection(),\n    )\n    kw = dict(\n        norm=LogNorm(),\n        cbar_kwargs={\n            **dict(orientation='horizontal', extend='both'),\n            **dict(extend='neither', label='Sum of area [km$^2$]'),\n        },\n        transform=oet.plotting.plot.get_cartopy_transform(),\n    )\n    if field == 'cell_area':\n        ds_dummy[field] /= 1e6\n        tot_area = float(ds_dummy[field].sum(skipna=True))\n        ds_dummy[field].values[ds_dummy[field] > 0] = tot_area\n        kw.update(\n            dict(\n                vmin=1,\n                vmax=510100000,\n            ),  # type: ignore\n        )\n    ds_dummy[field].plot(**kw)\n    ax.coastlines()\n    if field == 'cell_area':\n        exponent = int(np.log10(tot_area))  # type: ignore\n\n        plt.title(f'Area ${tot_area/(10**exponent):.1f}\\\\times10^{{{exponent}}}$ km$^2$')  # type: ignore\n    gl = ax.gridlines(draw_labels=True)\n    gl.top_labels = False\n", "coverage": [1, 1, 1, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, null, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, null, null, null, 1, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, null, null, null, null, 1, 1, 1, 1, null, 1, 1, 1]}, {"name": "optim_esm_tools/plotting/plot.py", "source": "import typing as ty\n\nimport matplotlib.pyplot as plt\nimport xarray as xr\n\nimport optim_esm_tools as oet\nfrom optim_esm_tools.config import config\nfrom optim_esm_tools.config import get_logger\n\n\ndef setup_map(\n    projection: ty.Optional[str] = None,\n    coastlines: bool = True,\n    add_features: bool = False,\n    add_gridlines: bool = True,\n    coast_line_kw: ty.Optional[dict] = None,\n    gridline_kw: ty.Optional[dict] = None,\n    no_top_labels: bool = True,\n    **projection_kwargs,\n):\n    plt.gcf().add_subplot(\n        projection=get_cartopy_projection(projection, **projection_kwargs),\n    )\n    ax = plt.gca()\n    if coastlines:\n        coast_line_kw = coast_line_kw or dict()\n        ax.coastlines(**coast_line_kw)\n    if add_features:\n        import cartopy.feature as cfeature\n\n        allowed = 'LAND OCEAN COASTLINE BORDERS LAKES RIVERS'.split()\n        for feat in oet.utils.to_str_tuple(add_features):\n            assert feat.upper() in allowed, f'{feat} not in {allowed}'\n            ax.add_feature(getattr(cfeature, feat.upper()))\n    if add_gridlines:\n        gridline_kw = gridline_kw or dict(draw_labels=True)\n        gl = ax.gridlines(**gridline_kw)\n        if no_top_labels:\n            gl.top_labels = False\n\n\ndef _show(show):\n    if show:\n        plt.show()  # pragma: no cover\n    else:\n        plt.clf()\n        plt.close()\n\n\ndef default_variable_labels() -> ty.Dict[str, str]:\n    labels = dict(config['variable_label'].items())\n    ma = config['analyze']['moving_average_years']\n    for k, v in list(labels.items()):\n        labels[f'{k}_detrend'] = f'Detrend {v}'\n        labels[f'{k}_run_mean_{ma}'] = f'$RM_{{{ma}}}$ {v}'\n        labels[f'{k}_detrend_run_mean_{ma}'] = f'Detrend $RM_{{{ma}}}$ {v}'\n    return labels\n\n\ndef get_range(var: str) -> ty.List[float]:\n    r = (\n        dict(oet.config.config['variable_range'].items())\n        .get(var, 'None,None')\n        .split(',')\n    )\n    return [(float(l) if l != 'None' else None) for l in r]\n\n\ndef set_y_lim_var(var: str) -> None:\n    d, u = get_range(var)\n    cd, cu = plt.ylim()\n    plt.ylim(\n        cd if d is None else min(cd, d),\n        cu if u is None else max(cu, u),\n    )\n\n\ndef get_unit_da(da: xr.DataArray) -> str:\n    return da.attrs.get('units', '?').replace('%', r'\\%')\n\n\ndef get_unit(ds: xr.Dataset, var: str) -> str:\n    return get_unit_da(ds[var])\n\n\ndef get_cartopy_projection(\n    projection: ty.Optional[ty.Any] = None,\n    _field: str = 'projection',\n    **projection_kwargs,\n) -> ty.Any:\n    import cartopy.crs as ccrs\n\n    projection = projection or config['cartopy'][_field]\n    if not hasattr(ccrs, projection):\n        raise ValueError(f'Invalid projection {projection}')  # pragma: no cover\n    return getattr(ccrs, projection)(**projection_kwargs)\n\n\ndef get_cartopy_transform(\n    projection: ty.Optional[ty.Any] = None,\n    **projection_kwargs,\n) -> ty.Any:\n    return get_cartopy_projection(\n        projection=projection,\n        _field='transform',\n        **projection_kwargs,\n    )\n\n\ndef get_xy_lim_for_projection(\n    projection: ty.Optional[str] = None,\n) -> ty.Tuple[ty.Tuple[float, float], ty.Tuple[float, float]]:\n    \"\"\"Blunt hardcoding for the different projections.\n\n    Calling plt.xlim(0, 360) will have vastly different outcomes\n    depending on the projection used. Here we hardcoded some of the more\n    common.\n    \"\"\"\n    projection = projection or config['cartopy']['projection']\n    lims = dict(\n        Robinson=(\n            (-17005833.33052523, 17005833.33052523),\n            (-8625154.6651, 8625154.6651),\n        ),\n        EqualEarth=(\n            (-17243959.06221695, 17243959.06221695),\n            (-8392927.59846645, 8392927.598466456),\n        ),\n        Mollweide=(\n            (-18040095.696147293, 18040095.696147293),\n            (-9020047.848073646, 9020047.848073646),\n        ),\n        PlateCarree=((0, 360), (-90, 90)),\n    )\n    if projection not in lims:\n        get_logger().warning(\n            f'No hardcoded x/y lims for {projection}, might yield odd figures.',\n        )  # pragma: no cover\n    return lims.get(projection, ((0, 360), (-90, 90)))\n\n\ndef plot_da(\n    da: xr.DataArray,\n    projection: ty.Optional[str] = None,\n    setup_kw: ty.Optional[dict] = None,\n    **kw,\n):\n    \"\"\"Simple wrapper for da.plot() with correct transforms and projections.\"\"\"\n    setup_kw = setup_kw or dict()\n    setup_map(projection=projection, **setup_kw)\n    da.plot(transform=get_cartopy_transform(), **kw)\n\n\ndef get_ylabel(ds: xr.Dataset, var: ty.Optional[str] = None):\n    var = var or ds.attrs.get('variable_id', 'var')\n    return f'{oet.plotting.plot.default_variable_labels().get(var, var)} [{get_unit(ds, var)}]'\n", "coverage": [1, null, 1, 1, null, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, 1, null, null, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, null, null, 1, null, null, 1, 1, 1, 1, null, null, null, null, null, 1, 1, null, null, 1, 1, null, null, 1, null, null, null, null, 1, null, 1, 1, null, 1, null, null, 1, null, null, null, 1, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, 0, 0, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 0, null, null, null, 0, null, null, 1, null, null, null, null, null, null, 1, 1, 1, null, null, 1, 1, 1]}, {"name": "optim_esm_tools/plotting/plot_utils.py", "source": "import os\nimport typing as ty\n\nimport numpy as np\nimport optim_esm_tools as oet\n\n\ndef legend_kw(**kw) -> dict:\n    options = dict(\n        bbox_to_anchor=(0.0, 1.02, 1, 0.32),\n        loc=3,\n        ncol=3,\n        mode='expand',\n        borderaxespad=0.0,\n        frameon=True,\n    )\n    options.update(kw)\n    return options\n\n\ndef save_fig(\n    name: str,\n    file_types: ty.Tuple[str] = ('png', 'pdf'),\n    save_in: ty.Optional[str] = None,\n    sub_dir: str = 'figures',\n    skip: bool = False,\n    remove_space: bool = True,\n    **kwargs,\n):\n    \"\"\"Save a figure in the figures dir.\"\"\"\n    import matplotlib.pyplot as plt\n\n    save_in = save_in or oet.utils.root_folder\n\n    kwargs.setdefault('dpi', 150)\n    kwargs.setdefault('bbox_inches', 'tight')\n    if remove_space:\n        name = name.replace(' ', '_')\n        save_in = save_in.replace(' ', '')\n        sub_dir = sub_dir.replace(' ', '')\n    if sub_dir is None:\n        sub_dir = ''\n    for file_type in file_types:\n        path = os.path.join(save_in, sub_dir, f'{name}.{file_type}')\n        if not os.path.exists(p := os.path.join(save_in, sub_dir)):\n            os.makedirs(p, exist_ok=True)\n        if skip:\n            print(f'Skip save {path}')\n            return\n        plt.savefig(path, **kwargs)\n\n\ndef get_plt_colors():\n    \"\"\"Get matplotlib colors.\"\"\"\n    import matplotlib.pyplot as plt\n    import matplotlib\n\n    my_colors = [matplotlib.colors.to_hex(c) for c in plt.cm.Set1.colors]\n    # I don't like the yellowish color\n    del my_colors[5]\n    return my_colors\n\n\ndef default_plt_params():\n    return {\n        'axes.grid': True,\n        'font.size': 18,\n        'axes.titlesize': 20,\n        'axes.labelsize': 18,\n        'axes.linewidth': 2,\n        'xtick.labelsize': 18,\n        'ytick.labelsize': 18,\n        'ytick.major.size': 8,\n        'ytick.minor.size': 4,\n        'xtick.major.size': 8,\n        'xtick.minor.size': 4,\n        'xtick.major.width': 2,\n        'xtick.minor.width': 2,\n        'ytick.major.width': 2,\n        'ytick.minor.width': 2,\n        'xtick.direction': 'in',\n        'ytick.direction': 'in',\n        'legend.fontsize': 14,\n        'figure.facecolor': 'w',\n        'figure.figsize': (8, 6),\n        'image.cmap': 'viridis',\n        'lines.linewidth': 2,\n    }\n\n\ndef setup_plt(use_tex: bool = True, custom_cmap_name: str = 'custom_map'):\n    \"\"\"Change the plots to have uniform style defaults.\"\"\"\n\n    import matplotlib.pyplot as plt\n    import matplotlib\n    from cycler import cycler\n\n    params = default_plt_params()\n    if use_tex:\n        params.update(\n            {\n                'font.family': 'Times New Roman',\n            },\n        )\n    plt.rcParams.update(params)\n\n    custom_cycler = cycler(color=get_plt_colors())\n    # Could add cycler(marker=['o', 's', 'v', '^', 'D', 'P', '>', 'x'])\n\n    plt.rcParams.update({'axes.prop_cycle': custom_cycler})\n    if use_tex and not os.environ.get('DISABLE_LATEX', False):\n        # Allow latex to be disabled from the environment coverage see\n        matplotlib.rc('text', usetex=True)\n\n    from matplotlib.colors import ListedColormap\n    import matplotlib as mpl\n\n    # Create capped custom map for printing (yellow does not print well)\n    custom = ListedColormap(mpl.colormaps['viridis'](np.linspace(0, 0.85, 1000)))\n    mpl.colormaps.register(custom, name=custom_cmap_name, force=True)\n    setattr(mpl.pyplot.cm, custom_cmap_name, custom)\n\n    custom_cmap_name += '_r'\n    custom = ListedColormap(mpl.colormaps['viridis_r'](np.linspace(0.15, 1, 1000)))\n    mpl.colormaps.register(custom, name=custom_cmap_name, force=True)\n    setattr(mpl.pyplot.cm, custom_cmap_name, custom)\n", "coverage": [1, 1, null, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, 1, null, 1, null, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, null, null, 1, null, 1, 1, null, 1, null, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, 1, 1, 1, null, 1, 1, 1, null, null, null, null, 1, null, 1, null, null, 1, 1, null, 1, null, 1, 1, null, null, 1, 1, 1, null, 1, 1, 1, 1]}, {"name": "optim_esm_tools/region_finding/__init__.py", "source": "from .keep_all import MaskAll\nfrom .iter_ranges import *\nfrom .local_history import LocalHistory\nfrom .max_region import MaxRegion\nfrom .named_region import Asia\nfrom .named_region import Medeteranian\nfrom .percentiles import Percentiles\nfrom .product_percentiles import ProductPercentiles\n", "coverage": [1, 1, 1, 1, 1, 1, 1, 1]}, {"name": "optim_esm_tools/region_finding/_base.py", "source": "import inspect\nimport logging\nimport os\nimport typing as ty\nfrom functools import wraps\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport xarray as xr\n\nimport optim_esm_tools as oet\nfrom optim_esm_tools.analyze import tipping_criteria\nfrom optim_esm_tools.analyze.globals import _CMIP_HANDLER_VERSION\nfrom optim_esm_tools.analyze.xarray_tools import mask_to_reduced_dataset\nfrom optim_esm_tools.plotting.plot import _show\n\n_mask_cluster_type = ty.Tuple[ty.List[np.ndarray], ty.List[np.ndarray]]\n\n# >>> import scipy\n# >>> scipy.stats.norm.cdf(3)\n# 0.9986501019683699\n# >> scipy.stats.norm.cdf(2)\n# 0.9772498680518208\n_two_sigma_percent = 97.72498680518208\n\n\ndef plt_show(*a):\n    \"\"\"Wrapper to disable class methods to follow up with show.\"\"\"\n\n    def somedec_outer(fn):\n        @wraps(fn)\n        def plt_func(*args, **kwargs):\n            res = fn(*args, **kwargs)\n            self = args[0]\n            _show(getattr(self, 'show', False))\n            return res\n\n        return plt_func\n\n    if a and isinstance(a[0], ty.Callable):\n        # Decorator that isn't closed\n        return somedec_outer(a[0])\n    return somedec_outer\n\n\ndef apply_options(*a):\n    \"\"\"If a function takes any arguments in self.extra_opt, apply it to the\n    method.\"\"\"\n\n    def somedec_outer(fn):\n        @wraps(fn)\n        def timed_func(*args, **kwargs):\n            self = args[0]\n            takes = inspect.signature(fn).parameters\n            kwargs.update({k: v for k, v in self.extra_opt.items() if k in takes})\n            res = fn(*args, **kwargs)\n            return res\n\n        return timed_func\n\n    if a and isinstance(a[0], ty.Callable):\n        # Decorator that isn't closed\n        return somedec_outer(a[0])\n    return somedec_outer\n\n\nclass RegionExtractor:\n    _logger: ty.Optional[logging.Logger] = None\n    labels: tuple = tuple('ii iii'.split())\n    show: bool = True\n\n    criteria: ty.Tuple = (tipping_criteria.StdDetrended, tipping_criteria.MaxJump)\n    extra_opt: ty.Mapping\n    save_kw: ty.Mapping\n    save_statistics: bool = True\n    data_set: xr.Dataset\n\n    def __init__(\n        self,\n        data_set: xr.Dataset,\n        variable: ty.Optional[str] = None,\n        save_kw: ty.Optional[dict] = None,\n        extra_opt: ty.Optional[dict] = None,\n    ) -> None:\n        \"\"\"The function initializes an object with various parameters and\n        assigns default values if necessary.\n\n        :param variable: The `variable` parameter is used to specify the variable ID. If it is not\n        provided, the code will try to retrieve the variable ID from the `data_set` attribute. If it is\n        not found, it will default to the string 'NO_VAR_ID!'\n        :param path: The path to the data file that will be read\n        :param data_set: The `data_set` parameter is used to specify the dataset that will be used for\n        further processing.\n        :param save_kw: A dictionary containing the following keys and values:\n        :param extra_opt: The `extra_opt` parameter is a dictionary that contains additional options for\n        the object. It has the following keys:\n        \"\"\"\n        self.data_set = data_set\n\n        save_kw = save_kw or dict(\n            save_in='./',\n            file_types=(\n                'png',\n                'pdf',\n            ),\n            skip=False,\n            sub_dir=None,\n        )\n        extra_opt = extra_opt or dict(show_basic=True)\n\n        self.extra_opt = extra_opt\n        self.save_kw = save_kw\n        self.variable = variable or self.data_set.attrs.get('variable_id', 'NO_VAR_ID!')  # type: ignore\n\n    @property\n    def log(self) -> logging.Logger:\n        \"\"\"The function returns a logger object, creating one if it doesn't\n        already exist.\n\n        :return: The method is returning the `_logger` attribute.\n        \"\"\"\n        if self._logger is None:\n            self._logger = oet.config.get_logger(f'{self.__class__.__name__}')\n        return self._logger\n\n    def get_masks(self) -> _mask_cluster_type:  # pragma: no cover\n        raise NotImplementedError(\n            f'{self.__class__.__name__} has no get_masks',\n        )\n\n    def plot_masks(\n        self,\n        masks_and_clusters: _mask_cluster_type,\n        **kw,\n    ) -> _mask_cluster_type:  # pragma: no cover\n        raise NotImplementedError(\n            f'{self.__class__.__name__} has no plot_masks',\n        )\n\n    def plot_mask_time_series(\n        self,\n        masks_and_clusters: _mask_cluster_type,\n        **kw,\n    ) -> _mask_cluster_type:  # pragma: no cover\n        raise NotImplementedError(\n            f'{self.__class__.__name__} has no plot_mask_time_series',\n        )\n\n    def _plot_basic_map(self):  # pragma: no cover\n        raise NotImplementedError(\n            f'{self.__class__.__name__} has no _plot_basic_map',\n        )\n\n    def mask_area(self, mask: np.ndarray) -> np.ndarray:\n        \"\"\"The function `mask_area` returns the cell areas from a dataset based\n        on a given mask.\n\n        :param mask: The `mask` parameter is a numpy array that represents a mask. It is used to select\n        specific elements from the `data_set['cell_area'].values` array. The mask should have the same shape\n        as the `data_set['cell_area'].values` array, and its elements should be boolean\n        :type mask: np.ndarray\n        :return: an array containing the values of the 'cell_area' column from the 'data_set' attribute,\n        filtered by the provided 'mask' array.\n        \"\"\"\n        try:\n            if mask is None or not np.sum(mask):\n                return np.array([0])  # pragma: no cover\n        except Exception as e:  # pragma: no cover\n            raise ValueError(mask) from e\n        self.check_shape(mask)\n        return self.data_set['cell_area'].values[mask]\n\n    def check_shape(\n        self,\n        data: ty.Union[np.ndarray, xr.DataArray],\n        compare_with='cell_area',\n    ) -> None:\n        \"\"\"The `check_shape` function compares the shape of a given data array\n        with the shape of a reference array and raises a ValueError if they are\n        not equal.\n\n        :param data: The `data` parameter can be either a NumPy array (`np.ndarray`) or an xarray DataArray\n        (`xr.DataArray`). It represents the data that needs to be checked for its shape\n        :type data: ty.Union[np.ndarray, xr.DataArray]\n        :param compare_with: The `compare_with` parameter is a string that specifies the variable name in\n        the `self.data_set` object that you want to compare the shape of the `data` parameter with. It is\n        used to determine the expected shape of the `data` parameter, defaults to cell_area (optional)\n        :return: `None` if the shape of the input `data` matches the shape specified by `shape_should_be`.\n        \"\"\"\n        shape_should_be = self.data_set[compare_with].shape\n        if data.shape == shape_should_be:\n            return\n        error_message = f'Got {data.shape}, expected {shape_should_be}'\n        if name := getattr(data, 'name', False):\n            error_message = f'For {name}: {error_message}'\n        if dims := getattr(data, 'dims', False):\n            error_message = f'{error_message}. Dims are {dims}, expected'\n        error_message += f'for {self.data_set[compare_with].dims}'\n        raise ValueError(error_message)\n\n    @apply_options\n    def mask_is_large_enough(self, mask: np.ndarray, min_area_sq: float = 0.0) -> bool:\n        \"\"\"The function checks if the area of a given mask is larger than or\n        equal to a specified minimum area.\n\n        :param mask: The `mask` parameter is a numpy array representing a binary mask. It is typically used\n        to represent a region of interest or a segmentation mask in an image. The mask should have the same\n        shape as the image it corresponds to, with a value of 1 indicating the presence of the object and a\n        :type mask: np.ndarray\n        :param min_area_sq: The parameter `min_area_sq` represents the minimum area (in square units) that\n        the `mask` should have in order for the function to return `True`\n        :type min_area_sq: float\n        :return: a boolean value, indicating whether the sum of the areas of the given mask is greater than\n        or equal to the specified minimum area.\n        \"\"\"\n        return self.mask_area(mask).sum() >= min_area_sq\n\n    def mask_to_lon_lat(self, mask):\n        ds = self.data_set\n        if not isinstance(mask, np.ndarray):\n            mask = mask.values\n        lats, lons = np.meshgrid(ds.lat.values, ds.lon.values)\n        lon_coords = lons.T[mask]\n        lat_coords = lats.T[mask]\n        return np.vstack([lon_coords, lat_coords]).T\n\n    def filter_masks_and_clusters(\n        self,\n        masks_and_clusters: _mask_cluster_type,\n    ) -> _mask_cluster_type:\n        \"\"\"The function filters a list of masks and clusters based on the size\n        of the masks, and returns the filtered lists.\n\n        :param masks_and_clusters: A tuple containing two lists. The first list contains masks, and the\n        second list contains clusters\n        :type masks_and_clusters: _mask_cluster_type\n        :return: two lists: `ret_m` and `ret_c`.\n        \"\"\"\n        if not len(masks_and_clusters[0]):\n            return [], []\n        ret_m = []\n        ret_c = []\n        for m, c in zip(*masks_and_clusters):\n            if self.mask_is_large_enough(m):\n                ret_m.append(m)\n                ret_c.append(c)\n\n        self.log.info(f'Keeping {len(ret_m)}/{len(masks_and_clusters[0])} of masks')\n        return ret_m, ret_c\n", "coverage": [1, 1, 1, 1, 1, null, 1, 1, 1, null, 1, 1, 1, 1, 1, null, 1, null, null, null, null, null, null, 1, null, null, 1, null, null, 0, 0, 0, 0, 0, 0, 0, null, 0, null, 0, null, 0, 0, null, null, 1, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, null, 1, null, 1, 0, null, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1, null, null, null, null, null, null, null, null, 1, null, 1, 1, 1, null, 1, 1, null, null, null, null, null, 1, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, null, null, 1, 1, null, null, null, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1, 1, 1, 0, 1, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, 1, 0, 1, 1, 1, 1, 1, 1, null, 1, 1]}, {"name": "optim_esm_tools/region_finding/iter_ranges.py", "source": "import typing as ty\nfrom abc import ABC\n\nimport numpy as np\nimport xarray as xr\n\nimport optim_esm_tools as oet\nfrom ._base import _mask_cluster_type\nfrom ._base import apply_options\nfrom .local_history import LocalHistory\nfrom .percentiles import Percentiles\nfrom .product_percentiles import ProductPercentiles\nfrom optim_esm_tools.analyze import tipping_criteria\nfrom optim_esm_tools.analyze.clustering import build_cluster_mask\nfrom optim_esm_tools.region_finding._base import _mask_cluster_type\n\n\nclass _ThresholdIterator(ABC):\n    _tqmd: bool = False\n\n    data_set: xr.Dataset\n\n    _get_mask_function_and_kw: ty.Callable\n    _force_continuity: ty.Callable\n    mask_area: ty.Callable\n\n    def _get_masks_masked(\n        self,\n        iterable_range: ty.Dict[str, ty.Iterable] = dict(percentiles=(99.5, 97.5, 90)),\n        lon_lat_dim: ty.Tuple[str, str] = ('lon', 'lat'),\n        _mask_method: str = 'not_specified',\n        iter_mask_min_area: float = 1e12,\n        iter_mask_max_area: float = 999e12,\n        force_continuity: bool = False,\n    ) -> _mask_cluster_type:\n        \"\"\"The function `_get_masks_masked` builds masks and clusters based on\n        a given iterable range and a combination of masks, and returns the\n        masks and clusters that meet certain size criteria.\n\n        :param iterable_range: A dictionary that specifies the range of values for the iteration. The\n        keys represent the name of the parameter being iterated over, and the values represent the\n        iterable range of values for that parameter\n        :type iterable_range: ty.Dict[str, ty.Iterable]\n        :param lon_lat_dim: The `lon_lat_dim` parameter is a tuple that specifies the names of the\n        longitude and latitude dimensions in your dataset. These dimensions are used to extract the\n        corresponding coordinate values for building the mask\n        :param _mask_method: The `_mask_method` parameter is a string that specifies the method to be\n        used for building the combined mask. It is not specified in the code snippet provided, so its\n        value is not known, defaults to not_specified (optional)\n        :param iter_mask_min_area: The parameter `iter_mask_min_area` represents the minimum area\n        threshold for a mask. It is used to filter out masks that have a size smaller than this\n        threshold\n        :param iter_mask_max_area: The parameter `iter_mask_max_area` represents the maximum area that a\n        mask can have. Masks with an area greater than or equal to `iter_mask_max_area` will raise an\n        error\n        :return: two lists: `masks` and `clusters`.\n        \"\"\"\n        already_seen = None\n        masks, clusters = [], []\n\n        _, filtered_kwargs = self._get_mask_function_and_kw(\n            method=_mask_method,\n            **iterable_range,\n        )\n        iter_key, iter_values = list(filtered_kwargs.items())[0]\n\n        pbar = oet.utils.tqdm(iter_values, disable=not self._tqmd)\n        for value in pbar:\n            pbar.desc = f'{iter_key} = {value:.3g}'\n\n            all_mask = self._build_combined_mask(  # type: ignore\n                method=_mask_method,\n                **{iter_key: value},\n            )\n\n            if already_seen is not None:\n                all_mask[already_seen] = False\n\n            these_masks, these_clusters = build_cluster_mask(\n                all_mask,\n                lon_coord=self.data_set[lon_lat_dim[0]].values,\n                lat_coord=self.data_set[lon_lat_dim[1]].values,\n                force_continuity=force_continuity,\n            )\n            for m, c in zip(these_masks, these_clusters):\n                size = self.mask_area(m).sum()\n                if size >= iter_mask_min_area and size < iter_mask_max_area:\n                    masks.append(m)\n                    clusters.append(c)\n                    if already_seen is None:\n                        already_seen = m.copy()\n                    already_seen[m] = True\n                elif size >= iter_mask_max_area:\n                    raise ValueError(\n                        f'Got {size/iter_mask_min_area:.1%} target size for {value}',\n                    )\n\n        pbar.close()\n        return masks, clusters\n\n    def _get_masks_weighted(self, *a, **kw):\n        raise NotImplementedError\n\n\nclass IterProductPercentiles(_ThresholdIterator, ProductPercentiles):\n    @apply_options\n    def _get_masks_masked(\n        self,\n        iterable_range=dict(product_percentiles=(99.5, 97.5, 90)),\n        lon_lat_dim=('lon', 'lat'),\n        iter_mask_min_area=1e12,\n        iter_mask_max_area=999e12,\n        force_continuity=False,\n    ) -> _mask_cluster_type:\n        return super()._get_masks_masked(\n            iterable_range=iterable_range,\n            lon_lat_dim=lon_lat_dim,\n            iter_mask_min_area=iter_mask_min_area,\n            iter_mask_max_area=iter_mask_max_area,\n            force_continuity=force_continuity,\n            _mask_method='product_rank_past_threshold',\n        )\n\n\nclass IterLocalHistory(_ThresholdIterator, LocalHistory):\n    @apply_options\n    def get_masks(\n        self,\n        iterable_range=dict(n_times_historical=(6, 5, 4, 3)),\n        lon_lat_dim=('lon', 'lat'),\n        iter_mask_min_area=1e12,\n        iter_mask_max_area=999e12,\n        force_continuity=False,\n    ) -> _mask_cluster_type:\n        return super()._get_masks_masked(\n            iterable_range=iterable_range,\n            lon_lat_dim=lon_lat_dim,\n            iter_mask_min_area=iter_mask_min_area,\n            iter_mask_max_area=iter_mask_max_area,\n            force_continuity=force_continuity,\n            _mask_method='all_pass_historical',\n        )\n\n\nclass IterPercentiles(_ThresholdIterator, Percentiles):\n    @apply_options\n    def _get_masks_masked(\n        self,\n        iterable_range=dict(percentiles=(99.5, 97.5, 90)),\n        lon_lat_dim=('lon', 'lat'),\n        iter_mask_min_area=1e12,\n        iter_mask_max_area=999e12,\n        force_continuity=False,\n    ) -> _mask_cluster_type:\n        return super()._get_masks_masked(\n            iterable_range=iterable_range,\n            lon_lat_dim=lon_lat_dim,\n            iter_mask_min_area=iter_mask_min_area,\n            iter_mask_max_area=iter_mask_max_area,\n            force_continuity=force_continuity,\n            _mask_method='all_pass_percentile',\n        )\n\n\nclass IterStartEnd(_ThresholdIterator, ProductPercentiles):\n    labels = ('i',)\n    criteria: ty.Tuple = (tipping_criteria.StartEndDifference,)\n\n    @apply_options\n    def _get_masks_masked(\n        self,\n        iterable_range=dict(product_percentiles=np.linspace(99.9, 85, 41)),\n        lon_lat_dim=('lon', 'lat'),\n        iter_mask_min_area=1e12,\n        iter_mask_max_area=999e12,\n        force_continuity=False,\n    ) -> _mask_cluster_type:\n        return super()._get_masks_masked(\n            iterable_range=iterable_range,\n            lon_lat_dim=lon_lat_dim,\n            iter_mask_min_area=iter_mask_min_area,\n            iter_mask_max_area=iter_mask_max_area,\n            force_continuity=force_continuity,\n            _mask_method='product_rank_past_threshold',\n        )\n\n\nclass IterSNR(IterStartEnd):\n    labels = ('i',)\n    criteria: ty.Tuple = (tipping_criteria.SNR,)\n\n\nclass IterStd(IterStartEnd):\n    labels = ('ii',)\n    criteria: ty.Tuple = (tipping_criteria.StdDetrended,)\n\n\nclass IterMJ(IterStartEnd):\n    labels = ('iii',)\n    criteria: ty.Tuple = (tipping_criteria.MaxJump,)\n", "coverage": [1, 1, null, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, 1, null, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, null, null, null, 1, null, 1, 1, 1, null, 1, null, null, null, null, 1, 1, null, 1, null, null, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, null, null, null, 1, 1, null, 1, 1, null, null, 1, 1, 1, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, 1, null, null, 1, 1, 1, null, null, 1, 1, 1]}, {"name": "optim_esm_tools/region_finding/keep_all.py", "source": "from ._base import RegionExtractor, _mask_cluster_type, apply_options\nimport itertools\nimport numpy as np\nimport typing as ty\nimport xarray as xr\nimport optim_esm_tools as oet\n\n\nclass MaskAll(RegionExtractor):\n    @apply_options\n    def get_masks(\n        self,\n        step_size: int = 5,\n        force_continuity: bool = True,\n    ) -> _mask_cluster_type:  # pragma: no cover\n        mask_2d: xr.DataArray = ~self.data_set[self.variable].isnull().all(dim='time')\n        mask_values: np.ndarray = mask_2d.values\n        lats: np.ndarray = self.data_set['lat'].values\n        lons: np.ndarray = self.data_set['lon'].values\n        masks: ty.List[np.ndarray] = []\n        coords: ty.List[np.ndarray] = []\n\n        for i, j in itertools.product(\n            range(0, mask_2d.shape[0], step_size),\n            range(0, mask_2d.shape[1], step_size),\n        ):\n            this_mask = np.zeros_like(mask_values)\n            this_coords = []\n            for ii in range(i, i + step_size):\n                if ii >= len(lats):\n                    continue\n                for jj in range(j, j + step_size):\n                    if jj >= len(lons):\n                        continue\n                    if mask_values[ii][jj]:\n                        this_coords.append([lats[ii], lons[jj]])\n                        this_mask[ii][jj] = True\n            if this_mask.sum():\n                coords.append(np.array(this_coords))\n                masks.append(this_mask)\n        if force_continuity:\n            masks = oet.analyze.clustering._split_to_continuous(masks=masks)\n            lat, lon = np.meshgrid(lats, lons)\n            coords = [\n                oet.analyze.clustering._find_lat_lon_values(m, lats=lat.T, lons=lon.T)\n                for m in masks\n            ]\n        return masks, coords\n\n    def filter_masks_and_clusters(\n        self,\n        masks_and_clusters: _mask_cluster_type,\n    ) -> _mask_cluster_type:\n        return masks_and_clusters\n", "coverage": [1, 1, 1, 1, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, 0]}, {"name": "optim_esm_tools/region_finding/local_history.py", "source": "import abc\nimport typing as ty\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.typing as npt\nimport xarray as xr\n\nimport optim_esm_tools as oet\nfrom ._base import _mask_cluster_type\nfrom ._base import apply_options\nfrom optim_esm_tools.analyze.clustering import build_cluster_mask\nfrom optim_esm_tools.region_finding.percentiles import Percentiles\nfrom optim_esm_tools.utils import check_accepts\n\n\nclass _HistroricalLookup(abc.ABC):\n    data_set: xr.Dataset\n    data_set_pic: ty.Optional[xr.Dataset]\n\n    def __init__(self, *a, data_set_pic: xr.Dataset, **kw) -> None:\n        super().__init__(*a, **kw)\n        self.data_set_pic: ty.Optional[xr.Dataset] = data_set_pic\n\n    @apply_options\n    def find_historical(\n        self,\n        match_to: str = 'piControl',\n        look_back_extra: int = 0,\n        query_updates: ty.Optional[ty.Mapping] = None,\n        search_kw: ty.Optional[ty.Mapping] = None,\n    ) -> ty.Optional[ty.List[str]]:\n        raise NotImplementedError(\n            'This behavior is deprecated, use data_set_pic at __init__ instead',\n        )\n\n    @apply_options\n    def get_historical_ds(\n        self,\n        read_ds_kw: ty.Optional[ty.Mapping] = None,\n        **kw,\n    ) -> xr.Dataset:\n        if self.data_set_pic is not None:\n            return self.data_set_pic\n\n        # Which raises NotImplementedError\n        return self.find_historical()\n\n\nclass LocalHistory(_HistroricalLookup, Percentiles):\n    def _all_pass_historical(\n        self,\n        labels: ty.List[str],\n        n_times_historical: ty.Union[float, int],\n        read_ds_kw: ty.Optional[ty.Mapping] = None,\n    ) -> npt.NDArray[np.bool_]:\n        read_ds_kw = read_ds_kw or {}\n        for k, v in dict(min_time=None, max_time=None).items():\n            read_ds_kw.setdefault(k, v)  # type: ignore\n\n        historical_ds = self.get_historical_ds(read_ds_kw=read_ds_kw)\n\n        masks = []\n\n        for lab in labels:\n            arr = self.data_set[lab].values\n            arr_historical = historical_ds[lab].values\n\n            # If arr_historical is 0, the division is going to get a nan assigned,\n            # despite this being the most interesting region (no historical\n            # changes, only in the scenario's)!\n            mask_no_std = arr_historical == 0\n            mask_divide = np.zeros_like(mask_no_std)\n            mask_divide[~mask_no_std] = (\n                arr[~mask_no_std] / arr_historical[~mask_no_std] > n_times_historical\n            )\n            masks.append(mask_divide | (mask_no_std & (arr != 0)))\n\n        all_mask = np.ones_like(masks[0])\n        for m in masks:\n            all_mask &= m\n        return all_mask\n\n    @apply_options\n    def get_masks(\n        self,\n        n_times_historical: ty.Union[int, float] = 4,\n        read_ds_kw: ty.Optional[ty.Mapping] = None,\n        lon_lat_dim: ty.Tuple[str, str] = ('lon', 'lat'),\n    ) -> _mask_cluster_type:\n        all_mask = self._build_combined_mask(\n            method='all_pass_historical',\n            n_times_historical=n_times_historical,\n            read_ds_kw=read_ds_kw,\n        )\n        masks, clusters = build_cluster_mask(\n            all_mask,\n            lon_coord=self.data_set[lon_lat_dim[0]].values,\n            lat_coord=self.data_set[lon_lat_dim[1]].values,\n        )\n        return masks, clusters\n", "coverage": [1, 1, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, null, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, 0, null, null, null, 1, 1, null, null, null, null, 1, 1, null, null, 0, null, null, 1, 1, null, null, null, null, null, 1, 1, 1, null, 1, null, 1, null, 1, 1, 1, null, null, null, null, 1, 1, 1, null, null, 1, null, 1, 1, 1, 1, null, 1, 1, null, null, null, null, null, 1, null, null, null, null, 1, null, null, null, null, 1]}, {"name": "optim_esm_tools/region_finding/max_region.py", "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\nimport optim_esm_tools as oet\nfrom ._base import _mask_cluster_type\nfrom ._base import apply_options\nfrom ._base import plt_show\nfrom ._base import RegionExtractor\n\nfrom optim_esm_tools.plotting.plot import _show\n\n\nclass MaxRegion(RegionExtractor):\n    def get_masks(self) -> _mask_cluster_type:\n        \"\"\"Get mask for max of ii and iii and a box around that.\"\"\"\n\n        def _val(label):\n            return self.data_set[label].values\n\n        def _max(label):\n            return _val(label)[~np.isnan(_val(label))].max()\n\n        masks = [_val(label) == _max(label) for label in self._labels]\n        return masks, [np.array([]) for _ in range(len(masks))]\n\n    @apply_options\n    def filter_masks_and_clusters(self, masks_and_clusters, min_area_km_sq=0):\n        \"\"\"Wrap filter to work on dicts.\"\"\"\n        if min_area_km_sq:  # pragma: no cover\n            message = f'Calling {self.__class__.__name__}.filter_masks_and_clusters is nonsensical as masks are single grid cells'\n            self.log.warning(message)\n        return masks_and_clusters\n\n    @property\n    def _labels(self):\n        return [crit.short_description for crit in self.criteria]\n", "coverage": [1, 1, null, 1, 1, 1, 1, 1, null, 1, null, null, 1, 1, null, null, 1, 1, null, 1, 1, null, 1, 1, null, 1, 1, null, null, null, null, 0, null, 1, 1, 1]}, {"name": "optim_esm_tools/region_finding/named_region.py", "source": "import itertools\nimport typing as ty\n\nimport numpy as np\nimport regionmask\n\nfrom ._base import _mask_cluster_type\nfrom ._base import RegionExtractor\nfrom ._base import apply_options\n\n\nclass _NamedRegions(RegionExtractor):\n    region_database = regionmask.defined_regions.ar6.all\n\n    _default_regions: ty.Tuple[str, ...]\n\n    @apply_options\n    def get_masks(\n        self,\n        select_regions: ty.Optional[ty.Tuple[str, ...]] = None,\n    ) -> _mask_cluster_type:\n        if select_regions is None:\n            select_regions = self._default_regions\n        mask_2d = ~self.data_set[self.variable].isnull().all(dim='time')\n        region_map = self.region_database.mask(self.data_set.lon, self.data_set.lat)\n        mask_values = mask_2d.values\n\n        masks = []\n        for i, b in enumerate(self.region_database.names):\n            if b not in select_regions:\n                continue\n\n            masks.append((region_map == i).values & mask_values)\n\n        coords = []\n\n        for m in masks:\n            # Format to lat,lon instead of lon,lat\n            coords.append(np.array(self.mask_to_lon_lat(m)))\n        return masks, coords\n\n    def filter_masks_and_clusters(\n        self,\n        masks_and_clusters: _mask_cluster_type,\n    ) -> _mask_cluster_type:\n        return masks_and_clusters\n\n\nclass Medeteranian(_NamedRegions):\n    _default_regions: ty.Tuple[str, ...] = (\n        'S.W.South-America',\n        'W.North-America',\n        'N.Central-America',\n        'Mediterranean',\n        'S.Australia',\n        'W.Southern-Africa',\n        'E.Southern-Africa',\n    )\n\n\nclass Asia(_NamedRegions):\n    _default_regions: ty.Tuple[str, ...] = (\n        'Russian-Far-East',\n        'E.Asia',\n        'S.E.Asia',\n        'N.Australia',\n    )\n", "coverage": [1, 1, null, 1, 1, null, 1, 1, 1, null, null, 1, 1, null, 1, null, 1, 1, null, null, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, 1, null, 1, null, 1, null, 1, 1, null, 1, null, null, null, 0, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, 1, 1, null, null, null, null, null]}, {"name": "optim_esm_tools/region_finding/percentiles.py", "source": "import typing as ty\n\nimport immutabledict\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.typing as npt\n\nimport optim_esm_tools as oet\nfrom ._base import _mask_cluster_type\nfrom ._base import _two_sigma_percent\nfrom ._base import apply_options\nfrom ._base import plt_show\nfrom ._base import RegionExtractor\nfrom optim_esm_tools.analyze import tipping_criteria\nfrom optim_esm_tools.analyze.clustering import build_cluster_mask\nfrom optim_esm_tools.analyze.clustering import build_weighted_cluster\nfrom optim_esm_tools.analyze.xarray_tools import mask_xr_ds\nfrom optim_esm_tools.plotting.plot import _show\nfrom optim_esm_tools.plotting.plot import setup_map\nfrom optim_esm_tools.utils import check_accepts\n\n\nclass Percentiles(RegionExtractor):\n    @oet.utils.check_accepts(\n        accepts=immutabledict.immutabledict(cluster_method=('weighted', 'masked')),\n    )\n    @apply_options\n    def get_masks(self, cluster_method='masked') -> _mask_cluster_type:\n        \"\"\"The function `get_masks` returns masks and clusters based on the\n        specified cluster method.\n\n        :param cluster_method: The `cluster_method` parameter is a string that determines the method used to\n        generate masks. It can have two possible values:, defaults to masked (optional)\n        :return: two values: masks and clusters.\n        \"\"\"\n        if cluster_method == 'weighted':\n            masks, clusters = self._get_masks_weighted()\n        else:\n            masks, clusters = self._get_masks_masked()\n        if len(masks):\n            self.check_shape(masks[0])\n        return masks, clusters\n\n    @apply_options\n    def _get_masks_weighted(\n        self,\n        min_weight=0.95,\n        lon_lat_dim=('lon', 'lat'),\n        _mask_method='sum_rank',\n    ):\n        \"\"\"The function `_get_masks_weighted` calculates weighted masks and\n        clusters based on a minimum weight threshold.\n\n        :param min_weight: The min_weight parameter is the minimum weight threshold for including a mask in\n        the output. Masks with weights below this threshold will be excluded\n        :param lon_lat_dim: The `lon_lat_dim` parameter is a tuple that specifies the names of the longitude\n        and latitude dimensions in the dataset. These dimensions are used to extract the longitude and\n        latitude coordinates from the dataset\n        :param _mask_method: The `_mask_method` parameter is used to specify the method for building the\n        combined mask. It is a string that can take one of the following values:, defaults to sum_rank\n        (optional)\n        :return: two variables: masks and clusters.\n        \"\"\"\n        tot_sum = self._build_combined_mask(method=_mask_method)\n        masks, clusters = build_weighted_cluster(\n            weights=tot_sum,\n            lon_coord=self.data_set[lon_lat_dim[0]].values,\n            lat_coord=self.data_set[lon_lat_dim[1]].values,\n            threshold=min_weight,\n        )\n        return masks, clusters\n\n    @apply_options\n    def _get_masks_masked(\n        self,\n        lon_lat_dim=('lon', 'lat'),\n        percentiles=_two_sigma_percent,\n        _mask_method='all_pass_percentile',\n    ):\n        \"\"\"The function `_get_masks_masked` builds a combined mask using a\n        specified method and percentiles, and then builds cluster masks based\n        on the combined mask and lon/lat coordinates.\n\n        :param lon_lat_dim: A tuple specifying the names of the longitude and latitude dimensions in the\n        dataset\n        :param percentiles: The `percentiles` parameter is a list of percentiles used for masking. It is set\n        to `_two_sigma_percent` in the code, which suggests that it is a predefined variable containing a\n        list of percentiles\n        :param _mask_method: The `_mask_method` parameter is used to specify the method for building the\n        combined mask. It determines how the individual masks are combined to create the final mask. The\n        available options for `_mask_method` are:, defaults to all_pass_percentile (optional)\n        :return: two values: masks and clusters.\n        \"\"\"\n        all_mask = self._build_combined_mask(\n            method=_mask_method,\n            percentiles=percentiles,\n        )\n        masks, clusters = build_cluster_mask(\n            all_mask,\n            lon_coord=self.data_set[lon_lat_dim[0]].values,\n            lat_coord=self.data_set[lon_lat_dim[1]].values,\n        )\n        return masks, clusters\n\n    @check_accepts(\n        accepts=dict(\n            method=(\n                'sum_rank',\n                'all_pass_percentile',\n                'product_rank',\n                'product_rank_past_threshold',\n                'all_pass_historical',\n            ),\n        ),\n    )\n    def _get_mask_function_and_kw(\n        self,\n        method: str,\n        **kw,\n    ) -> ty.Tuple[ty.Callable, dict]:\n        functions = dict(\n            all_pass_percentile=self._all_pass_percentile,\n            all_pass_historical=self._all_pass_historical,\n            product_rank=self._product_rank,\n            product_rank_past_threshold=self._product_rank_past_threshold,\n            sum_rank=self._sum_rank,\n        )\n        func = functions[method]\n        assert isinstance(func, ty.Callable)\n        filter_kw = oet.utils.filter_keyword_arguments(kw, func, allow_varkw=False)\n        if removed := set(kw) - set(filter_kw):\n            self.log.info(\n                f'Removed {removed}. Started with {kw}, returning res {filter_kw}',\n            )\n        return func, filter_kw\n\n    def _build_combined_mask(self, method: str, **kw) -> np.ndarray:\n        \"\"\"The `_build_combined_mask` function takes a method and keyword\n        arguments, uses the method to select a function from a dictionary, and\n        applies the selected function to a list of labels to generate a result.\n\n        :param method: The \"method\" parameter is a string that specifies\n            the method to be used for building the combined mask. The\n            available methods are:\n                - all_pass_percentile\n                - all_pass_historical\n                - product_rank\n                - product_rank_past_threshold\n                - sum_rank\n        :type method: str\n        :return: a numpy array.\n        \"\"\"\n        labels = [crit.short_description for crit in self.criteria]\n        func, filter_kw = self._get_mask_function_and_kw(method=method, **kw)\n        result = func(labels, **filter_kw)\n        self.check_shape(result)\n        return result\n\n    def _all_pass_percentile(\n        self,\n        labels: ty.List[str],\n        percentiles: ty.Union[float, int],\n    ) -> npt.NDArray[np.bool_]:\n        \"\"\"The `_all_pass_percentile` function calculates a mask that indicates\n        whether each element in the data set is greater than or equal to the\n        percentile threshold for each label.\n\n        :param labels: A list of strings representing the labels of the data set. Each label corresponds to\n        a column in the data set\n        :type labels: ty.List[str]\n        :param percentiles: The `percentiles` parameter is a list of values representing the percentiles at\n        which you want to calculate the threshold. It can be a single value or a list of values. For\n        example, if you pass `percentiles=90`, it will calculate the threshold at the 90th percentile\n        :type percentiles: ty.Union[float, int]\n        :return: a NumPy array of boolean values.\n        \"\"\"\n        masks = []\n\n        for lab in labels:\n            arr = self.data_set[lab].values\n            arr_no_nan = arr[~np.isnan(arr)]\n            thr = np.percentile(arr_no_nan, percentiles)\n            masks.append(arr >= thr)\n\n        all_mask = np.ones_like(masks[0])\n        for m in masks:\n            all_mask &= m\n        return all_mask\n\n    def _all_pass_historical(self, *a, **kw):\n        raise NotImplementedError(\n            f'{self.__class__.__name__} has not method all_pass_historical',\n        )  # pragma: no cover\n\n    def _sum_rank(self, labels: ty.List[str]) -> npt.NDArray[np.float64]:\n        \"\"\"The `_sum_rank` function calculates the average rank of values in a\n        dataset for a given list of labels.\n\n        :param labels: The `labels` parameter is a list of strings representing the labels of the data set\n        :type labels: ty.List[str]\n        :return: a numpy array of type np.float64.\n        \"\"\"\n        sums = []\n        for lab in labels:\n            vals = self.data_set[lab].values\n            vals = tipping_criteria.rank2d(vals)\n            vals[np.isnan(vals)] = 0\n            sums.append(vals)\n\n        tot_sum = np.zeros_like(sums[0], dtype=np.float64)\n        for s in sums:\n            tot_sum += s\n        tot_sum /= len(sums)\n        return tot_sum\n\n    def _product_rank(self, labels: ty.List[str]) -> npt.NDArray[np.float64]:\n        \"\"\"The `_product_rank` function calculates the combined score of\n        multiple labels using the `rank2d` function and returns the result as a\n        numpy array.\n\n        :param labels: A list of strings representing the labels of the\n            columns in the dataset\n        :type labels: ty.List[str]\n        :return: a NumPy array of type np.float64.\n        \"\"\"\n        ds = self.data_set.copy()\n        combined_score = np.ones_like(ds[labels[0]].values, dtype=np.float64)\n\n        for label in labels:\n            try:\n                combined_score *= tipping_criteria.rank2d(ds[label].values)\n            except ValueError:\n                raise ValueError(ds[label].values, label)\n        return combined_score\n\n    def _product_rank_past_threshold(\n        self,\n        labels: ty.List[str],\n        product_percentiles: ty.Union[float, int],\n    ) -> npt.NDArray[np.bool_]:\n        \"\"\"The function `_product_rank_past_threshold` calculates the combined\n        score for a list of labels and returns a boolean array indicating\n        whether each score is above a given percentile threshold.\n\n        :param labels: A list of strings representing the labels of the products\n        :type labels: ty.List[str]\n        :param product_percentiles: The `product_percentiles` parameter is a value that represents the\n        threshold for the combined score. It can be either a float or an integer. If it is a float, it\n        represents a fraction (e.g., 0.5 represents 50%). If it is an integer, it represents a\n        :type product_percentiles: ty.Union[float, int]\n        :return: a NumPy array of boolean values.\n        \"\"\"\n        combined_score = self._product_rank(labels)\n        # Combined score is fraction, not percent!\n        return combined_score > (product_percentiles / 100)\n", "coverage": [1, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, null, null, null, null, 1, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, 1, null, null, null, null, 1, null, 1, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, null, 1, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, null, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, null, 1, null, null, null, null, 1, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, null, 1, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, 0, 0, 1, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, 1]}, {"name": "optim_esm_tools/region_finding/product_percentiles.py", "source": "import immutabledict\nimport numpy as np\n\nimport optim_esm_tools as oet\nfrom ._base import _mask_cluster_type\nfrom ._base import _two_sigma_percent\nfrom ._base import apply_options\nfrom optim_esm_tools.analyze import tipping_criteria\nfrom optim_esm_tools.analyze.clustering import build_cluster_mask\nfrom optim_esm_tools.analyze.clustering import build_weighted_cluster\nfrom optim_esm_tools.region_finding.percentiles import Percentiles\nfrom optim_esm_tools.utils import deprecated\n\n\nclass ProductPercentiles(Percentiles):\n    labels = ('ii', 'iii', 'v')\n\n    @apply_options\n    def _get_masks_weighted(\n        self,\n        min_weight=0.95,\n        lon_lat_dim=('lon', 'lat'),\n        _mask_method='product_rank',\n    ):\n        return super()._get_masks_weighted(\n            min_weight=min_weight,\n            lon_lat_dim=lon_lat_dim,\n            _mask_method=_mask_method,\n        )\n\n    @apply_options\n    def _get_masks_masked(\n        self,\n        product_percentiles=_two_sigma_percent,\n        lon_lat_dim=('lon', 'lat'),\n        _mask_method='product_rank_past_threshold',\n    ) -> _mask_cluster_type:\n        \"\"\"Get mask for max of ii and iii and a box around that.\"\"\"\n        all_mask = self._build_combined_mask(\n            method=_mask_method,\n            product_percentiles=product_percentiles,\n        )\n        masks, clusters = build_cluster_mask(\n            all_mask,\n            lon_coord=self.data_set[lon_lat_dim[0]].values,\n            lat_coord=self.data_set[lon_lat_dim[1]].values,\n        )\n        return masks, clusters\n", "coverage": [1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, null, 1, 1, null, null, null, null, null, 1, null, null, null, null, null, 1, 1, null, null, null, null, null, null, 1, null, null, null, 1, null, null, null, null, 1]}, {"name": "optim_esm_tools/utils.py", "source": "import inspect\nimport os\nimport socket\nimport sys\nimport time\nimport typing as ty\nimport warnings\nfrom collections import defaultdict\nfrom functools import wraps\nfrom importlib import import_module\nfrom platform import python_version\n\nimport numpy as np\nimport pandas as pd\nfrom immutabledict import immutabledict\n\ntry:\n    from git import Repo, InvalidGitRepositoryError\n\n    GIT_INSTALLED = True\nexcept ImportError:  # pragma: no cover\n    GIT_INSTALLED = False\n\nimport sys\n\n\n# From https://github.com/AxFoundation/strax/blob/136a16975b18ee87500051fd81a90c894d9b58dc/strax/utils.py#L33\nif any('jupyter' in arg for arg in sys.argv):\n    # In some cases we are not using any notebooks,\n    # Taken from 44952863 on stack overflow thanks!\n    from tqdm.notebook import tqdm  # pragma: no cover\nelse:\n    from tqdm import tqdm\n\n\n# https://github.com/JoranAngevaare/thesis_plots/blob/d828c08e6f6c9c6926527220a23fd0e61e5d8c60/thesis_plots/main.py\nroot_folder = os.path.join(os.path.split(os.path.realpath(__file__))[0], '..')\n\nfrom optim_esm_tools.plotting.plot_utils import *\n\n\ndef print_versions(\n    modules=('optim_esm_tools',),\n    print_output=True,\n    include_python=True,\n    return_string=False,\n    include_git=True,\n):\n    \"\"\"Print versions of modules installed.\n\n    :param modules: Modules to print, should be str, tuple or list. E.g.\n        print_versions(modules=('numpy', 'optim_esm_tools',))\n    :param return_string: optional. Instead of printing the message,\n        return a string\n    :param include_git: Include the current branch and latest commit\n        hash\n    :return: optional, the message that would have been printed\n    \"\"\"\n    versions = defaultdict(list)\n    if not GIT_INSTALLED and include_git:  # pragma: no cover\n        warnings.warn('Git is not installed, maybe try pip install gitpython')\n        include_git = False\n    if include_python:\n        versions['module'] = ['python']\n        versions['version'] = [python_version()]\n        versions['path'] = [sys.executable]\n        versions['git'] = [None]\n    for m in to_str_tuple(modules):\n        result = _version_info_for_module(m, include_git=include_git)\n        if result is None:\n            continue  # pragma: no cover\n        version, path, git_info = result\n        versions['module'].append(m)\n        versions['version'].append(version)\n        versions['path'].append(path)\n        versions['git'].append(git_info)\n    df = pd.DataFrame(versions)\n    info = f'Host {socket.getfqdn()}\\n{df.to_string(index=False)}'\n    if print_output:\n        print(info)\n    return info if return_string else df\n\n\ndef _version_info_for_module(module_name, include_git):\n    try:\n        mod = import_module(module_name)\n    except ImportError:\n        print(f'{module_name} is not installed')\n        return\n    git = None\n    version = mod.__dict__.get('__version__', None)\n    module_path = mod.__dict__.get('__path__', [None])[0]\n    if include_git:\n        try:\n            repo = Repo(module_path, search_parent_directories=True)\n        except InvalidGitRepositoryError:\n            # not a git repo\n            pass\n        else:\n            try:\n                branch = repo.active_branch\n            except TypeError:  # pragma: no cover\n                branch = 'unknown'\n            try:\n                commit_hash = repo.head.object.hexsha\n            except TypeError:  # pragma: no cover\n                commit_hash = 'unknown'\n            git = f'branch:{branch} | {commit_hash[:7]}'\n    return version, module_path, git\n\n\ndef to_str_tuple(\n    x: ty.Union[str, bytes, list, tuple, pd.Series, np.ndarray],\n) -> ty.Tuple[str]:\n    \"\"\"\n    Convert any sensible instance to a tuple of strings\n    from https://github.com/AxFoundation/strax/blob/d3608efc77acd52e1d5a208c3092b6b45b27a6e2/strax/utils.py#242\n    \"\"\"\n    if isinstance(x, (str, bytes)):\n        return (x,)\n    if isinstance(x, list):\n        return tuple(x)\n    if isinstance(x, tuple):\n        return x\n    raise TypeError(\n        f'Expected string or tuple of strings, got {type(x)}',\n    )  # pragma: no cover\n\n\ndef mathrm(string):\n    return string_to_mathrm(string)\n\n\ndef string_to_mathrm(string):\n    \"\"\"Wrap a string in mathrm mode for latex labels.\"\"\"\n    string = string.replace(' ', r'\\ ')\n    return fr'$\\mathrm{{{string}}}$'\n\n\ndef filter_keyword_arguments(\n    kw: ty.Mapping,\n    func: ty.Callable,\n    allow_varkw: bool = False,\n) -> ty.Mapping:\n    \"\"\"Only pass accepted keyword arguments (from kw) into function \"func\".\n\n    Args:\n        kw (ty.Mapping): kwargs that could go into function func\n        func (type): a function\n        allow_varkw (bool, optional): If True and the function take kwargs, just return the <kw>\n            argument. Defaults to False.\n\n    Returns:\n        dict: Filtered keyword arguments\n    \"\"\"\n    spec = inspect.getfullargspec(func)\n    if allow_varkw and spec.varkw is not None:\n        return kw  # pragma: no cover\n    return {k: v for k, v in kw.items() if k in spec.args}\n\n\ndef check_accepts(\n    accepts: ty.Mapping[str, ty.Iterable] = immutabledict(unit=('absolute', 'std')),\n    do_raise: bool = True,\n):\n    \"\"\"Wrapper for function if certain kwargs are from a defined list of\n    variables.\n\n    Example:\n        ```\n            @check_accepts(accepts=dict(far=('boo', 'booboo')))\n            def bla(far):\n                print(far)\n\n            bla(far='boo')  # prints boo\n            bla(far='booboo')  # prints booboo\n            bla(far=1)  # raises ValueError\n        ```\n\n\n    Args:\n        accepts (ty.Mapping[str, ty.Iterable], optional): which kwarg to accept a limited set of options.\n            Defaults to immutabledict(unit=('absolute', 'std')).\n        do_raise (bool, optional): if False, don't raise an error but just warn. Defaults to True.\n\n    Returns:\n        wrapped function\n    \"\"\"\n\n    def somedec_outer(fn):\n        @wraps(fn)\n        def somedec_inner(*args, **kwargs):\n            message = ''\n            for k, v in kwargs.items():\n                if k in accepts and v not in accepts[k]:\n                    message += (\n                        f'{k} for {v} but only accepts {accepts[k]}'  # pragma: no cover\n                    )\n            if do_raise and message:  # pragma: no cover\n                raise ValueError(message)\n            if message:  # pragma: no cover\n                warnings.warn(message)\n            response = fn(*args, **kwargs)\n            return response\n\n        return somedec_inner\n\n    return somedec_outer\n\n\ndef add_load_kw(func):\n    \"\"\"Add apply `.load` method to the dataset returned by wrapped function.\"\"\"\n\n    @wraps(func)\n    def dep_fun(*args, **kwargs):\n        if 'load' not in inspect.signature(func).parameters:\n            add_load = kwargs.pop('load', False)\n        else:\n            add_load = kwargs.get('load', False)\n        res = func(*args, **kwargs)\n        return res.load() if add_load else res\n\n    return dep_fun\n\n\ndef deprecated(func, message='is deprecated'):\n    @wraps(func)\n    def dep_fun(*args, **kwargs):\n        warnings.warn(\n            f'calling {func.__name__} {message}',\n            category=DeprecationWarning,\n        )\n        return func(*args, **kwargs)\n\n    return dep_fun\n\n\ndef _chopped_string(string, max_len):\n    string = str(string)\n    return string if len(string) < max_len else string[:max_len] + '...'\n\n\n@check_accepts(accepts=dict(_report=('debug', 'info', 'warning', 'error', 'print')))\ndef timed(\n    *a,\n    seconds: ty.Optional[int] = None,\n    _report: ty.Optional[str] = None,\n    _args_max: int = 20,\n    _fmt: str = '.2g',\n    _stacklevel: int = 2,\n):\n    \"\"\"Time a function and print if it takes more than <seconds>\n\n    Args:\n        seconds (int, optional): Defaults to 5.\n        _report (str, optional): Method of reporting, either print or use the global logger. Defaults to 'print'.\n        _args_max (int, optional): Max number of characters in the message for the args and kwars of the function. Defaults to 20.\n        _fmt (str, optional): time format specification. Defaults to '.2g'.\n    \"\"\"\n    if seconds is None or _report is None:\n        from .config import config\n\n        if seconds is None:\n            seconds = float(config['time_tool']['min_seconds'])\n        if _report is None:\n            _report = config['time_tool']['reporter']\n\n    def somedec_outer(fn):\n        @wraps(fn)\n        def timed_func(*args, **kwargs):\n            t0 = time.time()\n            res = fn(*args, **kwargs)\n            dt = time.time() - t0\n            if dt > seconds:\n                hours = '' if dt < 3600 else f' ({dt/3600:{_fmt}} h) '\n                message = (\n                    f'{fn.__name__} took {dt:{_fmt}} s{hours} (for '\n                    f'{_chopped_string(args, _args_max)}, '\n                    f'{_chopped_string(kwargs, _args_max)})'\n                ).replace('\\n', ' ')\n                if _report == 'print':\n                    print(message)\n                else:\n                    from .config import get_logger\n\n                    getattr(get_logger(), _report)(message, stacklevel=_stacklevel)\n            return res\n\n        return timed_func\n\n    if a and isinstance(a[0], ty.Callable):\n        # Decorator that isn't closed\n        return somedec_outer(a[0])\n    return somedec_outer\n\n\n@check_accepts(accepts=dict(_report=('debug', 'info', 'warning', 'error', 'print')))\ndef logged_tqdm(*a, log=None, _report='warning', **kw):\n    from .config import get_logger\n\n    log = log or get_logger()\n    pbar = tqdm(*a, **kw)\n    generator = iter(pbar)\n    while True:\n        try:\n            v = next(generator)\n            getattr(log, _report)(pbar, stacklevel=2)\n            yield v\n\n        except StopIteration:\n            pbar.close()\n            getattr(log, _report)(pbar, stacklevel=2)\n            return\n\n\ndef scientific_latex_notation(\n    value: ty.Union[str, float, int],\n    high: float = 1e3,\n    low: float = 1e-3,\n    precision: str = '.2e',\n):\n    \"\"\"convert a string of float-representation to latex-format with exponents\"\"\"\n    if isinstance(value, float):\n        fl = value\n    elif isinstance(value, int):\n        fl = float(value)\n    elif isinstance(value, str):\n        value = str(value)\n\n        try:\n            fl = float(value)\n        except (TypeError, ValueError) as e:\n            return value\n    else:\n        raise TypeError(f'misunderstood {value} ({type(value)})')\n    if abs(fl) > high or abs(fl) < low or \"e\" in str(value):\n        fl_s = f\"{fl:{precision}}\"\n        if \"e\" not in fl_s:\n            return value\n        a, b = fl_s.split(\"e+\") if \"e+\" in fl_s else fl_s.split(\"e-\")\n        if '-' in fl_s:\n            exp = f'-{int(b)}'\n        else:\n            exp = f'{int(b)}'\n        res = f\"${a}\\\\times 10^{{{exp}}}$\"\n\n        return res\n    return value\n", "coverage": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, null, 1, 1, null, 1, null, null, null, 1, null, null, null, 1, null, null, null, null, 1, null, null, null, 1, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, null, null, null, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, 1, null, 1, 1, null, null, 1, 1, null, null, 1, 1, null, null, 1, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, null, null, null, null, null, 1, 1, null, null, 1, null, 1, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, null, null, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, 1, 1, 1, 1, null, null, null, null, null, null, null, 1, 1, null, 1, null, 1, null, null, 1, null, null, 1, 1, 1, 1, null, 1, 1, 1, null, 1, null, null, 1, 1, 1, 1, null, null, null, 1, null, 1, null, null, 1, 1, 1, null, null, 1, 1, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, 1, 1, null, 1, 1, 1, 1, null, 1, 1, 1, 1, 1, 1, 1, 1, 1, null, null, null, null, 1, 1, null, 1, null, 1, 1, null, 1, null, 1, null, 1, 1, null, null, 1, 1, 0, null, 0, 0, 0, 0, 0, 0, 0, 0, null, 0, 0, 0, 0, null, null, 1, null, null, null, null, null, null, 0, 0, 0, 0, 0, 0, null, 0, 0, 0, 0, null, 0, 0, 0, 0, 0, 0, 0, 0, null, 0, 0, null, 0, 0]}], "git": {"branch": "master", "head": {"id": "a2c715bf9bedd9aaae5a151bf0dd438146cf784e", "author_name": "J. R. Angevaare", "author_email": "joranangevaare@gmail.com", "committer_name": "J. R. Angevaare", "committer_email": "joranangevaare@gmail.com", "message": "fix typo"}, "remotes": [{"name": "origin", "url": "https://github.com/JoranAngevaare/optim_esm_tools.git"}]}, "service_name": "coveralls-python", "repo_token": "[secure]", "config_file": ".coveragerc", "base_dir": "", "src_dir": ""}
==
Reporting 34 files
==

optim_esm_tools/__init__.py - 13/16
optim_esm_tools/_test_utils.py - 74/165
optim_esm_tools/analyze/__init__.py - 14/14
optim_esm_tools/analyze/clustering.py - 222/595
optim_esm_tools/analyze/cmip_handler.py - 72/274
optim_esm_tools/analyze/combine_variables.py - 247/647
optim_esm_tools/analyze/concise_dataframe.py - 69/136
optim_esm_tools/analyze/discontinuous_grid_patcher.py - 81/221
optim_esm_tools/analyze/find_matches.py - 115/353
optim_esm_tools/analyze/globals.py - 6/9
optim_esm_tools/analyze/io.py - 10/36
optim_esm_tools/analyze/merge_candidate_regions.py - 233/457
optim_esm_tools/analyze/pre_process.py - 140/496
optim_esm_tools/analyze/region_calculation.py - 242/686
optim_esm_tools/analyze/region_finding.py - 4/5
optim_esm_tools/analyze/time_statistics.py - 52/197
optim_esm_tools/analyze/tipping_criteria.py - 180/405
optim_esm_tools/analyze/tools.py - 148/322
optim_esm_tools/analyze/xarray_tools.py - 186/549
optim_esm_tools/config.py - 18/45
optim_esm_tools/plotting/__init__.py - 3/3
optim_esm_tools/plotting/map_maker.py - 38/80
optim_esm_tools/plotting/plot.py - 61/156
optim_esm_tools/plotting/plot_utils.py - 53/126
optim_esm_tools/region_finding/__init__.py - 8/8
optim_esm_tools/region_finding/_base.py - 85/249
optim_esm_tools/region_finding/iter_ranges.py - 71/200
optim_esm_tools/region_finding/keep_all.py - 8/54
optim_esm_tools/region_finding/local_history.py - 47/101
optim_esm_tools/region_finding/max_region.py - 21/36
optim_esm_tools/region_finding/named_region.py - 31/67
optim_esm_tools/region_finding/percentiles.py - 87/255
optim_esm_tools/region_finding/product_percentiles.py - 21/48
optim_esm_tools/utils.py - 136/348
Coverage submitted!
{'message': 'Job ##3.1', 'url': 'https://coveralls.io/jobs/161839823'}
Job ##3.1
https://coveralls.io/jobs/161839823
